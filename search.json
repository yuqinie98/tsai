[
  {
    "objectID": "models.tst.html",
    "href": "models.tst.html",
    "title": "TST",
    "section": "",
    "text": "This is an unofficial PyTorch implementation by Ignacio Oguiza of - oguiza@timeseriesAI.co based on: * George Zerveas et al. A Transformer-based Framework for Multivariate Time Series Representation Learning, in Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery and Data Mining (KDD ’21), August 14–18, 2021. ArXiV version: https://arxiv.org/abs/2010.02803 * Official implementation: https://github.com/gzerveas/mvts_transformer\nThis paper uses ‘Attention is all you need’ as a major reference: * Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., … & Polosukhin, I. (2017). Attention is all you need. In Advances in neural information processing systems (pp. 5998-6008).\nThis implementation is adapted to work with the rest of the tsai library, and contain some hyperparameters that are not available in the original implementation. They are included to experiment with them."
  },
  {
    "objectID": "models.tst.html#tst-arguments",
    "href": "models.tst.html#tst-arguments",
    "title": "TST",
    "section": "TST arguments",
    "text": "TST arguments\nUsual values are the ones that appear in the “Attention is all you need” and “A Transformer-based Framework for Multivariate Time Series Representation Learning” papers.\nThe default values are the ones selected as a default configuration in the latter.\n\nc_in: the number of features (aka variables, dimensions, channels) in the time series dataset. dls.var\nc_out: the number of target classes. dls.c\nseq_len: number of time steps in the time series. dls.len\nmax_seq_len: useful to control the temporal resolution in long time series to avoid memory issues. Default. None.\nd_model: total dimension of the model (number of features created by the model). Usual values: 128-1024. Default: 128.\nn_heads: parallel attention heads. Usual values: 8-16. Default: 16.\nd_k: size of the learned linear projection of queries and keys in the MHA. Usual values: 16-512. Default: None -> (d_model/n_heads) = 32.\nd_v: size of the learned linear projection of values in the MHA. Usual values: 16-512. Default: None -> (d_model/n_heads) = 32.\nd_ff: the dimension of the feedforward network model. Usual values: 256-4096. Default: 256.\ndropout: amount of residual dropout applied in the encoder. Usual values: 0.-0.3. Default: 0.1.\nactivation: the activation function of intermediate layer, relu or gelu. Default: ‘gelu’.\nn_layers: the number of sub-encoder-layers in the encoder. Usual values: 2-8. Default: 3.\nfc_dropout: dropout applied to the final fully connected layer. Usual values: 0.-0.8. Default: 0.\ny_range: range of possible y values (used in regression tasks). Default: None\nkwargs: nn.Conv1d kwargs. If not {}, a nn.Conv1d with those kwargs will be applied to original time series."
  },
  {
    "objectID": "models.tst.html#imports",
    "href": "models.tst.html#imports",
    "title": "TST",
    "section": "Imports",
    "text": "Imports"
  },
  {
    "objectID": "models.tst.html#tst",
    "href": "models.tst.html#tst",
    "title": "TST",
    "section": "TST",
    "text": "TST\n\nt = torch.rand(16, 50, 128)\noutput, attn = _MultiHeadAttention(d_model=128, n_heads=3, d_k=8, d_v=6)(t, t, t)\noutput.shape, attn.shape\n\n(torch.Size([16, 50, 128]), torch.Size([16, 3, 50, 50]))\n\n\n\nt = torch.rand(16, 50, 128)\noutput = _TSTEncoderLayer(q_len=50, d_model=128, n_heads=3, d_k=None, d_v=None, d_ff=512, dropout=0.1, activation='gelu')(t)\noutput.shape\n\ntorch.Size([16, 50, 128])\n\n\n\nsource\n\nTST\n\n TST (c_in:int, c_out:int, seq_len:int, max_seq_len:Optional[int]=None,\n      n_layers:int=3, d_model:int=128, n_heads:int=16,\n      d_k:Optional[int]=None, d_v:Optional[int]=None, d_ff:int=256,\n      dropout:float=0.1, act:str='gelu', fc_dropout:float=0.0,\n      y_range:Optional[tuple]=None, verbose:bool=False, **kwargs)\n\nSame as nn.Module, but no need for subclasses to call super().__init__\n\nbs = 32\nc_in = 9  # aka channels, features, variables, dimensions\nc_out = 2\nseq_len = 5000\n\nxb = torch.randn(bs, c_in, seq_len)\n\n# standardize by channel by_var based on the training set\nxb = (xb - xb.mean((0, 2), keepdim=True)) / xb.std((0, 2), keepdim=True)\n\n# Settings\nmax_seq_len = 256\nd_model = 128\nn_heads = 16\nd_k = d_v = None # if None --> d_model // n_heads\nd_ff = 256\ndropout = 0.1\nactivation = \"gelu\"\nn_layers = 3\nfc_dropout = 0.1\nkwargs = {}\n\nmodel = TST(c_in, c_out, seq_len, max_seq_len=max_seq_len, d_model=d_model, n_heads=n_heads,\n            d_k=d_k, d_v=d_v, d_ff=d_ff, dropout=dropout, activation=activation, n_layers=n_layers,\n            fc_dropout=fc_dropout, **kwargs)\ntest_eq(model.to(xb.device)(xb).shape, [bs, c_out])\nprint(f'model parameters: {count_parameters(model)}')\n\nmodel parameters: 517378\n\n\n\nbs = 32\nc_in = 9  # aka channels, features, variables, dimensions\nc_out = 2\nseq_len = 60\n\nxb = torch.randn(bs, c_in, seq_len)\n\n# standardize by channel by_var based on the training set\nxb = (xb - xb.mean((0, 2), keepdim=True)) / xb.std((0, 2), keepdim=True)\n\n# Settings\nmax_seq_len = 120\nd_model = 128\nn_heads = 16\nd_k = d_v = None # if None --> d_model // n_heads\nd_ff = 256\ndropout = 0.1\nact = \"gelu\"\nn_layers = 3\nfc_dropout = 0.1\nkwargs = {}\n# kwargs = dict(kernel_size=5, padding=2)\n\nmodel = TST(c_in, c_out, seq_len, max_seq_len=max_seq_len, d_model=d_model, n_heads=n_heads,\n            d_k=d_k, d_v=d_v, d_ff=d_ff, dropout=dropout, act=act, n_layers=n_layers,\n            fc_dropout=fc_dropout, **kwargs)\ntest_eq(model.to(xb.device)(xb).shape, [bs, c_out])\nprint(f'model parameters: {count_parameters(model)}')\n\nmodel parameters: 420226"
  },
  {
    "objectID": "models.gatedtabtransformer.html",
    "href": "models.gatedtabtransformer.html",
    "title": "GatedTabTransformer",
    "section": "",
    "text": "This implementation is based on:\n\nCholakov, R., & Kolev, T. (2022). The GatedTabTransformer. An enhanced deep learning architecture for tabular modeling. arXiv preprint arXiv:2201.00199. arXiv preprint https://arxiv.org/abs/2201.00199\nHuang, X., Khetan, A., Cvitkovic, M., & Karnin, Z. (2020). TabTransformer: Tabular Data Modeling Using Contextual Embeddings. arXiv preprint https://arxiv.org/pdf/2012.06678\n\nOfficial repo: https://github.com/radi-cho/GatedTabTransformer\n\nsource\n\nGatedTabTransformer\n\n GatedTabTransformer (classes, cont_names, c_out, column_embed=True,\n                      add_shared_embed=False, shared_embed_div=8,\n                      embed_dropout=0.1, drop_whole_embed=False,\n                      d_model=32, n_layers=6, n_heads=8, d_k=None,\n                      d_v=None, d_ff=None, res_attention=True,\n                      attention_act='gelu', res_dropout=0.1,\n                      norm_cont=True, mlp_d_model=32, mlp_d_ffn=64,\n                      mlp_layers=4)\n\nBase class for all neural network modules.\nYour models should also subclass this class.\nModules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes::\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 20, 5)\n        self.conv2 = nn.Conv2d(20, 20, 5)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        return F.relu(self.conv2(x))\nSubmodules assigned in this way will be registered, and will have their parameters converted too when you call :meth:to, etc.\n.. note:: As per the example above, an __init__() call to the parent class must be made before assignment on the child.\n:ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool\n\nfrom fastcore.test import test_eq\nfrom fastcore.basics import first\nfrom fastai.data.external import untar_data, URLs\nfrom fastai.tabular.data import TabularDataLoaders\nfrom fastai.tabular.core import Categorify, FillMissing\nfrom fastai.data.transforms import Normalize\nimport pandas as pd\n\n\npath = untar_data(URLs.ADULT_SAMPLE)\ndf = pd.read_csv(path/'adult.csv')\ndls = TabularDataLoaders.from_csv(path/'adult.csv', path=path, y_names=\"salary\",\n    cat_names = ['workclass', 'education', 'marital-status', 'occupation', 'relationship', 'race'],\n    cont_names = ['age', 'fnlwgt', 'education-num'],\n    procs = [Categorify, FillMissing, Normalize])\nx_cat, x_cont, yb = first(dls.train)\nmodel = GatedTabTransformer(dls.classes, dls.cont_names, dls.c)\ntest_eq(model(x_cat, x_cont).shape, (dls.train.bs, dls.c))"
  },
  {
    "objectID": "data.features.html",
    "href": "data.features.html",
    "title": "Featurizing Time Series",
    "section": "",
    "text": "Functions used to transform time series into a dataframe that can be used to create tabular dataloaders.\n\nIn this case we are using tsfresh that is one of the most widely known libraries used to create features from time series. You can get more details about this library here: https://tsfresh.readthedocs.io/en/latest/\n\nsource\n\nget_ts_features\n\n get_ts_features (X:Union[numpy.ndarray,torch.Tensor],\n                  y:Union[NoneType,numpy.ndarray,torch.Tensor]=None,\n                  features:Union[str,dict]='min',\n                  n_jobs:Optional[int]=None, **kwargs)\n\nArgs: X: np.array or torch.Tesnor of shape [samples, dimensions, timesteps]. y: Not required for unlabeled data. Otherwise, you need to pass it. features: ‘min’, ‘efficient’, ‘all’, or a dictionary. Be aware that ‘efficient’ and ‘all’ may required substantial memory and time.\n\ndsid = 'NATOPS'\nX, y, splits = get_UCR_data(dsid, return_split=False)\nX.shape\n\n(360, 24, 51)\n\n\nThere are 3 levels of fatures you can extract: ‘min’, ‘efficient’ and ‘all’. I’d encourage you to start with min as feature creation may take a long time.\nIn addition to this, you can pass a dictionary to build the desired features (see tsfresh documentation in the link above).\n\nts_features_df = get_ts_features(X, y)\nts_features_df.shape\n\nFeature Extraction: 100%|█████████████████████████████████████| 40/40 [00:14<00:00,  2.82it/s]\n/Users/nacho/opt/anaconda3/envs/py37torch110/lib/python3.7/site-packages/ipykernel_launcher.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n\n\n(360, 241)\n\n\nThe ‘min’ set creates a dataframe with 8 features per channel + 1 per target (total 193) for each time series sample (360).\n\ncont_names = ts_features_df.columns[:-1]\ny_names = 'target'\ndls = get_tabular_dls(ts_features_df, splits=splits, cont_names=cont_names, y_names=y_names)\ndls.show_batch()\n\n\n\n  \n    \n      \n      0__sum_values\n      0__median\n      0__mean\n      0__length\n      0__standard_deviation\n      0__variance\n      0__root_mean_square\n      0__maximum\n      0__absolute_maximum\n      0__minimum\n      1__sum_values\n      1__median\n      1__mean\n      1__length\n      1__standard_deviation\n      1__variance\n      1__root_mean_square\n      1__maximum\n      1__absolute_maximum\n      1__minimum\n      2__sum_values\n      2__median\n      2__mean\n      2__length\n      2__standard_deviation\n      2__variance\n      2__root_mean_square\n      2__maximum\n      2__absolute_maximum\n      2__minimum\n      3__sum_values\n      3__median\n      3__mean\n      3__length\n      3__standard_deviation\n      3__variance\n      3__root_mean_square\n      3__maximum\n      3__absolute_maximum\n      3__minimum\n      4__sum_values\n      4__median\n      4__mean\n      4__length\n      4__standard_deviation\n      4__variance\n      4__root_mean_square\n      4__maximum\n      4__absolute_maximum\n      4__minimum\n      5__sum_values\n      5__median\n      5__mean\n      5__length\n      5__standard_deviation\n      5__variance\n      5__root_mean_square\n      5__maximum\n      5__absolute_maximum\n      5__minimum\n      6__sum_values\n      6__median\n      6__mean\n      6__length\n      6__standard_deviation\n      6__variance\n      6__root_mean_square\n      6__maximum\n      6__absolute_maximum\n      6__minimum\n      7__sum_values\n      7__median\n      7__mean\n      7__length\n      7__standard_deviation\n      7__variance\n      7__root_mean_square\n      7__maximum\n      7__absolute_maximum\n      7__minimum\n      8__sum_values\n      8__median\n      8__mean\n      8__length\n      8__standard_deviation\n      8__variance\n      8__root_mean_square\n      8__maximum\n      8__absolute_maximum\n      8__minimum\n      9__sum_values\n      9__median\n      9__mean\n      9__length\n      9__standard_deviation\n      9__variance\n      9__root_mean_square\n      9__maximum\n      9__absolute_maximum\n      9__minimum\n      10__sum_values\n      10__median\n      10__mean\n      10__length\n      10__standard_deviation\n      10__variance\n      10__root_mean_square\n      10__maximum\n      10__absolute_maximum\n      10__minimum\n      11__sum_values\n      11__median\n      11__mean\n      11__length\n      11__standard_deviation\n      11__variance\n      11__root_mean_square\n      11__maximum\n      11__absolute_maximum\n      11__minimum\n      12__sum_values\n      12__median\n      12__mean\n      12__length\n      12__standard_deviation\n      12__variance\n      12__root_mean_square\n      12__maximum\n      12__absolute_maximum\n      12__minimum\n      13__sum_values\n      13__median\n      13__mean\n      13__length\n      13__standard_deviation\n      13__variance\n      13__root_mean_square\n      13__maximum\n      13__absolute_maximum\n      13__minimum\n      14__sum_values\n      14__median\n      14__mean\n      14__length\n      14__standard_deviation\n      14__variance\n      14__root_mean_square\n      14__maximum\n      14__absolute_maximum\n      14__minimum\n      15__sum_values\n      15__median\n      15__mean\n      15__length\n      15__standard_deviation\n      15__variance\n      15__root_mean_square\n      15__maximum\n      15__absolute_maximum\n      15__minimum\n      16__sum_values\n      16__median\n      16__mean\n      16__length\n      16__standard_deviation\n      16__variance\n      16__root_mean_square\n      16__maximum\n      16__absolute_maximum\n      16__minimum\n      17__sum_values\n      17__median\n      17__mean\n      17__length\n      17__standard_deviation\n      17__variance\n      17__root_mean_square\n      17__maximum\n      17__absolute_maximum\n      17__minimum\n      18__sum_values\n      18__median\n      18__mean\n      18__length\n      18__standard_deviation\n      18__variance\n      18__root_mean_square\n      18__maximum\n      18__absolute_maximum\n      18__minimum\n      19__sum_values\n      19__median\n      19__mean\n      19__length\n      19__standard_deviation\n      19__variance\n      19__root_mean_square\n      19__maximum\n      19__absolute_maximum\n      19__minimum\n      20__sum_values\n      20__median\n      20__mean\n      20__length\n      20__standard_deviation\n      20__variance\n      20__root_mean_square\n      20__maximum\n      20__absolute_maximum\n      20__minimum\n      21__sum_values\n      21__median\n      21__mean\n      21__length\n      21__standard_deviation\n      21__variance\n      21__root_mean_square\n      21__maximum\n      21__absolute_maximum\n      21__minimum\n      22__sum_values\n      22__median\n      22__mean\n      22__length\n      22__standard_deviation\n      22__variance\n      22__root_mean_square\n      22__maximum\n      22__absolute_maximum\n      22__minimum\n      23__sum_values\n      23__median\n      23__mean\n      23__length\n      23__standard_deviation\n      23__variance\n      23__root_mean_square\n      23__maximum\n      23__absolute_maximum\n      23__minimum\n      target\n    \n  \n  \n    \n      0\n      -4.150354\n      -0.432566\n      -0.081380\n      51.0\n      0.597360\n      0.356840\n      0.602878\n      0.778331\n      0.861279\n      -0.861279\n      -58.627594\n      -1.552286\n      -1.149561\n      51.0\n      0.601466\n      0.361761\n      1.297401\n      -0.056605\n      1.699415\n      -1.699415\n      -45.035744\n      -0.893296\n      -0.883054\n      51.0\n      0.181012\n      0.032765\n      0.901415\n      -0.495560\n      1.175184\n      -1.175184\n      38.219048\n      0.796074\n      0.749393\n      51.0\n      0.234144\n      0.054824\n      0.785120\n      1.111021\n      1.111021\n      0.392384\n      -38.509396\n      -1.455609\n      -0.755086\n      51.0\n      1.031962\n      1.064946\n      1.278711\n      0.693374\n      1.788006\n      -1.788006\n      -31.586580\n      -0.608169\n      -0.619345\n      51.0\n      0.421872\n      0.177976\n      0.749376\n      0.048263\n      1.458466\n      -1.458466\n      -31.067938\n      -0.635204\n      -0.609175\n      51.0\n      0.081749\n      0.006683\n      0.614636\n      -0.482192\n      0.746402\n      -0.746402\n      -31.909603\n      -0.629038\n      -0.625678\n      51.0\n      0.036791\n      0.001354\n      0.626759\n      -0.535260\n      0.697649\n      -0.697649\n      -17.736265\n      -0.304125\n      -0.347770\n      51.0\n      0.087451\n      0.007648\n      0.358597\n      -0.237202\n      0.495997\n      -0.495997\n      32.072475\n      0.629244\n      0.628872\n      51.0\n      0.025069\n      0.000628\n      0.629372\n      0.705176\n      0.705176\n      0.578095\n      -29.936869\n      -0.710722\n      -0.586997\n      51.0\n      0.179691\n      0.032289\n      0.613885\n      -0.265087\n      0.757163\n      -0.757163\n      -12.696542\n      -0.096251\n      -0.248952\n      51.0\n      0.292832\n      0.085750\n      0.384353\n      0.064884\n      0.710493\n      -0.710493\n      -15.762158\n      -0.560294\n      -0.309062\n      51.0\n      0.451446\n      0.203804\n      0.547104\n      0.387947\n      0.861255\n      -0.861255\n      -50.898941\n      -1.250778\n      -0.998018\n      51.0\n      0.350596\n      0.122918\n      1.057808\n      -0.417412\n      1.338018\n      -1.338018\n      -37.527363\n      -0.719907\n      -0.735831\n      51.0\n      0.064895\n      0.004211\n      0.738687\n      -0.624461\n      0.893202\n      -0.893202\n      36.929703\n      0.767098\n      0.724112\n      51.0\n      0.122116\n      0.014912\n      0.734336\n      0.937925\n      0.937925\n      0.508076\n      -35.144825\n      -1.175173\n      -0.689114\n      51.0\n      0.687803\n      0.473073\n      0.973628\n      0.371802\n      1.436428\n      -1.436428\n      -22.387865\n      -0.307232\n      -0.438978\n      51.0\n      0.260977\n      0.068109\n      0.510696\n      -0.122424\n      1.079398\n      -1.079398\n      -12.441168\n      -0.456977\n      -0.243944\n      51.0\n      0.540300\n      0.291924\n      0.592817\n      0.543158\n      0.968512\n      -0.968512\n      -55.026745\n      -1.469483\n      -1.078956\n      51.0\n      0.552103\n      0.304817\n      1.212008\n      -0.084885\n      1.574698\n      -1.574698\n      -38.722870\n      -0.801516\n      -0.759272\n      51.0\n      0.202432\n      0.040979\n      0.785794\n      -0.339747\n      1.160803\n      -1.160803\n      36.171883\n      0.779261\n      0.709253\n      51.0\n      0.187150\n      0.035025\n      0.733529\n      1.006178\n      1.006178\n      0.436847\n      -39.467243\n      -1.469226\n      -0.773867\n      51.0\n      0.965357\n      0.931915\n      1.237249\n      0.637314\n      1.777838\n      -1.777838\n      -30.077049\n      -0.508991\n      -0.589746\n      51.0\n      0.351033\n      0.123224\n      0.686312\n      -0.159678\n      1.447092\n      -1.447092\n      6.0\n    \n    \n      1\n      -13.291943\n      -0.478186\n      -0.260626\n      51.0\n      0.565626\n      0.319933\n      0.622783\n      0.792820\n      1.303543\n      -1.303543\n      -61.319569\n      -1.450219\n      -1.202344\n      51.0\n      0.738642\n      0.545592\n      1.411108\n      0.212865\n      2.012848\n      -2.012848\n      -56.104191\n      -1.049989\n      -1.100082\n      51.0\n      0.301088\n      0.090654\n      1.140541\n      -0.652380\n      1.808150\n      -1.808150\n      37.879345\n      0.528623\n      0.742732\n      51.0\n      0.421161\n      0.177376\n      0.853831\n      1.720074\n      1.720074\n      0.307349\n      -17.564539\n      -0.316360\n      -0.344403\n      51.0\n      1.473537\n      2.171310\n      1.513249\n      1.420735\n      2.088120\n      -2.088120\n      -28.667639\n      -0.747896\n      -0.562111\n      51.0\n      0.310389\n      0.096341\n      0.642113\n      -0.115223\n      1.038840\n      -1.038840\n      -28.772152\n      -0.563779\n      -0.564160\n      51.0\n      0.124031\n      0.015384\n      0.577633\n      -0.245861\n      0.824872\n      -0.824872\n      -33.772125\n      -0.645600\n      -0.662199\n      51.0\n      0.114726\n      0.013162\n      0.672063\n      -0.401062\n      0.818782\n      -0.818782\n      -15.236373\n      -0.159198\n      -0.298752\n      51.0\n      0.170302\n      0.029003\n      0.343883\n      -0.086246\n      0.585862\n      -0.585862\n      38.112534\n      0.780826\n      0.747305\n      51.0\n      0.179499\n      0.032220\n      0.768560\n      1.097624\n      1.097624\n      0.537276\n      -17.315622\n      -0.583021\n      -0.339522\n      51.0\n      0.437091\n      0.191049\n      0.553465\n      0.450234\n      0.775951\n      -0.775951\n      -19.989185\n      -0.264662\n      -0.391945\n      51.0\n      0.217890\n      0.047476\n      0.448438\n      -0.065923\n      0.795455\n      -0.795455\n      -19.768250\n      -0.556956\n      -0.387613\n      51.0\n      0.419555\n      0.176027\n      0.571201\n      0.267592\n      1.162521\n      -1.162521\n      -51.892273\n      -1.095654\n      -1.017496\n      51.0\n      0.480092\n      0.230489\n      1.125071\n      -0.065249\n      1.600320\n      -1.600320\n      -39.717018\n      -0.745883\n      -0.778765\n      51.0\n      0.260267\n      0.067739\n      0.821105\n      -0.417944\n      1.310366\n      -1.310366\n      40.596100\n      0.665194\n      0.796002\n      51.0\n      0.278905\n      0.077788\n      0.843450\n      1.461865\n      1.461865\n      0.516946\n      -16.516878\n      -0.423822\n      -0.323860\n      51.0\n      1.098530\n      1.206768\n      1.145274\n      1.163813\n      1.595865\n      -1.595865\n      -24.584900\n      -0.500748\n      -0.482057\n      51.0\n      0.119756\n      0.014342\n      0.496710\n      -0.280011\n      0.684521\n      -0.684521\n      -14.034793\n      -0.415769\n      -0.275192\n      51.0\n      0.505788\n      0.255822\n      0.575806\n      0.657360\n      1.240724\n      -1.240724\n      -53.058266\n      -1.193786\n      -1.040358\n      51.0\n      0.601415\n      0.361700\n      1.201684\n      0.181549\n      1.764784\n      -1.764784\n      -55.295517\n      -1.040858\n      -1.084226\n      51.0\n      0.286421\n      0.082037\n      1.121420\n      -0.701432\n      1.678984\n      -1.678984\n      40.820068\n      0.654046\n      0.800394\n      51.0\n      0.323895\n      0.104908\n      0.863445\n      1.589356\n      1.589356\n      0.446026\n      -18.153927\n      -0.256226\n      -0.355959\n      51.0\n      1.253342\n      1.570865\n      1.302909\n      1.182885\n      1.789366\n      -1.789366\n      -27.441236\n      -0.664347\n      -0.538063\n      51.0\n      0.272570\n      0.074294\n      0.603164\n      -0.136812\n      0.977601\n      -0.977601\n      6.0\n    \n    \n      2\n      -26.369993\n      -0.541299\n      -0.517059\n      51.0\n      0.039179\n      0.001535\n      0.518541\n      -0.452868\n      0.559545\n      -0.559545\n      -92.051758\n      -1.793647\n      -1.804936\n      51.0\n      0.039004\n      0.001521\n      1.805358\n      -1.764378\n      1.951791\n      -1.951791\n      -35.927155\n      -0.705275\n      -0.704454\n      51.0\n      0.045429\n      0.002064\n      0.705917\n      -0.629029\n      0.770954\n      -0.770954\n      62.661503\n      0.939935\n      1.228657\n      51.0\n      0.709640\n      0.503590\n      1.418868\n      2.120925\n      2.120925\n      0.458483\n      -57.456329\n      -1.724902\n      -1.126595\n      51.0\n      0.855248\n      0.731448\n      1.414448\n      0.300213\n      1.855833\n      -1.855833\n      -23.699812\n      -0.450462\n      -0.464702\n      51.0\n      0.112595\n      0.012678\n      0.478148\n      -0.285705\n      0.623726\n      -0.623726\n      -31.125208\n      -0.622847\n      -0.610298\n      51.0\n      0.022984\n      0.000528\n      0.610731\n      -0.557779\n      0.637554\n      -0.637554\n      -36.694515\n      -0.718917\n      -0.719500\n      51.0\n      0.016971\n      0.000288\n      0.719700\n      -0.695670\n      0.778265\n      -0.778265\n      -6.442357\n      -0.144027\n      -0.126321\n      51.0\n      0.033981\n      0.001155\n      0.130812\n      -0.070041\n      0.161564\n      -0.161564\n      40.764172\n      0.681456\n      0.799298\n      51.0\n      0.238139\n      0.056710\n      0.834019\n      1.111102\n      1.111102\n      0.552852\n      -25.416428\n      -0.704276\n      -0.498361\n      51.0\n      0.288164\n      0.083038\n      0.575676\n      -0.020578\n      0.764308\n      -0.764308\n      -2.718482\n      -0.039640\n      -0.053304\n      51.0\n      0.046194\n      0.002134\n      0.070534\n      0.009132\n      0.120817\n      -0.120817\n      -29.257219\n      -0.596503\n      -0.573671\n      51.0\n      0.036931\n      0.001364\n      0.574858\n      -0.513906\n      0.613934\n      -0.613934\n      -70.801453\n      -1.392250\n      -1.388264\n      51.0\n      0.038705\n      0.001498\n      1.388803\n      -1.335494\n      1.483569\n      -1.483569\n      -24.860741\n      -0.503068\n      -0.487466\n      51.0\n      0.051361\n      0.002638\n      0.490164\n      -0.399412\n      0.554868\n      -0.554868\n      55.241459\n      0.890764\n      1.083166\n      51.0\n      0.499532\n      0.249532\n      1.192804\n      1.726409\n      1.726409\n      0.550334\n      -47.054955\n      -1.390331\n      -0.922646\n      51.0\n      0.645009\n      0.416036\n      1.125750\n      0.124965\n      1.487803\n      -1.487803\n      -14.331758\n      -0.274985\n      -0.281015\n      51.0\n      0.115668\n      0.013379\n      0.303889\n      -0.115613\n      0.459527\n      -0.459527\n      -19.895189\n      -0.410751\n      -0.390102\n      51.0\n      0.040469\n      0.001638\n      0.392195\n      -0.319673\n      0.470861\n      -0.470861\n      -79.544144\n      -1.545947\n      -1.559689\n      51.0\n      0.048223\n      0.002325\n      1.560434\n      -1.502446\n      1.719284\n      -1.719284\n      -35.871010\n      -0.713294\n      -0.703353\n      51.0\n      0.033074\n      0.001094\n      0.704130\n      -0.635125\n      0.811442\n      -0.811442\n      63.523643\n      1.030555\n      1.245562\n      51.0\n      0.591418\n      0.349775\n      1.378840\n      2.013603\n      2.013603\n      0.633188\n      -50.020065\n      -1.554366\n      -0.980786\n      51.0\n      0.832042\n      0.692294\n      1.286171\n      0.388921\n      1.755015\n      -1.755015\n      -22.236259\n      -0.419159\n      -0.436005\n      51.0\n      0.130094\n      0.016924\n      0.455000\n      -0.184703\n      0.600059\n      -0.600059\n      2.0\n    \n    \n      3\n      -22.089481\n      -0.426620\n      -0.433127\n      51.0\n      0.051887\n      0.002692\n      0.436224\n      -0.353775\n      0.493256\n      -0.493256\n      -79.072937\n      -1.560552\n      -1.550450\n      51.0\n      0.035141\n      0.001235\n      1.550848\n      -1.475678\n      1.623195\n      -1.623195\n      -48.691841\n      -0.953417\n      -0.954742\n      51.0\n      0.021985\n      0.000483\n      0.954995\n      -0.907814\n      0.998319\n      -0.998319\n      47.297810\n      0.649993\n      0.927408\n      51.0\n      0.424749\n      0.180412\n      1.020048\n      1.830051\n      1.830051\n      0.447356\n      -19.371216\n      -1.533620\n      -0.379828\n      51.0\n      1.421009\n      2.019268\n      1.470897\n      1.603023\n      1.656069\n      -1.656069\n      -14.617298\n      -0.637720\n      -0.286614\n      51.0\n      0.478551\n      0.229011\n      0.557816\n      0.564454\n      0.663262\n      -0.663262\n      -36.714886\n      -0.733394\n      -0.719900\n      51.0\n      0.051507\n      0.002653\n      0.721740\n      -0.645469\n      0.778831\n      -0.778831\n      -25.552364\n      -0.470940\n      -0.501027\n      51.0\n      0.054890\n      0.003013\n      0.504025\n      -0.450767\n      0.609533\n      -0.609533\n      -6.847224\n      -0.122346\n      -0.134259\n      51.0\n      0.041205\n      0.001698\n      0.140440\n      -0.079976\n      0.189690\n      -0.189690\n      37.426327\n      0.639158\n      0.733850\n      51.0\n      0.173450\n      0.030085\n      0.754069\n      1.164017\n      1.164017\n      0.588437\n      -13.343306\n      -0.673153\n      -0.261633\n      51.0\n      0.566572\n      0.321004\n      0.624064\n      0.630515\n      0.746939\n      -0.746939\n      6.375568\n      0.068729\n      0.125011\n      51.0\n      0.151819\n      0.023049\n      0.196664\n      0.368656\n      0.368656\n      -0.025980\n      -28.704739\n      -0.568022\n      -0.562838\n      51.0\n      0.050026\n      0.002503\n      0.565057\n      -0.490766\n      0.616910\n      -0.616910\n      -54.063286\n      -1.046715\n      -1.060064\n      51.0\n      0.033979\n      0.001155\n      1.060609\n      -1.027859\n      1.161271\n      -1.161271\n      -32.238930\n      -0.625409\n      -0.632136\n      51.0\n      0.034916\n      0.001219\n      0.633099\n      -0.577443\n      0.684264\n      -0.684264\n      45.496464\n      0.672017\n      0.892088\n      51.0\n      0.311601\n      0.097095\n      0.944942\n      1.600711\n      1.600711\n      0.630026\n      -16.706364\n      -1.150480\n      -0.327576\n      51.0\n      1.040363\n      1.082355\n      1.090716\n      1.165667\n      1.251064\n      -1.251064\n      -5.941157\n      -0.304926\n      -0.116493\n      51.0\n      0.311124\n      0.096798\n      0.332218\n      0.396875\n      0.396875\n      -0.387906\n      -29.257990\n      -0.564004\n      -0.573686\n      51.0\n      0.071533\n      0.005117\n      0.578129\n      -0.337868\n      0.652336\n      -0.652336\n      -76.428627\n      -1.494161\n      -1.498600\n      51.0\n      0.042848\n      0.001836\n      1.499213\n      -1.412966\n      1.614412\n      -1.614412\n      -44.839191\n      -0.880543\n      -0.879200\n      51.0\n      0.030976\n      0.000959\n      0.879745\n      -0.823003\n      0.934714\n      -0.934714\n      41.487835\n      0.517329\n      0.813487\n      51.0\n      0.446360\n      0.199238\n      0.927900\n      1.735987\n      1.735987\n      0.444740\n      -18.961103\n      -1.415916\n      -0.371786\n      51.0\n      1.268295\n      1.608572\n      1.321665\n      1.450674\n      1.501040\n      -1.501040\n      -14.482611\n      -0.537522\n      -0.283973\n      51.0\n      0.351348\n      0.123445\n      0.451759\n      0.273131\n      0.644848\n      -0.644848\n      1.0\n    \n    \n      4\n      -32.628990\n      -0.644253\n      -0.639784\n      51.0\n      0.020251\n      0.000410\n      0.640105\n      -0.572807\n      0.679860\n      -0.679860\n      -89.749023\n      -1.733067\n      -1.759785\n      51.0\n      0.056407\n      0.003182\n      1.760689\n      -1.711828\n      1.927346\n      -1.927346\n      -32.873745\n      -0.624872\n      -0.644583\n      51.0\n      0.038699\n      0.001498\n      0.645744\n      -0.606139\n      0.737429\n      -0.737429\n      53.582626\n      0.669528\n      1.050640\n      51.0\n      0.652160\n      0.425313\n      1.236591\n      2.193117\n      2.193117\n      0.406379\n      -22.418583\n      -1.047039\n      -0.439580\n      51.0\n      1.489712\n      2.219241\n      1.553213\n      1.570286\n      1.922065\n      -1.922065\n      -22.601217\n      -0.572285\n      -0.443161\n      51.0\n      0.230570\n      0.053163\n      0.499554\n      -0.027844\n      0.701309\n      -0.701309\n      -33.820431\n      -0.664035\n      -0.663146\n      51.0\n      0.006335\n      0.000040\n      0.663176\n      -0.645632\n      0.673221\n      -0.673221\n      -39.140232\n      -0.745812\n      -0.767456\n      51.0\n      0.040633\n      0.001651\n      0.768530\n      -0.724313\n      0.849395\n      -0.849395\n      -5.892992\n      -0.106280\n      -0.115549\n      51.0\n      0.031008\n      0.000961\n      0.119637\n      -0.072815\n      0.166745\n      -0.166745\n      41.573116\n      0.809982\n      0.815159\n      51.0\n      0.202847\n      0.041147\n      0.840019\n      1.217381\n      1.217381\n      0.599070\n      -13.143985\n      -0.537164\n      -0.257725\n      51.0\n      0.616849\n      0.380502\n      0.668524\n      0.673150\n      0.841494\n      -0.841494\n      -6.945975\n      -0.156077\n      -0.136196\n      51.0\n      0.042467\n      0.001803\n      0.142663\n      -0.047679\n      0.181731\n      -0.181731\n      -35.198357\n      -0.694434\n      -0.690164\n      51.0\n      0.009910\n      0.000098\n      0.690235\n      -0.670637\n      0.706775\n      -0.706775\n      -71.361565\n      -1.372697\n      -1.399246\n      51.0\n      0.053486\n      0.002861\n      1.400268\n      -1.343534\n      1.580812\n      -1.580812\n      -23.765221\n      -0.452385\n      -0.465985\n      51.0\n      0.034227\n      0.001171\n      0.467240\n      -0.432714\n      0.554767\n      -0.554767\n      50.013210\n      0.789709\n      0.980651\n      51.0\n      0.437645\n      0.191533\n      1.073876\n      1.828212\n      1.828212\n      0.564621\n      -18.617342\n      -0.836133\n      -0.365046\n      51.0\n      1.162697\n      1.351865\n      1.218656\n      1.316921\n      1.487885\n      -1.487885\n      -17.978405\n      -0.443914\n      -0.352518\n      51.0\n      0.151959\n      0.023092\n      0.383875\n      -0.074808\n      0.500007\n      -0.500007\n      -28.447517\n      -0.551409\n      -0.557794\n      51.0\n      0.028066\n      0.000788\n      0.558500\n      -0.505120\n      0.621642\n      -0.621642\n      -78.526917\n      -1.509886\n      -1.539744\n      51.0\n      0.057831\n      0.003344\n      1.540829\n      -1.480418\n      1.671857\n      -1.671857\n      -33.922092\n      -0.656946\n      -0.665139\n      51.0\n      0.042927\n      0.001843\n      0.666523\n      -0.610782\n      0.774058\n      -0.774058\n      53.689266\n      0.806081\n      1.052731\n      51.0\n      0.530544\n      0.281477\n      1.178863\n      2.034978\n      2.034978\n      0.476695\n      -20.115297\n      -0.966044\n      -0.394418\n      51.0\n      1.327914\n      1.763356\n      1.385251\n      1.476527\n      1.724308\n      -1.724308\n      -23.407701\n      -0.512247\n      -0.458975\n      51.0\n      0.164467\n      0.027049\n      0.487552\n      -0.137660\n      0.776553\n      -0.776553\n      1.0\n    \n    \n      5\n      -24.176491\n      -0.488977\n      -0.474049\n      51.0\n      0.043168\n      0.001863\n      0.476010\n      -0.366197\n      0.527277\n      -0.527277\n      -87.327805\n      -1.710973\n      -1.712310\n      51.0\n      0.031561\n      0.000996\n      1.712601\n      -1.631805\n      1.772976\n      -1.772976\n      -21.451313\n      -0.408822\n      -0.420614\n      51.0\n      0.032761\n      0.001073\n      0.421888\n      -0.376194\n      0.493162\n      -0.493162\n      58.397629\n      0.748125\n      1.145051\n      51.0\n      0.692690\n      0.479820\n      1.338269\n      2.103378\n      2.103378\n      0.480623\n      -48.922287\n      -1.675132\n      -0.959261\n      51.0\n      0.974441\n      0.949535\n      1.367375\n      0.852968\n      1.817363\n      -1.817363\n      -29.856304\n      -0.640901\n      -0.585418\n      51.0\n      0.124195\n      0.015424\n      0.598447\n      -0.312080\n      0.750260\n      -0.750260\n      -27.958775\n      -0.565279\n      -0.548211\n      51.0\n      0.035193\n      0.001239\n      0.549340\n      -0.492060\n      0.583746\n      -0.583746\n      -37.864830\n      -0.732618\n      -0.742448\n      51.0\n      0.041091\n      0.001688\n      0.743584\n      -0.692059\n      0.802448\n      -0.802448\n      -0.649227\n      -0.005362\n      -0.012730\n      51.0\n      0.039293\n      0.001544\n      0.041304\n      0.035067\n      0.069732\n      -0.069732\n      43.424145\n      0.733217\n      0.851454\n      51.0\n      0.218725\n      0.047841\n      0.879099\n      1.171012\n      1.171012\n      0.632703\n      -21.846182\n      -0.721695\n      -0.428356\n      51.0\n      0.389760\n      0.151912\n      0.579139\n      0.287935\n      0.756129\n      -0.756129\n      -13.696244\n      -0.290477\n      -0.268554\n      51.0\n      0.049435\n      0.002444\n      0.273066\n      -0.169651\n      0.314362\n      -0.314362\n      -28.335342\n      -0.570419\n      -0.555595\n      51.0\n      0.031328\n      0.000981\n      0.556478\n      -0.508227\n      0.598616\n      -0.598616\n      -67.415817\n      -1.322900\n      -1.321879\n      51.0\n      0.020617\n      0.000425\n      1.322040\n      -1.252832\n      1.356348\n      -1.356348\n      -14.509363\n      -0.280158\n      -0.284497\n      51.0\n      0.027159\n      0.000738\n      0.285791\n      -0.237379\n      0.326241\n      -0.326241\n      54.223145\n      0.808409\n      1.063199\n      51.0\n      0.487772\n      0.237921\n      1.169749\n      1.747880\n      1.747880\n      0.582484\n      -38.194836\n      -1.286616\n      -0.748918\n      51.0\n      0.720042\n      0.518461\n      1.038913\n      0.589410\n      1.341641\n      -1.341641\n      -24.306427\n      -0.513999\n      -0.476597\n      51.0\n      0.113742\n      0.012937\n      0.489981\n      -0.236320\n      0.606179\n      -0.606179\n      -20.204275\n      -0.414832\n      -0.396162\n      51.0\n      0.036496\n      0.001332\n      0.397840\n      -0.328904\n      0.434622\n      -0.434622\n      -71.983742\n      -1.408678\n      -1.411446\n      51.0\n      0.031827\n      0.001013\n      1.411805\n      -1.346405\n      1.477625\n      -1.477625\n      -23.991568\n      -0.466880\n      -0.470423\n      51.0\n      0.027061\n      0.000732\n      0.471201\n      -0.430881\n      0.532167\n      -0.532167\n      53.270744\n      0.767735\n      1.044524\n      51.0\n      0.671843\n      0.451373\n      1.241936\n      2.002148\n      2.002148\n      0.376754\n      -38.753330\n      -1.399783\n      -0.759869\n      51.0\n      0.882002\n      0.777927\n      1.164186\n      0.885538\n      1.513484\n      -1.513484\n      -31.401262\n      -0.670306\n      -0.615711\n      51.0\n      0.152118\n      0.023140\n      0.634224\n      -0.286498\n      0.862852\n      -0.862852\n      2.0\n    \n    \n      6\n      -6.016619\n      -0.559591\n      -0.117973\n      51.0\n      0.587422\n      0.345064\n      0.599151\n      0.673467\n      0.863061\n      -0.863061\n      -59.364861\n      -1.720628\n      -1.164017\n      51.0\n      0.790454\n      0.624818\n      1.407037\n      0.057125\n      1.929770\n      -1.929770\n      -42.813259\n      -0.903392\n      -0.839476\n      51.0\n      0.224383\n      0.050348\n      0.868946\n      -0.450092\n      1.215354\n      -1.215354\n      23.519859\n      0.432229\n      0.461174\n      51.0\n      0.124770\n      0.015568\n      0.477754\n      0.710500\n      0.710500\n      0.220367\n      -16.209703\n      0.118847\n      -0.317837\n      51.0\n      1.314500\n      1.727910\n      1.352380\n      1.112054\n      1.939464\n      -1.939464\n      -37.199425\n      -0.725223\n      -0.729400\n      51.0\n      0.380468\n      0.144756\n      0.822667\n      -0.163260\n      1.562108\n      -1.562108\n      -28.374754\n      -0.577443\n      -0.556368\n      51.0\n      0.082587\n      0.006821\n      0.562464\n      -0.426011\n      0.700476\n      -0.700476\n      -33.831715\n      -0.692064\n      -0.663367\n      51.0\n      0.055319\n      0.003060\n      0.665670\n      -0.562892\n      0.785335\n      -0.785335\n      -9.785879\n      -0.120882\n      -0.191880\n      51.0\n      0.189782\n      0.036017\n      0.269880\n      0.029362\n      0.480300\n      -0.480300\n      31.432211\n      0.615669\n      0.616318\n      51.0\n      0.047582\n      0.002264\n      0.618152\n      0.724455\n      0.724455\n      0.508044\n      -17.988781\n      -0.505427\n      -0.352721\n      51.0\n      0.404562\n      0.163670\n      0.536733\n      0.349224\n      0.806139\n      -0.806139\n      -27.356274\n      -0.701033\n      -0.536398\n      51.0\n      0.296890\n      0.088144\n      0.613079\n      -0.133365\n      0.880810\n      -0.880810\n      -17.563057\n      -0.612013\n      -0.344374\n      51.0\n      0.363105\n      0.131845\n      0.500438\n      0.263199\n      0.805357\n      -0.805357\n      -51.258610\n      -1.292328\n      -1.005071\n      51.0\n      0.465966\n      0.217124\n      1.107832\n      -0.286899\n      1.439254\n      -1.439254\n      -32.161274\n      -0.654616\n      -0.630613\n      51.0\n      0.213219\n      0.045462\n      0.665684\n      -0.275235\n      1.050876\n      -1.050876\n      27.698662\n      0.561557\n      0.543111\n      51.0\n      0.077043\n      0.005936\n      0.548548\n      0.716459\n      0.716459\n      0.435221\n      -14.300674\n      -0.091825\n      -0.280405\n      51.0\n      1.000590\n      1.001181\n      1.039138\n      0.962385\n      1.426472\n      -1.426472\n      -35.823841\n      -0.588274\n      -0.702428\n      51.0\n      0.258499\n      0.066822\n      0.748483\n      -0.320927\n      1.462986\n      -1.462986\n      -12.420489\n      -0.572502\n      -0.243539\n      51.0\n      0.496830\n      0.246840\n      0.553309\n      0.556555\n      0.795882\n      -0.795882\n      -52.548584\n      -1.538987\n      -1.030364\n      51.0\n      0.756947\n      0.572969\n      1.278523\n      0.127194\n      1.766567\n      -1.766567\n      -39.116135\n      -0.778130\n      -0.766983\n      51.0\n      0.221984\n      0.049277\n      0.798461\n      -0.459320\n      1.228481\n      -1.228481\n      22.935024\n      0.383720\n      0.449706\n      51.0\n      0.130926\n      0.017142\n      0.468377\n      0.765694\n      0.765694\n      0.272558\n      -14.095066\n      0.043224\n      -0.276374\n      51.0\n      1.124726\n      1.265009\n      1.158184\n      0.981844\n      1.666249\n      -1.666249\n      -37.573940\n      -0.750010\n      -0.736744\n      51.0\n      0.329468\n      0.108549\n      0.807057\n      -0.145154\n      1.427878\n      -1.427878\n      6.0\n    \n    \n      7\n      -30.054667\n      -0.586762\n      -0.589307\n      51.0\n      0.026791\n      0.000718\n      0.589916\n      -0.537303\n      0.637069\n      -0.637069\n      -89.805847\n      -1.763582\n      -1.760899\n      51.0\n      0.051333\n      0.002635\n      1.761647\n      -1.595123\n      1.831372\n      -1.831372\n      -30.299511\n      -0.599651\n      -0.594108\n      51.0\n      0.017791\n      0.000317\n      0.594374\n      -0.555780\n      0.624197\n      -0.624197\n      57.054878\n      0.590898\n      1.118723\n      51.0\n      0.690255\n      0.476451\n      1.314531\n      2.181025\n      2.181025\n      0.523483\n      -54.861839\n      -1.783685\n      -1.075722\n      51.0\n      1.023313\n      1.047169\n      1.484705\n      0.780975\n      1.866018\n      -1.866018\n      -30.463106\n      -0.687972\n      -0.597316\n      51.0\n      0.233482\n      0.054514\n      0.641327\n      -0.052382\n      0.885776\n      -0.885776\n      -31.239834\n      -0.610331\n      -0.612546\n      51.0\n      0.008938\n      0.000080\n      0.612611\n      -0.589627\n      0.626576\n      -0.626576\n      -35.954945\n      -0.707961\n      -0.704999\n      51.0\n      0.009692\n      0.000094\n      0.705065\n      -0.681811\n      0.723934\n      -0.723934\n      -1.693566\n      -0.034172\n      -0.033207\n      51.0\n      0.005121\n      0.000026\n      0.033600\n      -0.025034\n      0.043316\n      -0.043316\n      41.362114\n      0.611083\n      0.811022\n      51.0\n      0.266158\n      0.070840\n      0.853579\n      1.230610\n      1.230610\n      0.590663\n      -27.009262\n      -0.764834\n      -0.529593\n      51.0\n      0.352210\n      0.124052\n      0.636019\n      0.164998\n      0.783654\n      -0.783654\n      -4.671733\n      -0.120645\n      -0.091603\n      51.0\n      0.039557\n      0.001565\n      0.099779\n      0.012086\n      0.132251\n      -0.132251\n      -33.829697\n      -0.661835\n      -0.663327\n      51.0\n      0.016654\n      0.000277\n      0.663536\n      -0.629331\n      0.698473\n      -0.698473\n      -67.871452\n      -1.321654\n      -1.330813\n      51.0\n      0.049303\n      0.002431\n      1.331726\n      -1.228574\n      1.440540\n      -1.440540\n      -18.070618\n      -0.351222\n      -0.354326\n      51.0\n      0.036134\n      0.001306\n      0.356164\n      -0.280387\n      0.420330\n      -0.420330\n      52.747173\n      0.672712\n      1.034258\n      51.0\n      0.501497\n      0.251499\n      1.149430\n      1.832273\n      1.832273\n      0.611130\n      -44.644840\n      -1.400284\n      -0.875389\n      51.0\n      0.739913\n      0.547472\n      1.146201\n      0.524037\n      1.451716\n      -1.451716\n      -19.645128\n      -0.469470\n      -0.385199\n      51.0\n      0.163351\n      0.026683\n      0.418403\n      -0.018850\n      0.508339\n      -0.508339\n      -25.314772\n      -0.484376\n      -0.496368\n      51.0\n      0.038528\n      0.001484\n      0.497861\n      -0.424774\n      0.636847\n      -0.636847\n      -77.000114\n      -1.515560\n      -1.509806\n      51.0\n      0.045546\n      0.002074\n      1.510493\n      -1.345332\n      1.600864\n      -1.600864\n      -31.448179\n      -0.617746\n      -0.616631\n      51.0\n      0.018259\n      0.000333\n      0.616901\n      -0.558595\n      0.647373\n      -0.647373\n      52.555771\n      0.479092\n      1.030505\n      51.0\n      0.712963\n      0.508317\n      1.253099\n      2.134455\n      2.134455\n      0.432490\n      -51.433918\n      -1.547762\n      -1.008508\n      51.0\n      0.842653\n      0.710063\n      1.314212\n      0.560371\n      1.694775\n      -1.694775\n      -30.186773\n      -0.691490\n      -0.591898\n      51.0\n      0.220352\n      0.048555\n      0.631583\n      -0.083429\n      0.843667\n      -0.843667\n      2.0\n    \n    \n      8\n      -16.060301\n      -0.667104\n      -0.314908\n      51.0\n      0.627565\n      0.393838\n      0.702143\n      0.812297\n      0.983708\n      -0.983708\n      -74.788139\n      -1.897522\n      -1.466434\n      51.0\n      0.800985\n      0.641577\n      1.670930\n      -0.321444\n      2.274097\n      -2.274097\n      -59.309460\n      -1.029285\n      -1.162931\n      51.0\n      0.350030\n      0.122521\n      1.214466\n      -0.699534\n      1.934789\n      -1.934789\n      50.793800\n      0.795298\n      0.995957\n      51.0\n      0.342537\n      0.117332\n      1.053215\n      1.691689\n      1.691689\n      0.662314\n      -43.320175\n      -1.703624\n      -0.849415\n      51.0\n      1.443293\n      2.083095\n      1.674694\n      1.171774\n      2.230080\n      -2.230080\n      -44.213379\n      -0.858646\n      -0.866929\n      51.0\n      0.205225\n      0.042117\n      0.890889\n      -0.585371\n      1.341791\n      -1.341791\n      -34.511520\n      -0.695397\n      -0.676696\n      51.0\n      0.136958\n      0.018757\n      0.690417\n      -0.398315\n      0.945305\n      -0.945305\n      -35.628326\n      -0.752094\n      -0.698595\n      51.0\n      0.152363\n      0.023214\n      0.715017\n      -0.438759\n      0.870392\n      -0.870392\n      -21.092607\n      -0.313079\n      -0.413581\n      51.0\n      0.179414\n      0.032189\n      0.450820\n      -0.235212\n      0.879265\n      -0.879265\n      42.521458\n      0.793477\n      0.833754\n      51.0\n      0.112848\n      0.012735\n      0.841356\n      1.138605\n      1.138605\n      0.712911\n      -31.566500\n      -0.889622\n      -0.618951\n      51.0\n      0.374500\n      0.140250\n      0.723430\n      0.124171\n      0.946994\n      -0.946994\n      -19.891979\n      -0.189327\n      -0.390039\n      51.0\n      0.288866\n      0.083444\n      0.485360\n      -0.155909\n      0.919305\n      -0.919305\n      -23.640608\n      -0.695592\n      -0.463541\n      51.0\n      0.435593\n      0.189741\n      0.636091\n      0.436059\n      0.912495\n      -0.912495\n      -62.360683\n      -1.449398\n      -1.222759\n      51.0\n      0.569461\n      0.324286\n      1.348860\n      -0.391525\n      1.833394\n      -1.833394\n      -48.145874\n      -0.905404\n      -0.944037\n      51.0\n      0.225033\n      0.050640\n      0.970487\n      -0.683651\n      1.391908\n      -1.391908\n      48.720406\n      0.832892\n      0.955302\n      51.0\n      0.268162\n      0.071911\n      0.992226\n      1.627967\n      1.627967\n      0.656658\n      -37.167072\n      -1.435417\n      -0.728766\n      51.0\n      1.070713\n      1.146427\n      1.295194\n      0.923269\n      1.716019\n      -1.716019\n      -33.509415\n      -0.583996\n      -0.657047\n      51.0\n      0.171576\n      0.029438\n      0.679080\n      -0.394781\n      1.083638\n      -1.083638\n      -19.663610\n      -0.613898\n      -0.385561\n      51.0\n      0.564985\n      0.319208\n      0.684007\n      0.629170\n      1.007365\n      -1.007365\n      -70.090225\n      -1.704903\n      -1.374318\n      51.0\n      0.624232\n      0.389666\n      1.509442\n      -0.415971\n      2.108225\n      -2.108225\n      -52.366474\n      -0.926542\n      -1.026794\n      51.0\n      0.287308\n      0.082546\n      1.066232\n      -0.679140\n      1.731807\n      -1.731807\n      47.941799\n      0.909545\n      0.940035\n      51.0\n      0.348823\n      0.121677\n      1.002668\n      1.692501\n      1.692501\n      0.545740\n      -40.263035\n      -1.575590\n      -0.789471\n      51.0\n      1.202905\n      1.446981\n      1.438835\n      0.958755\n      2.064477\n      -2.064477\n      -40.953861\n      -0.808996\n      -0.803017\n      51.0\n      0.155434\n      0.024160\n      0.817922\n      -0.565620\n      1.240218\n      -1.240218\n      6.0\n    \n    \n      9\n      -35.325958\n      -0.716327\n      -0.692666\n      51.0\n      0.746616\n      0.557435\n      1.018440\n      0.675705\n      2.227856\n      -2.227856\n      -50.240242\n      -1.649973\n      -0.985103\n      51.0\n      0.882919\n      0.779545\n      1.322865\n      0.595470\n      1.769741\n      -1.769741\n      -43.784492\n      -0.854625\n      -0.858519\n      51.0\n      0.344355\n      0.118581\n      0.925006\n      0.057911\n      1.537225\n      -1.537225\n      42.223888\n      0.788022\n      0.827919\n      51.0\n      0.715912\n      0.512530\n      1.094523\n      2.239191\n      2.239191\n      -0.598289\n      -54.839199\n      -1.685413\n      -1.075278\n      51.0\n      0.818133\n      0.669341\n      1.351135\n      0.612057\n      1.832077\n      -1.832077\n      -20.362839\n      -0.457162\n      -0.399271\n      51.0\n      0.460404\n      0.211972\n      0.609417\n      0.856180\n      1.371877\n      -1.371877\n      -37.779381\n      -0.721153\n      -0.740772\n      51.0\n      0.192792\n      0.037169\n      0.765449\n      -0.347428\n      1.109891\n      -1.109891\n      -25.893764\n      -0.660764\n      -0.507721\n      51.0\n      0.221849\n      0.049217\n      0.554074\n      -0.062809\n      0.701248\n      -0.701248\n      -16.999693\n      -0.266673\n      -0.333327\n      51.0\n      0.174279\n      0.030373\n      0.376139\n      -0.160579\n      0.760730\n      -0.760730\n      35.923653\n      0.602151\n      0.704385\n      51.0\n      0.173165\n      0.029986\n      0.725358\n      1.077855\n      1.077855\n      0.535991\n      -33.310738\n      -0.766126\n      -0.653152\n      51.0\n      0.188555\n      0.035553\n      0.679824\n      -0.233124\n      0.794142\n      -0.794142\n      3.365476\n      0.101030\n      0.065990\n      51.0\n      0.211689\n      0.044812\n      0.221736\n      0.411540\n      0.411540\n      -0.391043\n      -37.174194\n      -0.756994\n      -0.728906\n      51.0\n      0.534179\n      0.285348\n      0.903688\n      0.445568\n      1.704221\n      -1.704221\n      -40.664139\n      -1.260805\n      -0.797336\n      51.0\n      0.613188\n      0.376000\n      1.005855\n      0.267926\n      1.335273\n      -1.335273\n      -33.628231\n      -0.656351\n      -0.659377\n      51.0\n      0.213429\n      0.045552\n      0.693058\n      -0.345712\n      1.234546\n      -1.234546\n      39.727489\n      0.724312\n      0.778970\n      51.0\n      0.488670\n      0.238798\n      0.919561\n      1.743218\n      1.743218\n      -0.255513\n      -44.954082\n      -1.299834\n      -0.881453\n      51.0\n      0.540043\n      0.291646\n      1.033733\n      0.210265\n      1.411317\n      -1.411317\n      -10.318247\n      -0.179946\n      -0.202319\n      51.0\n      0.344015\n      0.118346\n      0.399098\n      0.628464\n      1.018573\n      -1.018573\n      -37.053024\n      -0.741383\n      -0.726530\n      51.0\n      0.672777\n      0.452630\n      0.990189\n      0.653344\n      2.026216\n      -2.026216\n      -46.345230\n      -1.434940\n      -0.908730\n      51.0\n      0.742306\n      0.551018\n      1.173375\n      0.604051\n      1.582865\n      -1.582865\n      -40.739742\n      -0.794713\n      -0.798818\n      51.0\n      0.303034\n      0.091830\n      0.854366\n      0.037086\n      1.368740\n      -1.368740\n      39.737450\n      0.763644\n      0.779166\n      51.0\n      0.626881\n      0.392980\n      1.000040\n      1.959837\n      1.959837\n      -0.491782\n      -51.506882\n      -1.503687\n      -1.009939\n      51.0\n      0.735450\n      0.540887\n      1.249345\n      0.475179\n      1.694034\n      -1.694034\n      -20.552256\n      -0.483298\n      -0.402985\n      51.0\n      0.397737\n      0.158195\n      0.566209\n      0.663350\n      1.459374\n      -1.459374\n      4.0\n    \n  \n\n\n\n\nx_cat, x_cont, yb = first(dls.train)\nx_cont[:10]\n\ntensor([[ 0.2804,  0.4747,  0.2804,  ..., -0.4816, -0.3325,  0.0887],\n        [-0.3196, -0.4142, -0.3196,  ..., -0.0979, -0.7659,  0.4576],\n        [-0.0764, -0.1685, -0.0764,  ..., -0.1120,  0.7963, -0.8724],\n        ...,\n        [-1.3950, -0.6392, -1.3950,  ...,  0.6344,  1.4236, -1.4065],\n        [-0.0188,  0.0513, -0.0188,  ..., -1.4496, -0.2021, -0.0224],\n        [-0.3864, -0.1467, -0.3864,  ...,  0.3099,  1.5548, -1.5182]])"
  },
  {
    "objectID": "models.xresnet1d.html",
    "href": "models.xresnet1d.html",
    "title": "XResNet1d",
    "section": "",
    "text": "This is a modified version of fastai’s XResNet model in github\n\n\nsource\n\nxresnet1d50_deeper\n\n xresnet1d50_deeper (c_in, c_out, act=<class\n                     'torch.nn.modules.activation.ReLU'>, stride=1,\n                     groups=1, reduction=None, nh1=None, nh2=None,\n                     dw=False, g2=1, sa=False, sym=False,\n                     norm_type=<NormType.Batch: 1>, act_cls=<class\n                     'torch.nn.modules.activation.ReLU'>, ndim=2, ks=3,\n                     pool=<function AvgPool>, pool_first=True,\n                     padding=None, bias=None, bn_1st=True,\n                     transpose=False, init='auto', xtra=None,\n                     bias_std=0.01, dilation:Union[int,Tuple[int,int]]=1,\n                     padding_mode:str='zeros', device=None, dtype=None)\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nc_in\n\n\n\n\n\nc_out\n\n\n\n\n\nact\ntype\nReLU\n\n\n\nstride\nint\n1\n\n\n\ngroups\nint\n1\n\n\n\nreduction\nNoneType\nNone\n\n\n\nnh1\nNoneType\nNone\n\n\n\nnh2\nNoneType\nNone\n\n\n\ndw\nbool\nFalse\n\n\n\ng2\nint\n1\n\n\n\nsa\nbool\nFalse\n\n\n\nsym\nbool\nFalse\n\n\n\nnorm_type\nNormType\nNormType.Batch\n\n\n\nact_cls\ntype\nReLU\n\n\n\nndim\nint\n2\n\n\n\nks\nint\n3\n\n\n\npool\nfunction\nAvgPool\n\n\n\npool_first\nbool\nTrue\n\n\n\npadding\nNoneType\nNone\n\n\n\nbias\nNoneType\nNone\n\n\n\nbn_1st\nbool\nTrue\n\n\n\ntranspose\nbool\nFalse\n\n\n\ninit\nstr\nauto\n\n\n\nxtra\nNoneType\nNone\n\n\n\nbias_std\nfloat\n0.01\n\n\n\ndilation\ntyping.Union[int, typing.Tuple[int, int]]\n1\n\n\n\npadding_mode\nstr\nzeros\nTODO: refine this type\n\n\ndevice\nNoneType\nNone\n\n\n\ndtype\nNoneType\nNone\n\n\n\n\n\nsource\n\n\nxresnet1d34_deeper\n\n xresnet1d34_deeper (c_in, c_out, act=<class\n                     'torch.nn.modules.activation.ReLU'>, stride=1,\n                     groups=1, reduction=None, nh1=None, nh2=None,\n                     dw=False, g2=1, sa=False, sym=False,\n                     norm_type=<NormType.Batch: 1>, act_cls=<class\n                     'torch.nn.modules.activation.ReLU'>, ndim=2, ks=3,\n                     pool=<function AvgPool>, pool_first=True,\n                     padding=None, bias=None, bn_1st=True,\n                     transpose=False, init='auto', xtra=None,\n                     bias_std=0.01, dilation:Union[int,Tuple[int,int]]=1,\n                     padding_mode:str='zeros', device=None, dtype=None)\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nc_in\n\n\n\n\n\nc_out\n\n\n\n\n\nact\ntype\nReLU\n\n\n\nstride\nint\n1\n\n\n\ngroups\nint\n1\n\n\n\nreduction\nNoneType\nNone\n\n\n\nnh1\nNoneType\nNone\n\n\n\nnh2\nNoneType\nNone\n\n\n\ndw\nbool\nFalse\n\n\n\ng2\nint\n1\n\n\n\nsa\nbool\nFalse\n\n\n\nsym\nbool\nFalse\n\n\n\nnorm_type\nNormType\nNormType.Batch\n\n\n\nact_cls\ntype\nReLU\n\n\n\nndim\nint\n2\n\n\n\nks\nint\n3\n\n\n\npool\nfunction\nAvgPool\n\n\n\npool_first\nbool\nTrue\n\n\n\npadding\nNoneType\nNone\n\n\n\nbias\nNoneType\nNone\n\n\n\nbn_1st\nbool\nTrue\n\n\n\ntranspose\nbool\nFalse\n\n\n\ninit\nstr\nauto\n\n\n\nxtra\nNoneType\nNone\n\n\n\nbias_std\nfloat\n0.01\n\n\n\ndilation\ntyping.Union[int, typing.Tuple[int, int]]\n1\n\n\n\npadding_mode\nstr\nzeros\nTODO: refine this type\n\n\ndevice\nNoneType\nNone\n\n\n\ndtype\nNoneType\nNone\n\n\n\n\n\nsource\n\n\nxresnet1d18_deeper\n\n xresnet1d18_deeper (c_in, c_out, act=<class\n                     'torch.nn.modules.activation.ReLU'>, stride=1,\n                     groups=1, reduction=None, nh1=None, nh2=None,\n                     dw=False, g2=1, sa=False, sym=False,\n                     norm_type=<NormType.Batch: 1>, act_cls=<class\n                     'torch.nn.modules.activation.ReLU'>, ndim=2, ks=3,\n                     pool=<function AvgPool>, pool_first=True,\n                     padding=None, bias=None, bn_1st=True,\n                     transpose=False, init='auto', xtra=None,\n                     bias_std=0.01, dilation:Union[int,Tuple[int,int]]=1,\n                     padding_mode:str='zeros', device=None, dtype=None)\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nc_in\n\n\n\n\n\nc_out\n\n\n\n\n\nact\ntype\nReLU\n\n\n\nstride\nint\n1\n\n\n\ngroups\nint\n1\n\n\n\nreduction\nNoneType\nNone\n\n\n\nnh1\nNoneType\nNone\n\n\n\nnh2\nNoneType\nNone\n\n\n\ndw\nbool\nFalse\n\n\n\ng2\nint\n1\n\n\n\nsa\nbool\nFalse\n\n\n\nsym\nbool\nFalse\n\n\n\nnorm_type\nNormType\nNormType.Batch\n\n\n\nact_cls\ntype\nReLU\n\n\n\nndim\nint\n2\n\n\n\nks\nint\n3\n\n\n\npool\nfunction\nAvgPool\n\n\n\npool_first\nbool\nTrue\n\n\n\npadding\nNoneType\nNone\n\n\n\nbias\nNoneType\nNone\n\n\n\nbn_1st\nbool\nTrue\n\n\n\ntranspose\nbool\nFalse\n\n\n\ninit\nstr\nauto\n\n\n\nxtra\nNoneType\nNone\n\n\n\nbias_std\nfloat\n0.01\n\n\n\ndilation\ntyping.Union[int, typing.Tuple[int, int]]\n1\n\n\n\npadding_mode\nstr\nzeros\nTODO: refine this type\n\n\ndevice\nNoneType\nNone\n\n\n\ndtype\nNoneType\nNone\n\n\n\n\n\nsource\n\n\nxresnet1d50_deep\n\n xresnet1d50_deep (c_in, c_out, act=<class\n                   'torch.nn.modules.activation.ReLU'>, stride=1,\n                   groups=1, reduction=None, nh1=None, nh2=None, dw=False,\n                   g2=1, sa=False, sym=False, norm_type=<NormType.Batch:\n                   1>, act_cls=<class 'torch.nn.modules.activation.ReLU'>,\n                   ndim=2, ks=3, pool=<function AvgPool>, pool_first=True,\n                   padding=None, bias=None, bn_1st=True, transpose=False,\n                   init='auto', xtra=None, bias_std=0.01,\n                   dilation:Union[int,Tuple[int,int]]=1,\n                   padding_mode:str='zeros', device=None, dtype=None)\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nc_in\n\n\n\n\n\nc_out\n\n\n\n\n\nact\ntype\nReLU\n\n\n\nstride\nint\n1\n\n\n\ngroups\nint\n1\n\n\n\nreduction\nNoneType\nNone\n\n\n\nnh1\nNoneType\nNone\n\n\n\nnh2\nNoneType\nNone\n\n\n\ndw\nbool\nFalse\n\n\n\ng2\nint\n1\n\n\n\nsa\nbool\nFalse\n\n\n\nsym\nbool\nFalse\n\n\n\nnorm_type\nNormType\nNormType.Batch\n\n\n\nact_cls\ntype\nReLU\n\n\n\nndim\nint\n2\n\n\n\nks\nint\n3\n\n\n\npool\nfunction\nAvgPool\n\n\n\npool_first\nbool\nTrue\n\n\n\npadding\nNoneType\nNone\n\n\n\nbias\nNoneType\nNone\n\n\n\nbn_1st\nbool\nTrue\n\n\n\ntranspose\nbool\nFalse\n\n\n\ninit\nstr\nauto\n\n\n\nxtra\nNoneType\nNone\n\n\n\nbias_std\nfloat\n0.01\n\n\n\ndilation\ntyping.Union[int, typing.Tuple[int, int]]\n1\n\n\n\npadding_mode\nstr\nzeros\nTODO: refine this type\n\n\ndevice\nNoneType\nNone\n\n\n\ndtype\nNoneType\nNone\n\n\n\n\n\nsource\n\n\nxresnet1d34_deep\n\n xresnet1d34_deep (c_in, c_out, act=<class\n                   'torch.nn.modules.activation.ReLU'>, stride=1,\n                   groups=1, reduction=None, nh1=None, nh2=None, dw=False,\n                   g2=1, sa=False, sym=False, norm_type=<NormType.Batch:\n                   1>, act_cls=<class 'torch.nn.modules.activation.ReLU'>,\n                   ndim=2, ks=3, pool=<function AvgPool>, pool_first=True,\n                   padding=None, bias=None, bn_1st=True, transpose=False,\n                   init='auto', xtra=None, bias_std=0.01,\n                   dilation:Union[int,Tuple[int,int]]=1,\n                   padding_mode:str='zeros', device=None, dtype=None)\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nc_in\n\n\n\n\n\nc_out\n\n\n\n\n\nact\ntype\nReLU\n\n\n\nstride\nint\n1\n\n\n\ngroups\nint\n1\n\n\n\nreduction\nNoneType\nNone\n\n\n\nnh1\nNoneType\nNone\n\n\n\nnh2\nNoneType\nNone\n\n\n\ndw\nbool\nFalse\n\n\n\ng2\nint\n1\n\n\n\nsa\nbool\nFalse\n\n\n\nsym\nbool\nFalse\n\n\n\nnorm_type\nNormType\nNormType.Batch\n\n\n\nact_cls\ntype\nReLU\n\n\n\nndim\nint\n2\n\n\n\nks\nint\n3\n\n\n\npool\nfunction\nAvgPool\n\n\n\npool_first\nbool\nTrue\n\n\n\npadding\nNoneType\nNone\n\n\n\nbias\nNoneType\nNone\n\n\n\nbn_1st\nbool\nTrue\n\n\n\ntranspose\nbool\nFalse\n\n\n\ninit\nstr\nauto\n\n\n\nxtra\nNoneType\nNone\n\n\n\nbias_std\nfloat\n0.01\n\n\n\ndilation\ntyping.Union[int, typing.Tuple[int, int]]\n1\n\n\n\npadding_mode\nstr\nzeros\nTODO: refine this type\n\n\ndevice\nNoneType\nNone\n\n\n\ndtype\nNoneType\nNone\n\n\n\n\n\nsource\n\n\nxresnet1d18_deep\n\n xresnet1d18_deep (c_in, c_out, act=<class\n                   'torch.nn.modules.activation.ReLU'>, stride=1,\n                   groups=1, reduction=None, nh1=None, nh2=None, dw=False,\n                   g2=1, sa=False, sym=False, norm_type=<NormType.Batch:\n                   1>, act_cls=<class 'torch.nn.modules.activation.ReLU'>,\n                   ndim=2, ks=3, pool=<function AvgPool>, pool_first=True,\n                   padding=None, bias=None, bn_1st=True, transpose=False,\n                   init='auto', xtra=None, bias_std=0.01,\n                   dilation:Union[int,Tuple[int,int]]=1,\n                   padding_mode:str='zeros', device=None, dtype=None)\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nc_in\n\n\n\n\n\nc_out\n\n\n\n\n\nact\ntype\nReLU\n\n\n\nstride\nint\n1\n\n\n\ngroups\nint\n1\n\n\n\nreduction\nNoneType\nNone\n\n\n\nnh1\nNoneType\nNone\n\n\n\nnh2\nNoneType\nNone\n\n\n\ndw\nbool\nFalse\n\n\n\ng2\nint\n1\n\n\n\nsa\nbool\nFalse\n\n\n\nsym\nbool\nFalse\n\n\n\nnorm_type\nNormType\nNormType.Batch\n\n\n\nact_cls\ntype\nReLU\n\n\n\nndim\nint\n2\n\n\n\nks\nint\n3\n\n\n\npool\nfunction\nAvgPool\n\n\n\npool_first\nbool\nTrue\n\n\n\npadding\nNoneType\nNone\n\n\n\nbias\nNoneType\nNone\n\n\n\nbn_1st\nbool\nTrue\n\n\n\ntranspose\nbool\nFalse\n\n\n\ninit\nstr\nauto\n\n\n\nxtra\nNoneType\nNone\n\n\n\nbias_std\nfloat\n0.01\n\n\n\ndilation\ntyping.Union[int, typing.Tuple[int, int]]\n1\n\n\n\npadding_mode\nstr\nzeros\nTODO: refine this type\n\n\ndevice\nNoneType\nNone\n\n\n\ndtype\nNoneType\nNone\n\n\n\n\n\nsource\n\n\nxresnet1d152\n\n xresnet1d152 (c_in, c_out, act=<class\n               'torch.nn.modules.activation.ReLU'>, stride=1, groups=1,\n               reduction=None, nh1=None, nh2=None, dw=False, g2=1,\n               sa=False, sym=False, norm_type=<NormType.Batch: 1>,\n               act_cls=<class 'torch.nn.modules.activation.ReLU'>, ndim=2,\n               ks=3, pool=<function AvgPool>, pool_first=True,\n               padding=None, bias=None, bn_1st=True, transpose=False,\n               init='auto', xtra=None, bias_std=0.01,\n               dilation:Union[int,Tuple[int,int]]=1,\n               padding_mode:str='zeros', device=None, dtype=None)\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nc_in\n\n\n\n\n\nc_out\n\n\n\n\n\nact\ntype\nReLU\n\n\n\nstride\nint\n1\n\n\n\ngroups\nint\n1\n\n\n\nreduction\nNoneType\nNone\n\n\n\nnh1\nNoneType\nNone\n\n\n\nnh2\nNoneType\nNone\n\n\n\ndw\nbool\nFalse\n\n\n\ng2\nint\n1\n\n\n\nsa\nbool\nFalse\n\n\n\nsym\nbool\nFalse\n\n\n\nnorm_type\nNormType\nNormType.Batch\n\n\n\nact_cls\ntype\nReLU\n\n\n\nndim\nint\n2\n\n\n\nks\nint\n3\n\n\n\npool\nfunction\nAvgPool\n\n\n\npool_first\nbool\nTrue\n\n\n\npadding\nNoneType\nNone\n\n\n\nbias\nNoneType\nNone\n\n\n\nbn_1st\nbool\nTrue\n\n\n\ntranspose\nbool\nFalse\n\n\n\ninit\nstr\nauto\n\n\n\nxtra\nNoneType\nNone\n\n\n\nbias_std\nfloat\n0.01\n\n\n\ndilation\ntyping.Union[int, typing.Tuple[int, int]]\n1\n\n\n\npadding_mode\nstr\nzeros\nTODO: refine this type\n\n\ndevice\nNoneType\nNone\n\n\n\ndtype\nNoneType\nNone\n\n\n\n\n\nsource\n\n\nxresnet1d101\n\n xresnet1d101 (c_in, c_out, act=<class\n               'torch.nn.modules.activation.ReLU'>, stride=1, groups=1,\n               reduction=None, nh1=None, nh2=None, dw=False, g2=1,\n               sa=False, sym=False, norm_type=<NormType.Batch: 1>,\n               act_cls=<class 'torch.nn.modules.activation.ReLU'>, ndim=2,\n               ks=3, pool=<function AvgPool>, pool_first=True,\n               padding=None, bias=None, bn_1st=True, transpose=False,\n               init='auto', xtra=None, bias_std=0.01,\n               dilation:Union[int,Tuple[int,int]]=1,\n               padding_mode:str='zeros', device=None, dtype=None)\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nc_in\n\n\n\n\n\nc_out\n\n\n\n\n\nact\ntype\nReLU\n\n\n\nstride\nint\n1\n\n\n\ngroups\nint\n1\n\n\n\nreduction\nNoneType\nNone\n\n\n\nnh1\nNoneType\nNone\n\n\n\nnh2\nNoneType\nNone\n\n\n\ndw\nbool\nFalse\n\n\n\ng2\nint\n1\n\n\n\nsa\nbool\nFalse\n\n\n\nsym\nbool\nFalse\n\n\n\nnorm_type\nNormType\nNormType.Batch\n\n\n\nact_cls\ntype\nReLU\n\n\n\nndim\nint\n2\n\n\n\nks\nint\n3\n\n\n\npool\nfunction\nAvgPool\n\n\n\npool_first\nbool\nTrue\n\n\n\npadding\nNoneType\nNone\n\n\n\nbias\nNoneType\nNone\n\n\n\nbn_1st\nbool\nTrue\n\n\n\ntranspose\nbool\nFalse\n\n\n\ninit\nstr\nauto\n\n\n\nxtra\nNoneType\nNone\n\n\n\nbias_std\nfloat\n0.01\n\n\n\ndilation\ntyping.Union[int, typing.Tuple[int, int]]\n1\n\n\n\npadding_mode\nstr\nzeros\nTODO: refine this type\n\n\ndevice\nNoneType\nNone\n\n\n\ndtype\nNoneType\nNone\n\n\n\n\n\nsource\n\n\nxresnet1d50\n\n xresnet1d50 (c_in, c_out, act=<class 'torch.nn.modules.activation.ReLU'>,\n              stride=1, groups=1, reduction=None, nh1=None, nh2=None,\n              dw=False, g2=1, sa=False, sym=False,\n              norm_type=<NormType.Batch: 1>, act_cls=<class\n              'torch.nn.modules.activation.ReLU'>, ndim=2, ks=3,\n              pool=<function AvgPool>, pool_first=True, padding=None,\n              bias=None, bn_1st=True, transpose=False, init='auto',\n              xtra=None, bias_std=0.01,\n              dilation:Union[int,Tuple[int,int]]=1,\n              padding_mode:str='zeros', device=None, dtype=None)\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nc_in\n\n\n\n\n\nc_out\n\n\n\n\n\nact\ntype\nReLU\n\n\n\nstride\nint\n1\n\n\n\ngroups\nint\n1\n\n\n\nreduction\nNoneType\nNone\n\n\n\nnh1\nNoneType\nNone\n\n\n\nnh2\nNoneType\nNone\n\n\n\ndw\nbool\nFalse\n\n\n\ng2\nint\n1\n\n\n\nsa\nbool\nFalse\n\n\n\nsym\nbool\nFalse\n\n\n\nnorm_type\nNormType\nNormType.Batch\n\n\n\nact_cls\ntype\nReLU\n\n\n\nndim\nint\n2\n\n\n\nks\nint\n3\n\n\n\npool\nfunction\nAvgPool\n\n\n\npool_first\nbool\nTrue\n\n\n\npadding\nNoneType\nNone\n\n\n\nbias\nNoneType\nNone\n\n\n\nbn_1st\nbool\nTrue\n\n\n\ntranspose\nbool\nFalse\n\n\n\ninit\nstr\nauto\n\n\n\nxtra\nNoneType\nNone\n\n\n\nbias_std\nfloat\n0.01\n\n\n\ndilation\ntyping.Union[int, typing.Tuple[int, int]]\n1\n\n\n\npadding_mode\nstr\nzeros\nTODO: refine this type\n\n\ndevice\nNoneType\nNone\n\n\n\ndtype\nNoneType\nNone\n\n\n\n\n\nsource\n\n\nxresnet1d34\n\n xresnet1d34 (c_in, c_out, act=<class 'torch.nn.modules.activation.ReLU'>,\n              stride=1, groups=1, reduction=None, nh1=None, nh2=None,\n              dw=False, g2=1, sa=False, sym=False,\n              norm_type=<NormType.Batch: 1>, act_cls=<class\n              'torch.nn.modules.activation.ReLU'>, ndim=2, ks=3,\n              pool=<function AvgPool>, pool_first=True, padding=None,\n              bias=None, bn_1st=True, transpose=False, init='auto',\n              xtra=None, bias_std=0.01,\n              dilation:Union[int,Tuple[int,int]]=1,\n              padding_mode:str='zeros', device=None, dtype=None)\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nc_in\n\n\n\n\n\nc_out\n\n\n\n\n\nact\ntype\nReLU\n\n\n\nstride\nint\n1\n\n\n\ngroups\nint\n1\n\n\n\nreduction\nNoneType\nNone\n\n\n\nnh1\nNoneType\nNone\n\n\n\nnh2\nNoneType\nNone\n\n\n\ndw\nbool\nFalse\n\n\n\ng2\nint\n1\n\n\n\nsa\nbool\nFalse\n\n\n\nsym\nbool\nFalse\n\n\n\nnorm_type\nNormType\nNormType.Batch\n\n\n\nact_cls\ntype\nReLU\n\n\n\nndim\nint\n2\n\n\n\nks\nint\n3\n\n\n\npool\nfunction\nAvgPool\n\n\n\npool_first\nbool\nTrue\n\n\n\npadding\nNoneType\nNone\n\n\n\nbias\nNoneType\nNone\n\n\n\nbn_1st\nbool\nTrue\n\n\n\ntranspose\nbool\nFalse\n\n\n\ninit\nstr\nauto\n\n\n\nxtra\nNoneType\nNone\n\n\n\nbias_std\nfloat\n0.01\n\n\n\ndilation\ntyping.Union[int, typing.Tuple[int, int]]\n1\n\n\n\npadding_mode\nstr\nzeros\nTODO: refine this type\n\n\ndevice\nNoneType\nNone\n\n\n\ndtype\nNoneType\nNone\n\n\n\n\n\nsource\n\n\nxresnet1d18\n\n xresnet1d18 (c_in, c_out, act=<class 'torch.nn.modules.activation.ReLU'>,\n              stride=1, groups=1, reduction=None, nh1=None, nh2=None,\n              dw=False, g2=1, sa=False, sym=False,\n              norm_type=<NormType.Batch: 1>, act_cls=<class\n              'torch.nn.modules.activation.ReLU'>, ndim=2, ks=3,\n              pool=<function AvgPool>, pool_first=True, padding=None,\n              bias=None, bn_1st=True, transpose=False, init='auto',\n              xtra=None, bias_std=0.01,\n              dilation:Union[int,Tuple[int,int]]=1,\n              padding_mode:str='zeros', device=None, dtype=None)\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nc_in\n\n\n\n\n\nc_out\n\n\n\n\n\nact\ntype\nReLU\n\n\n\nstride\nint\n1\n\n\n\ngroups\nint\n1\n\n\n\nreduction\nNoneType\nNone\n\n\n\nnh1\nNoneType\nNone\n\n\n\nnh2\nNoneType\nNone\n\n\n\ndw\nbool\nFalse\n\n\n\ng2\nint\n1\n\n\n\nsa\nbool\nFalse\n\n\n\nsym\nbool\nFalse\n\n\n\nnorm_type\nNormType\nNormType.Batch\n\n\n\nact_cls\ntype\nReLU\n\n\n\nndim\nint\n2\n\n\n\nks\nint\n3\n\n\n\npool\nfunction\nAvgPool\n\n\n\npool_first\nbool\nTrue\n\n\n\npadding\nNoneType\nNone\n\n\n\nbias\nNoneType\nNone\n\n\n\nbn_1st\nbool\nTrue\n\n\n\ntranspose\nbool\nFalse\n\n\n\ninit\nstr\nauto\n\n\n\nxtra\nNoneType\nNone\n\n\n\nbias_std\nfloat\n0.01\n\n\n\ndilation\ntyping.Union[int, typing.Tuple[int, int]]\n1\n\n\n\npadding_mode\nstr\nzeros\nTODO: refine this type\n\n\ndevice\nNoneType\nNone\n\n\n\ndtype\nNoneType\nNone\n\n\n\n\n\nbs, c_in, seq_len = 2, 4, 32\nc_out = 2\nx = torch.rand(bs, c_in, seq_len)\narchs = [\n    xresnet1d18, xresnet1d34, xresnet1d50, \n    xresnet1d18_deep, xresnet1d34_deep, xresnet1d50_deep, xresnet1d18_deeper,\n    xresnet1d34_deeper, xresnet1d50_deeper\n#     # Long test\n#     xresnet1d101, xresnet1d152,\n]\nfor i, arch in enumerate(archs):\n    print(i, arch.__name__)\n    test_eq(arch(c_in, c_out, sa=True, act=Mish)(x).shape, (bs, c_out))\n\n0 xresnet1d18\n1 xresnet1d34\n2 xresnet1d50\n3 xresnet1d18_deep\n4 xresnet1d34_deep\n5 xresnet1d50_deep\n6 xresnet1d18_deeper\n7 xresnet1d34_deeper\n8 xresnet1d50_deeper\n\n\n\nm = xresnet1d34(4, 2, act=Mish)\ntest_eq(len(get_layers(m, is_bn)), 38)\ntest_eq(check_weight(m, is_bn)[0].sum(), 22)"
  },
  {
    "objectID": "models.xceptiontime.html",
    "href": "models.xceptiontime.html",
    "title": "XceptionTime",
    "section": "",
    "text": "This is an unofficial PyTorch implementation by Ignacio Oguiza - oguiza@timeseriesAI.co modified on:\nFawaz, H. I., Lucas, B., Forestier, G., Pelletier, C., Schmidt, D. F., Weber, J. & Petitjean, F. (2019). InceptionTime: Finding AlexNet for Time Series Classification. arXiv preprint arXiv:1909.04939.\nOfficial InceptionTime tensorflow implementation: https://github.com/hfawaz/InceptionTime\n\nsource\n\nXceptionTime\n\n XceptionTime (c_in, c_out, nf=16, nb_filters=None, adaptive_size=50,\n               residual=True)\n\nSame as nn.Module, but no need for subclasses to call super().__init__\n\nsource\n\n\nXceptionBlock\n\n XceptionBlock (ni, nf, residual=True, ks=40, bottleneck=True)\n\nSame as nn.Module, but no need for subclasses to call super().__init__\n\nsource\n\n\nXceptionModule\n\n XceptionModule (ni, nf, ks=40, bottleneck=True)\n\nSame as nn.Module, but no need for subclasses to call super().__init__\n\nbs = 16\nvars = 3\nseq_len = 12\nc_out = 6\nxb = torch.rand(bs, vars, seq_len)\ntest_eq(XceptionTime(vars,c_out)(xb).shape, [bs, c_out])\ntest_eq(XceptionTime(vars,c_out, bottleneck=False)(xb).shape, [bs, c_out])\ntest_eq(XceptionTime(vars,c_out, residual=False)(xb).shape, [bs, c_out])\ntest_eq(count_parameters(XceptionTime(3, 2)), 399540)\n\n\nm = XceptionTime(2,3)\ntest_eq(check_weight(m, is_bn)[0].sum(), 5) # 2 shortcut + 3 bn\ntest_eq(len(check_bias(m, is_conv)[0]), 0)\ntest_eq(len(check_bias(m)[0]), 5) # 2 shortcut + 3 bn\n\n\nXceptionTime(3, 2)\n\nXceptionTime(\n  (block): XceptionBlock(\n    (xception): ModuleList(\n      (0): XceptionModule(\n        (bottleneck): Conv1d(3, 16, kernel_size=(1,), stride=(1,), bias=False)\n        (convs): ModuleList(\n          (0): SeparableConv1d(\n            (depthwise_conv): Conv1d(16, 16, kernel_size=(39,), stride=(1,), padding=(19,), groups=16, bias=False)\n            (pointwise_conv): Conv1d(16, 16, kernel_size=(1,), stride=(1,), bias=False)\n          )\n          (1): SeparableConv1d(\n            (depthwise_conv): Conv1d(16, 16, kernel_size=(19,), stride=(1,), padding=(9,), groups=16, bias=False)\n            (pointwise_conv): Conv1d(16, 16, kernel_size=(1,), stride=(1,), bias=False)\n          )\n          (2): SeparableConv1d(\n            (depthwise_conv): Conv1d(16, 16, kernel_size=(9,), stride=(1,), padding=(4,), groups=16, bias=False)\n            (pointwise_conv): Conv1d(16, 16, kernel_size=(1,), stride=(1,), bias=False)\n          )\n        )\n        (maxconvpool): Sequential(\n          (0): MaxPool1d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)\n          (1): Conv1d(3, 16, kernel_size=(1,), stride=(1,), bias=False)\n        )\n        (concat): Concat(dim=1)\n      )\n      (1): XceptionModule(\n        (bottleneck): Conv1d(64, 32, kernel_size=(1,), stride=(1,), bias=False)\n        (convs): ModuleList(\n          (0): SeparableConv1d(\n            (depthwise_conv): Conv1d(32, 32, kernel_size=(39,), stride=(1,), padding=(19,), groups=32, bias=False)\n            (pointwise_conv): Conv1d(32, 32, kernel_size=(1,), stride=(1,), bias=False)\n          )\n          (1): SeparableConv1d(\n            (depthwise_conv): Conv1d(32, 32, kernel_size=(19,), stride=(1,), padding=(9,), groups=32, bias=False)\n            (pointwise_conv): Conv1d(32, 32, kernel_size=(1,), stride=(1,), bias=False)\n          )\n          (2): SeparableConv1d(\n            (depthwise_conv): Conv1d(32, 32, kernel_size=(9,), stride=(1,), padding=(4,), groups=32, bias=False)\n            (pointwise_conv): Conv1d(32, 32, kernel_size=(1,), stride=(1,), bias=False)\n          )\n        )\n        (maxconvpool): Sequential(\n          (0): MaxPool1d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)\n          (1): Conv1d(64, 32, kernel_size=(1,), stride=(1,), bias=False)\n        )\n        (concat): Concat(dim=1)\n      )\n      (2): XceptionModule(\n        (bottleneck): Conv1d(128, 64, kernel_size=(1,), stride=(1,), bias=False)\n        (convs): ModuleList(\n          (0): SeparableConv1d(\n            (depthwise_conv): Conv1d(64, 64, kernel_size=(39,), stride=(1,), padding=(19,), groups=64, bias=False)\n            (pointwise_conv): Conv1d(64, 64, kernel_size=(1,), stride=(1,), bias=False)\n          )\n          (1): SeparableConv1d(\n            (depthwise_conv): Conv1d(64, 64, kernel_size=(19,), stride=(1,), padding=(9,), groups=64, bias=False)\n            (pointwise_conv): Conv1d(64, 64, kernel_size=(1,), stride=(1,), bias=False)\n          )\n          (2): SeparableConv1d(\n            (depthwise_conv): Conv1d(64, 64, kernel_size=(9,), stride=(1,), padding=(4,), groups=64, bias=False)\n            (pointwise_conv): Conv1d(64, 64, kernel_size=(1,), stride=(1,), bias=False)\n          )\n        )\n        (maxconvpool): Sequential(\n          (0): MaxPool1d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)\n          (1): Conv1d(128, 64, kernel_size=(1,), stride=(1,), bias=False)\n        )\n        (concat): Concat(dim=1)\n      )\n      (3): XceptionModule(\n        (bottleneck): Conv1d(256, 128, kernel_size=(1,), stride=(1,), bias=False)\n        (convs): ModuleList(\n          (0): SeparableConv1d(\n            (depthwise_conv): Conv1d(128, 128, kernel_size=(39,), stride=(1,), padding=(19,), groups=128, bias=False)\n            (pointwise_conv): Conv1d(128, 128, kernel_size=(1,), stride=(1,), bias=False)\n          )\n          (1): SeparableConv1d(\n            (depthwise_conv): Conv1d(128, 128, kernel_size=(19,), stride=(1,), padding=(9,), groups=128, bias=False)\n            (pointwise_conv): Conv1d(128, 128, kernel_size=(1,), stride=(1,), bias=False)\n          )\n          (2): SeparableConv1d(\n            (depthwise_conv): Conv1d(128, 128, kernel_size=(9,), stride=(1,), padding=(4,), groups=128, bias=False)\n            (pointwise_conv): Conv1d(128, 128, kernel_size=(1,), stride=(1,), bias=False)\n          )\n        )\n        (maxconvpool): Sequential(\n          (0): MaxPool1d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)\n          (1): Conv1d(256, 128, kernel_size=(1,), stride=(1,), bias=False)\n        )\n        (concat): Concat(dim=1)\n      )\n    )\n    (shortcut): ModuleList(\n      (0): ConvBlock(\n        (0): Conv1d(3, 128, kernel_size=(1,), stride=(1,), bias=False)\n        (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n      (1): ConvBlock(\n        (0): Conv1d(128, 512, kernel_size=(1,), stride=(1,), bias=False)\n        (1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (add): Add\n    (act): ReLU()\n  )\n  (head): Sequential(\n    (0): AdaptiveAvgPool1d(output_size=50)\n    (1): ConvBlock(\n      (0): Conv1d(512, 256, kernel_size=(1,), stride=(1,), bias=False)\n      (1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (2): ReLU()\n    )\n    (2): ConvBlock(\n      (0): Conv1d(256, 128, kernel_size=(1,), stride=(1,), bias=False)\n      (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (2): ReLU()\n    )\n    (3): ConvBlock(\n      (0): Conv1d(128, 2, kernel_size=(1,), stride=(1,), bias=False)\n      (1): BatchNorm1d(2, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (2): ReLU()\n    )\n    (4): GAP1d(\n      (gap): AdaptiveAvgPool1d(output_size=1)\n      (flatten): Flatten(full=False)\n    )\n  )\n)"
  },
  {
    "objectID": "data.preprocessing.html",
    "href": "data.preprocessing.html",
    "title": "Data preprocessing",
    "section": "",
    "text": "Functions used to preprocess time series (both X and y).\nsource"
  },
  {
    "objectID": "data.preprocessing.html#y-transforms",
    "href": "data.preprocessing.html#y-transforms",
    "title": "Data preprocessing",
    "section": "y transforms",
    "text": "y transforms\n\nsource\n\nPreprocessor\n\n Preprocessor (preprocessor, **kwargs)\n\nInitialize self. See help(type(self)) for accurate signature.\n\n# Standardize\nfrom tsai.data.validation import TimeSplitter\n\n\ny = random_shuffle(np.random.randn(1000) * 10 + 5)\nsplits = TimeSplitter()(y)\npreprocessor = Preprocessor(StandardScaler)\npreprocessor.fit(y[splits[0]])\ny_tfm = preprocessor.transform(y)\ntest_close(preprocessor.inverse_transform(y_tfm), y)\nplt.hist(y, 50, label='ori',)\nplt.hist(y_tfm, 50, label='tfm')\nplt.legend(loc='best')\nplt.show()\n\n\n\n\n\n\n\n\n# RobustScaler\ny = random_shuffle(np.random.randn(1000) * 10 + 5)\nsplits = TimeSplitter()(y)\npreprocessor = Preprocessor(RobustScaler)\npreprocessor.fit(y[splits[0]])\ny_tfm = preprocessor.transform(y)\ntest_close(preprocessor.inverse_transform(y_tfm), y)\nplt.hist(y, 50, label='ori',)\nplt.hist(y_tfm, 50, label='tfm')\nplt.legend(loc='best')\nplt.show()\n\n\n\n\n\n\n\n\n# Normalize\ny = random_shuffle(np.random.rand(1000) * 3 + .5)\nsplits = TimeSplitter()(y)\npreprocessor = Preprocessor(Normalizer)\npreprocessor.fit(y[splits[0]])\ny_tfm = preprocessor.transform(y)\ntest_close(preprocessor.inverse_transform(y_tfm), y)\nplt.hist(y, 50, label='ori',)\nplt.hist(y_tfm, 50, label='tfm')\nplt.legend(loc='best')\nplt.show()\n\n\n\n\n\n\n\n\n# BoxCox\ny = random_shuffle(np.random.rand(1000) * 10 + 5)\nsplits = TimeSplitter()(y)\npreprocessor = Preprocessor(BoxCox)\npreprocessor.fit(y[splits[0]])\ny_tfm = preprocessor.transform(y)\ntest_close(preprocessor.inverse_transform(y_tfm), y)\nplt.hist(y, 50, label='ori',)\nplt.hist(y_tfm, 50, label='tfm')\nplt.legend(loc='best')\nplt.show()\n\n\n\n\n\n\n\n\n# YeoJohnshon\ny = random_shuffle(np.random.randn(1000) * 10 + 5)\ny = np.random.beta(.5, .5, size=1000)\nsplits = TimeSplitter()(y)\npreprocessor = Preprocessor(YeoJohnshon)\npreprocessor.fit(y[splits[0]])\ny_tfm = preprocessor.transform(y)\ntest_close(preprocessor.inverse_transform(y_tfm), y)\nplt.hist(y, 50, label='ori',)\nplt.hist(y_tfm, 50, label='tfm')\nplt.legend(loc='best')\nplt.show()\n\n\n\n\n\n\n\n\n# QuantileTransformer\ny = - np.random.beta(1, .5, 10000) * 10\nsplits = TimeSplitter()(y)\npreprocessor = Preprocessor(Quantile)\npreprocessor.fit(y[splits[0]])\nplt.hist(y, 50, label='ori',)\ny_tfm = preprocessor.transform(y)\nplt.legend(loc='best')\nplt.show()\nplt.hist(y_tfm, 50, label='tfm')\nplt.legend(loc='best')\nplt.show()\ntest_close(preprocessor.inverse_transform(y_tfm), y, 1e-1)\n\n\n\n\n\n\n\n\n\n\n\nsource\n\n\nReLabeler\n\n ReLabeler (cm)\n\nChanges the labels in a dataset based on a dictionary (class mapping) Args: cm = class mapping dictionary\n\nvals = {0:'a', 1:'b', 2:'c', 3:'d', 4:'e'}\ny = np.array([vals[i] for i in np.random.randint(0, 5, 20)])\nlabeler = ReLabeler(dict(a='x', b='x', c='y', d='z', e='z'))\ny_new = labeler(y)\ntest_eq(y.shape, y_new.shape)\ny, y_new\n\n(array(['e', 'e', 'd', 'e', 'a', 'd', 'a', 'c', 'c', 'e', 'b', 'c', 'a',\n        'a', 'd', 'e', 'c', 'd', 'b', 'e'], dtype='<U1'),\n array(['z', 'z', 'z', 'z', 'x', 'z', 'x', 'y', 'y', 'z', 'x', 'y', 'x',\n        'x', 'z', 'z', 'y', 'z', 'x', 'z'], dtype='<U1'))"
  },
  {
    "objectID": "models.rnn_fcnplus.html",
    "href": "models.rnn_fcnplus.html",
    "title": "RNN_FCNPlus",
    "section": "",
    "text": "This is an unofficial PyTorch implementation by Ignacio Oguiza - oguiza@timeseriesAI.co\n\n\nsource\n\nMGRU_FCNPlus\n\n MGRU_FCNPlus (*args, se=16, **kwargs)\n\nA sequential container. Modules will be added to it in the order they are passed in the constructor. Alternatively, an OrderedDict of modules can be passed in. The forward() method of [Sequential](https://timeseriesAI.github.io/models.layers.html#sequential) accepts any input and forwards it to the first module it contains. It then “chains” outputs to inputs sequentially for each subsequent module, finally returning the output of the last module.\nThe value a [Sequential](https://timeseriesAI.github.io/models.layers.html#sequential) provides over manually calling a sequence of modules is that it allows treating the whole container as a single module, such that performing a transformation on the [Sequential](https://timeseriesAI.github.io/models.layers.html#sequential) applies to each of the modules it stores (which are each a registered submodule of the [Sequential](https://timeseriesAI.github.io/models.layers.html#sequential)).\nWhat’s the difference between a [Sequential](https://timeseriesAI.github.io/models.layers.html#sequential) and a :class:torch.nn.ModuleList? A ModuleList is exactly what it sounds like–a list for storing Module s! On the other hand, the layers in a [Sequential](https://timeseriesAI.github.io/models.layers.html#sequential) are connected in a cascading way.\nExample::\n# Using Sequential to create a small model. When `model` is run,\n# input will first be passed to `Conv2d(1,20,5)`. The output of\n# `Conv2d(1,20,5)` will be used as the input to the first\n# `ReLU`; the output of the first `ReLU` will become the input\n# for `Conv2d(20,64,5)`. Finally, the output of\n# `Conv2d(20,64,5)` will be used as input to the second `ReLU`\nmodel = nn.Sequential(\n          nn.Conv2d(1,20,5),\n          nn.ReLU(),\n          nn.Conv2d(20,64,5),\n          nn.ReLU()\n        )\n\n# Using Sequential with OrderedDict. This is functionally the\n# same as the above code\nmodel = nn.Sequential(OrderedDict([\n          ('conv1', nn.Conv2d(1,20,5)),\n          ('relu1', nn.ReLU()),\n          ('conv2', nn.Conv2d(20,64,5)),\n          ('relu2', nn.ReLU())\n        ]))\n\nsource\n\n\nMLSTM_FCNPlus\n\n MLSTM_FCNPlus (*args, se=16, **kwargs)\n\nA sequential container. Modules will be added to it in the order they are passed in the constructor. Alternatively, an OrderedDict of modules can be passed in. The forward() method of [Sequential](https://timeseriesAI.github.io/models.layers.html#sequential) accepts any input and forwards it to the first module it contains. It then “chains” outputs to inputs sequentially for each subsequent module, finally returning the output of the last module.\nThe value a [Sequential](https://timeseriesAI.github.io/models.layers.html#sequential) provides over manually calling a sequence of modules is that it allows treating the whole container as a single module, such that performing a transformation on the [Sequential](https://timeseriesAI.github.io/models.layers.html#sequential) applies to each of the modules it stores (which are each a registered submodule of the [Sequential](https://timeseriesAI.github.io/models.layers.html#sequential)).\nWhat’s the difference between a [Sequential](https://timeseriesAI.github.io/models.layers.html#sequential) and a :class:torch.nn.ModuleList? A ModuleList is exactly what it sounds like–a list for storing Module s! On the other hand, the layers in a [Sequential](https://timeseriesAI.github.io/models.layers.html#sequential) are connected in a cascading way.\nExample::\n# Using Sequential to create a small model. When `model` is run,\n# input will first be passed to `Conv2d(1,20,5)`. The output of\n# `Conv2d(1,20,5)` will be used as the input to the first\n# `ReLU`; the output of the first `ReLU` will become the input\n# for `Conv2d(20,64,5)`. Finally, the output of\n# `Conv2d(20,64,5)` will be used as input to the second `ReLU`\nmodel = nn.Sequential(\n          nn.Conv2d(1,20,5),\n          nn.ReLU(),\n          nn.Conv2d(20,64,5),\n          nn.ReLU()\n        )\n\n# Using Sequential with OrderedDict. This is functionally the\n# same as the above code\nmodel = nn.Sequential(OrderedDict([\n          ('conv1', nn.Conv2d(1,20,5)),\n          ('relu1', nn.ReLU()),\n          ('conv2', nn.Conv2d(20,64,5)),\n          ('relu2', nn.ReLU())\n        ]))\n\nsource\n\n\nMRNN_FCNPlus\n\n MRNN_FCNPlus (*args, se=16, **kwargs)\n\nA sequential container. Modules will be added to it in the order they are passed in the constructor. Alternatively, an OrderedDict of modules can be passed in. The forward() method of [Sequential](https://timeseriesAI.github.io/models.layers.html#sequential) accepts any input and forwards it to the first module it contains. It then “chains” outputs to inputs sequentially for each subsequent module, finally returning the output of the last module.\nThe value a [Sequential](https://timeseriesAI.github.io/models.layers.html#sequential) provides over manually calling a sequence of modules is that it allows treating the whole container as a single module, such that performing a transformation on the [Sequential](https://timeseriesAI.github.io/models.layers.html#sequential) applies to each of the modules it stores (which are each a registered submodule of the [Sequential](https://timeseriesAI.github.io/models.layers.html#sequential)).\nWhat’s the difference between a [Sequential](https://timeseriesAI.github.io/models.layers.html#sequential) and a :class:torch.nn.ModuleList? A ModuleList is exactly what it sounds like–a list for storing Module s! On the other hand, the layers in a [Sequential](https://timeseriesAI.github.io/models.layers.html#sequential) are connected in a cascading way.\nExample::\n# Using Sequential to create a small model. When `model` is run,\n# input will first be passed to `Conv2d(1,20,5)`. The output of\n# `Conv2d(1,20,5)` will be used as the input to the first\n# `ReLU`; the output of the first `ReLU` will become the input\n# for `Conv2d(20,64,5)`. Finally, the output of\n# `Conv2d(20,64,5)` will be used as input to the second `ReLU`\nmodel = nn.Sequential(\n          nn.Conv2d(1,20,5),\n          nn.ReLU(),\n          nn.Conv2d(20,64,5),\n          nn.ReLU()\n        )\n\n# Using Sequential with OrderedDict. This is functionally the\n# same as the above code\nmodel = nn.Sequential(OrderedDict([\n          ('conv1', nn.Conv2d(1,20,5)),\n          ('relu1', nn.ReLU()),\n          ('conv2', nn.Conv2d(20,64,5)),\n          ('relu2', nn.ReLU())\n        ]))\n\nsource\n\n\nGRU_FCNPlus\n\n GRU_FCNPlus (c_in, c_out, seq_len=None, hidden_size=100, rnn_layers=1,\n              bias=True, cell_dropout=0, rnn_dropout=0.8,\n              bidirectional=False, shuffle=True, fc_dropout=0.0,\n              conv_layers=[128, 256, 128], kss=[7, 5, 3], se=0,\n              custom_head=None)\n\nA sequential container. Modules will be added to it in the order they are passed in the constructor. Alternatively, an OrderedDict of modules can be passed in. The forward() method of [Sequential](https://timeseriesAI.github.io/models.layers.html#sequential) accepts any input and forwards it to the first module it contains. It then “chains” outputs to inputs sequentially for each subsequent module, finally returning the output of the last module.\nThe value a [Sequential](https://timeseriesAI.github.io/models.layers.html#sequential) provides over manually calling a sequence of modules is that it allows treating the whole container as a single module, such that performing a transformation on the [Sequential](https://timeseriesAI.github.io/models.layers.html#sequential) applies to each of the modules it stores (which are each a registered submodule of the [Sequential](https://timeseriesAI.github.io/models.layers.html#sequential)).\nWhat’s the difference between a [Sequential](https://timeseriesAI.github.io/models.layers.html#sequential) and a :class:torch.nn.ModuleList? A ModuleList is exactly what it sounds like–a list for storing Module s! On the other hand, the layers in a [Sequential](https://timeseriesAI.github.io/models.layers.html#sequential) are connected in a cascading way.\nExample::\n# Using Sequential to create a small model. When `model` is run,\n# input will first be passed to `Conv2d(1,20,5)`. The output of\n# `Conv2d(1,20,5)` will be used as the input to the first\n# `ReLU`; the output of the first `ReLU` will become the input\n# for `Conv2d(20,64,5)`. Finally, the output of\n# `Conv2d(20,64,5)` will be used as input to the second `ReLU`\nmodel = nn.Sequential(\n          nn.Conv2d(1,20,5),\n          nn.ReLU(),\n          nn.Conv2d(20,64,5),\n          nn.ReLU()\n        )\n\n# Using Sequential with OrderedDict. This is functionally the\n# same as the above code\nmodel = nn.Sequential(OrderedDict([\n          ('conv1', nn.Conv2d(1,20,5)),\n          ('relu1', nn.ReLU()),\n          ('conv2', nn.Conv2d(20,64,5)),\n          ('relu2', nn.ReLU())\n        ]))\n\nsource\n\n\nLSTM_FCNPlus\n\n LSTM_FCNPlus (c_in, c_out, seq_len=None, hidden_size=100, rnn_layers=1,\n               bias=True, cell_dropout=0, rnn_dropout=0.8,\n               bidirectional=False, shuffle=True, fc_dropout=0.0,\n               conv_layers=[128, 256, 128], kss=[7, 5, 3], se=0,\n               custom_head=None)\n\nA sequential container. Modules will be added to it in the order they are passed in the constructor. Alternatively, an OrderedDict of modules can be passed in. The forward() method of [Sequential](https://timeseriesAI.github.io/models.layers.html#sequential) accepts any input and forwards it to the first module it contains. It then “chains” outputs to inputs sequentially for each subsequent module, finally returning the output of the last module.\nThe value a [Sequential](https://timeseriesAI.github.io/models.layers.html#sequential) provides over manually calling a sequence of modules is that it allows treating the whole container as a single module, such that performing a transformation on the [Sequential](https://timeseriesAI.github.io/models.layers.html#sequential) applies to each of the modules it stores (which are each a registered submodule of the [Sequential](https://timeseriesAI.github.io/models.layers.html#sequential)).\nWhat’s the difference between a [Sequential](https://timeseriesAI.github.io/models.layers.html#sequential) and a :class:torch.nn.ModuleList? A ModuleList is exactly what it sounds like–a list for storing Module s! On the other hand, the layers in a [Sequential](https://timeseriesAI.github.io/models.layers.html#sequential) are connected in a cascading way.\nExample::\n# Using Sequential to create a small model. When `model` is run,\n# input will first be passed to `Conv2d(1,20,5)`. The output of\n# `Conv2d(1,20,5)` will be used as the input to the first\n# `ReLU`; the output of the first `ReLU` will become the input\n# for `Conv2d(20,64,5)`. Finally, the output of\n# `Conv2d(20,64,5)` will be used as input to the second `ReLU`\nmodel = nn.Sequential(\n          nn.Conv2d(1,20,5),\n          nn.ReLU(),\n          nn.Conv2d(20,64,5),\n          nn.ReLU()\n        )\n\n# Using Sequential with OrderedDict. This is functionally the\n# same as the above code\nmodel = nn.Sequential(OrderedDict([\n          ('conv1', nn.Conv2d(1,20,5)),\n          ('relu1', nn.ReLU()),\n          ('conv2', nn.Conv2d(20,64,5)),\n          ('relu2', nn.ReLU())\n        ]))\n\nsource\n\n\nRNN_FCNPlus\n\n RNN_FCNPlus (c_in, c_out, seq_len=None, hidden_size=100, rnn_layers=1,\n              bias=True, cell_dropout=0, rnn_dropout=0.8,\n              bidirectional=False, shuffle=True, fc_dropout=0.0,\n              conv_layers=[128, 256, 128], kss=[7, 5, 3], se=0,\n              custom_head=None)\n\nA sequential container. Modules will be added to it in the order they are passed in the constructor. Alternatively, an OrderedDict of modules can be passed in. The forward() method of [Sequential](https://timeseriesAI.github.io/models.layers.html#sequential) accepts any input and forwards it to the first module it contains. It then “chains” outputs to inputs sequentially for each subsequent module, finally returning the output of the last module.\nThe value a [Sequential](https://timeseriesAI.github.io/models.layers.html#sequential) provides over manually calling a sequence of modules is that it allows treating the whole container as a single module, such that performing a transformation on the [Sequential](https://timeseriesAI.github.io/models.layers.html#sequential) applies to each of the modules it stores (which are each a registered submodule of the [Sequential](https://timeseriesAI.github.io/models.layers.html#sequential)).\nWhat’s the difference between a [Sequential](https://timeseriesAI.github.io/models.layers.html#sequential) and a :class:torch.nn.ModuleList? A ModuleList is exactly what it sounds like–a list for storing Module s! On the other hand, the layers in a [Sequential](https://timeseriesAI.github.io/models.layers.html#sequential) are connected in a cascading way.\nExample::\n# Using Sequential to create a small model. When `model` is run,\n# input will first be passed to `Conv2d(1,20,5)`. The output of\n# `Conv2d(1,20,5)` will be used as the input to the first\n# `ReLU`; the output of the first `ReLU` will become the input\n# for `Conv2d(20,64,5)`. Finally, the output of\n# `Conv2d(20,64,5)` will be used as input to the second `ReLU`\nmodel = nn.Sequential(\n          nn.Conv2d(1,20,5),\n          nn.ReLU(),\n          nn.Conv2d(20,64,5),\n          nn.ReLU()\n        )\n\n# Using Sequential with OrderedDict. This is functionally the\n# same as the above code\nmodel = nn.Sequential(OrderedDict([\n          ('conv1', nn.Conv2d(1,20,5)),\n          ('relu1', nn.ReLU()),\n          ('conv2', nn.Conv2d(20,64,5)),\n          ('relu2', nn.ReLU())\n        ]))\n\nfrom tsai.models.utils import count_parameters\nfrom tsai.models.RNN_FCN import *\n\n\nbs = 16\nn_vars = 3\nseq_len = 12\nc_out = 2\nxb = torch.rand(bs, n_vars, seq_len)\ntest_eq(RNN_FCNPlus(n_vars, c_out, seq_len)(xb).shape, [bs, c_out])\ntest_eq(LSTM_FCNPlus(n_vars, c_out, seq_len)(xb).shape, [bs, c_out])\ntest_eq(MLSTM_FCNPlus(n_vars, c_out, seq_len)(xb).shape, [bs, c_out])\ntest_eq(GRU_FCNPlus(n_vars, c_out, shuffle=False)(xb).shape, [bs, c_out])\ntest_eq(GRU_FCNPlus(n_vars, c_out, seq_len, shuffle=False)(xb).shape, [bs, c_out])\ntest_eq(count_parameters(LSTM_FCNPlus(n_vars, c_out, seq_len)), count_parameters(LSTM_FCN(n_vars, c_out, seq_len)))\n\n\nbs = 16\nn_vars = 3\nseq_len = 12\nc_out = 2\nxb = torch.rand(bs, n_vars, seq_len)\ncustom_head = nn.Linear(228, c_out)\ntest_eq(RNN_FCNPlus(n_vars, c_out, seq_len, custom_head=custom_head)(xb).shape, [bs, c_out])\n\n\nLSTM_FCNPlus(n_vars, seq_len, c_out, se=8)\n\nLSTM_FCNPlus(\n  (backbone): _RNN_FCN_Base_Backbone(\n    (rnn): LSTM(2, 100, batch_first=True)\n    (rnn_dropout): Dropout(p=0.8, inplace=False)\n    (convblock1): ConvBlock(\n      (0): Conv1d(3, 128, kernel_size=(7,), stride=(1,), padding=(3,), bias=False)\n      (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (2): ReLU()\n    )\n    (se1): SqueezeExciteBlock(\n      (avg_pool): GAP1d(\n        (gap): AdaptiveAvgPool1d(output_size=1)\n        (flatten): Flatten(full=False)\n      )\n      (fc): Sequential(\n        (0): Linear(in_features=128, out_features=16, bias=False)\n        (1): ReLU()\n        (2): Linear(in_features=16, out_features=128, bias=False)\n        (3): Sigmoid()\n      )\n    )\n    (convblock2): ConvBlock(\n      (0): Conv1d(128, 256, kernel_size=(5,), stride=(1,), padding=(2,), bias=False)\n      (1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (2): ReLU()\n    )\n    (se2): SqueezeExciteBlock(\n      (avg_pool): GAP1d(\n        (gap): AdaptiveAvgPool1d(output_size=1)\n        (flatten): Flatten(full=False)\n      )\n      (fc): Sequential(\n        (0): Linear(in_features=256, out_features=32, bias=False)\n        (1): ReLU()\n        (2): Linear(in_features=32, out_features=256, bias=False)\n        (3): Sigmoid()\n      )\n    )\n    (convblock3): ConvBlock(\n      (0): Conv1d(256, 128, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)\n      (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (2): ReLU()\n    )\n    (gap): GAP1d(\n      (gap): AdaptiveAvgPool1d(output_size=1)\n      (flatten): Flatten(full=False)\n    )\n    (concat): Concat(dim=1)\n  )\n  (head): Sequential(\n    (0): Linear(in_features=228, out_features=12, bias=True)\n  )\n)"
  },
  {
    "objectID": "models.rnn_fcn.html",
    "href": "models.rnn_fcn.html",
    "title": "RNN_FCN",
    "section": "",
    "text": "This is an unofficial PyTorch implementation created by Ignacio Oguiza - oguiza@timeseriesAI.co\n\n\nsource\n\nMGRU_FCN\n\n MGRU_FCN (*args, se=16, **kwargs)\n\nSame as nn.Module, but no need for subclasses to call super().__init__\n\nsource\n\n\nMLSTM_FCN\n\n MLSTM_FCN (*args, se=16, **kwargs)\n\nSame as nn.Module, but no need for subclasses to call super().__init__\n\nsource\n\n\nMRNN_FCN\n\n MRNN_FCN (*args, se=16, **kwargs)\n\nSame as nn.Module, but no need for subclasses to call super().__init__\n\nsource\n\n\nGRU_FCN\n\n GRU_FCN (c_in, c_out, seq_len=None, hidden_size=100, rnn_layers=1,\n          bias=True, cell_dropout=0, rnn_dropout=0.8, bidirectional=False,\n          shuffle=True, fc_dropout=0.0, conv_layers=[128, 256, 128],\n          kss=[7, 5, 3], se=0)\n\nSame as nn.Module, but no need for subclasses to call super().__init__\n\nsource\n\n\nLSTM_FCN\n\n LSTM_FCN (c_in, c_out, seq_len=None, hidden_size=100, rnn_layers=1,\n           bias=True, cell_dropout=0, rnn_dropout=0.8,\n           bidirectional=False, shuffle=True, fc_dropout=0.0,\n           conv_layers=[128, 256, 128], kss=[7, 5, 3], se=0)\n\nSame as nn.Module, but no need for subclasses to call super().__init__\n\nsource\n\n\nRNN_FCN\n\n RNN_FCN (c_in, c_out, seq_len=None, hidden_size=100, rnn_layers=1,\n          bias=True, cell_dropout=0, rnn_dropout=0.8, bidirectional=False,\n          shuffle=True, fc_dropout=0.0, conv_layers=[128, 256, 128],\n          kss=[7, 5, 3], se=0)\n\nSame as nn.Module, but no need for subclasses to call super().__init__\n\nbs = 16\nn_vars = 3\nseq_len = 12\nc_out = 2\nxb = torch.rand(bs, n_vars, seq_len)\ntest_eq(RNN_FCN(n_vars, c_out, seq_len)(xb).shape, [bs, c_out])\ntest_eq(LSTM_FCN(n_vars, c_out, seq_len)(xb).shape, [bs, c_out])\ntest_eq(MLSTM_FCN(n_vars, c_out, seq_len)(xb).shape, [bs, c_out])\ntest_eq(GRU_FCN(n_vars, c_out, shuffle=False)(xb).shape, [bs, c_out])\ntest_eq(GRU_FCN(n_vars, c_out, seq_len, shuffle=False)(xb).shape, [bs, c_out])\n\n\nLSTM_FCN(n_vars, seq_len, c_out, se=8)\n\nLSTM_FCN(\n  (rnn): LSTM(2, 100, batch_first=True)\n  (rnn_dropout): Dropout(p=0.8, inplace=False)\n  (convblock1): ConvBlock(\n    (0): Conv1d(3, 128, kernel_size=(7,), stride=(1,), padding=(3,), bias=False)\n    (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (2): ReLU()\n  )\n  (se1): SqueezeExciteBlock(\n    (avg_pool): GAP1d(\n      (gap): AdaptiveAvgPool1d(output_size=1)\n      (flatten): Flatten(full=False)\n    )\n    (fc): Sequential(\n      (0): Linear(in_features=128, out_features=16, bias=False)\n      (1): ReLU()\n      (2): Linear(in_features=16, out_features=128, bias=False)\n      (3): Sigmoid()\n    )\n  )\n  (convblock2): ConvBlock(\n    (0): Conv1d(128, 256, kernel_size=(5,), stride=(1,), padding=(2,), bias=False)\n    (1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (2): ReLU()\n  )\n  (se2): SqueezeExciteBlock(\n    (avg_pool): GAP1d(\n      (gap): AdaptiveAvgPool1d(output_size=1)\n      (flatten): Flatten(full=False)\n    )\n    (fc): Sequential(\n      (0): Linear(in_features=256, out_features=32, bias=False)\n      (1): ReLU()\n      (2): Linear(in_features=32, out_features=256, bias=False)\n      (3): Sigmoid()\n    )\n  )\n  (convblock3): ConvBlock(\n    (0): Conv1d(256, 128, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)\n    (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (2): ReLU()\n  )\n  (gap): GAP1d(\n    (gap): AdaptiveAvgPool1d(output_size=1)\n    (flatten): Flatten(full=False)\n  )\n  (concat): Concat(dim=1)\n  (fc): Linear(in_features=228, out_features=12, bias=True)\n)"
  },
  {
    "objectID": "models.omniscalecnn.html",
    "href": "models.omniscalecnn.html",
    "title": "OmniScaleCNN",
    "section": "",
    "text": "This is an unofficial PyTorch implementation created by Ignacio Oguiza - oguiza@timeseriesAI.co\n\n\nsource\n\ngenerate_layer_parameter_list\n\n generate_layer_parameter_list (start, end, layers, in_channel=1)\n\n\nsource\n\n\nget_out_channel_number\n\n get_out_channel_number (paramenter_layer, in_channel, prime_list)\n\n\nsource\n\n\nget_Prime_number_in_a_range\n\n get_Prime_number_in_a_range (start, end)\n\n\nsource\n\n\nOmniScaleCNN\n\n OmniScaleCNN (c_in, c_out, seq_len, layers=[1024, 229376],\n               few_shot=False)\n\nSame as nn.Module, but no need for subclasses to call super().__init__\n\nsource\n\n\nbuild_layer_with_layer_parameter\n\n build_layer_with_layer_parameter (layer_parameters)\n\nformerly build_layer_with_layer_parameter\n\nsource\n\n\nSampaddingConv1D_BN\n\n SampaddingConv1D_BN (in_channels, out_channels, kernel_size)\n\nSame as nn.Module, but no need for subclasses to call super().__init__\n\nbs = 16\nc_in = 3\nseq_len = 12\nc_out = 2\nxb = torch.rand(bs, c_in, seq_len)\nm = create_model(OmniScaleCNN, c_in, c_out, seq_len)\ntest_eq(OmniScaleCNN(c_in, c_out, seq_len)(xb).shape, [bs, c_out])\nm\n\nOmniScaleCNN(\n  (net): Sequential(\n    (0): build_layer_with_layer_parameter(\n      (conv_list): ModuleList(\n        (0): SampaddingConv1D_BN(\n          (padding): ConstantPad1d(padding=(0, 0), value=0)\n          (conv1d): Conv1d(3, 56, kernel_size=(1,), stride=(1,))\n          (bn): BatchNorm1d(56, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n        (1): SampaddingConv1D_BN(\n          (padding): ConstantPad1d(padding=(0, 1), value=0)\n          (conv1d): Conv1d(3, 56, kernel_size=(2,), stride=(1,))\n          (bn): BatchNorm1d(56, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n        (2): SampaddingConv1D_BN(\n          (padding): ConstantPad1d(padding=(1, 1), value=0)\n          (conv1d): Conv1d(3, 56, kernel_size=(3,), stride=(1,))\n          (bn): BatchNorm1d(56, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n      )\n    )\n    (1): build_layer_with_layer_parameter(\n      (conv_list): ModuleList(\n        (0): SampaddingConv1D_BN(\n          (padding): ConstantPad1d(padding=(0, 0), value=0)\n          (conv1d): Conv1d(168, 227, kernel_size=(1,), stride=(1,))\n          (bn): BatchNorm1d(227, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n        (1): SampaddingConv1D_BN(\n          (padding): ConstantPad1d(padding=(0, 1), value=0)\n          (conv1d): Conv1d(168, 227, kernel_size=(2,), stride=(1,))\n          (bn): BatchNorm1d(227, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n        (2): SampaddingConv1D_BN(\n          (padding): ConstantPad1d(padding=(1, 1), value=0)\n          (conv1d): Conv1d(168, 227, kernel_size=(3,), stride=(1,))\n          (bn): BatchNorm1d(227, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n      )\n    )\n    (2): build_layer_with_layer_parameter(\n      (conv_list): ModuleList(\n        (0): SampaddingConv1D_BN(\n          (padding): ConstantPad1d(padding=(0, 0), value=0)\n          (conv1d): Conv1d(681, 510, kernel_size=(1,), stride=(1,))\n          (bn): BatchNorm1d(510, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n        (1): SampaddingConv1D_BN(\n          (padding): ConstantPad1d(padding=(0, 1), value=0)\n          (conv1d): Conv1d(681, 510, kernel_size=(2,), stride=(1,))\n          (bn): BatchNorm1d(510, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n      )\n    )\n  )\n  (gap): GAP1d(\n    (gap): AdaptiveAvgPool1d(output_size=1)\n    (flatten): Flatten(full=False)\n  )\n  (hidden): Linear(in_features=1020, out_features=2, bias=True)\n)"
  },
  {
    "objectID": "models.rescnn.html",
    "href": "models.rescnn.html",
    "title": "ResCNN",
    "section": "",
    "text": "This is an unofficial PyTorch implementation by Ignacio Oguiza - oguiza@timeseriesAI.co\n\n\nfrom tsai.models.utils import *\n\n\nsource\n\nResCNN\n\n ResCNN (c_in, c_out, coord=False, separable=False, zero_norm=False)\n\nSame as nn.Module, but no need for subclasses to call super().__init__\n\nxb = torch.rand(16, 3, 10)\ntest_eq(ResCNN(3,2,coord=True, separable=True)(xb).shape, [xb.shape[0], 2])\ntest_eq(count_parameters(ResCNN(3,2)), 257283)\n\n\nResCNN(3,2,coord=True, separable=True)\n\nResCNN(\n  (block1): _ResCNNBlock(\n    (convblock1): ConvBlock(\n      (0): AddCoords1d()\n      (1): SeparableConv1d(\n        (depthwise_conv): Conv1d(4, 4, kernel_size=(7,), stride=(1,), padding=(3,), groups=4, bias=False)\n        (pointwise_conv): Conv1d(4, 64, kernel_size=(1,), stride=(1,), bias=False)\n      )\n      (2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (3): ReLU()\n    )\n    (convblock2): ConvBlock(\n      (0): AddCoords1d()\n      (1): SeparableConv1d(\n        (depthwise_conv): Conv1d(65, 65, kernel_size=(5,), stride=(1,), padding=(2,), groups=65, bias=False)\n        (pointwise_conv): Conv1d(65, 64, kernel_size=(1,), stride=(1,), bias=False)\n      )\n      (2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (3): ReLU()\n    )\n    (convblock3): ConvBlock(\n      (0): AddCoords1d()\n      (1): SeparableConv1d(\n        (depthwise_conv): Conv1d(65, 65, kernel_size=(3,), stride=(1,), padding=(1,), groups=65, bias=False)\n        (pointwise_conv): Conv1d(65, 64, kernel_size=(1,), stride=(1,), bias=False)\n      )\n      (2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (shortcut): ConvBlock(\n      (0): AddCoords1d()\n      (1): Conv1d(4, 64, kernel_size=(1,), stride=(1,), bias=False)\n      (2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (add): Add\n    (act): ReLU()\n  )\n  (block2): ConvBlock(\n    (0): AddCoords1d()\n    (1): SeparableConv1d(\n      (depthwise_conv): Conv1d(65, 65, kernel_size=(3,), stride=(1,), padding=(1,), groups=65, bias=False)\n      (pointwise_conv): Conv1d(65, 128, kernel_size=(1,), stride=(1,), bias=False)\n    )\n    (2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (3): LeakyReLU(negative_slope=0.2)\n  )\n  (block3): ConvBlock(\n    (0): AddCoords1d()\n    (1): SeparableConv1d(\n      (depthwise_conv): Conv1d(129, 129, kernel_size=(3,), stride=(1,), padding=(1,), groups=129, bias=False)\n      (pointwise_conv): Conv1d(129, 256, kernel_size=(1,), stride=(1,), bias=False)\n    )\n    (2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (3): PReLU(num_parameters=1)\n  )\n  (block4): ConvBlock(\n    (0): AddCoords1d()\n    (1): SeparableConv1d(\n      (depthwise_conv): Conv1d(257, 257, kernel_size=(3,), stride=(1,), padding=(1,), groups=257, bias=False)\n      (pointwise_conv): Conv1d(257, 128, kernel_size=(1,), stride=(1,), bias=False)\n    )\n    (2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (3): ELU(alpha=0.3)\n  )\n  (gap): AdaptiveAvgPool1d(output_size=1)\n  (squeeze): Squeeze(dim=-1)\n  (lin): Linear(in_features=128, out_features=2, bias=True)\n)\n\n\n\ncheck_weight(ResCNN(3,2, zero_norm=True), is_bn)\n\n(array([1., 1., 0., 1., 1., 1., 1.], dtype=float32),\n array([0., 0., 0., 0., 0., 0., 0.], dtype=float32))"
  },
  {
    "objectID": "data.preparation.html",
    "href": "data.preparation.html",
    "title": "Data preparation",
    "section": "",
    "text": "Functions required to prepare X (and y) from a pandas dataframe.\n\n\nsource\n\napply_sliding_window\n\n apply_sliding_window (data, window_len:Union[int,list],\n                       horizon:Union[int,list]=0,\n                       x_vars:Union[int,list]=None,\n                       y_vars:Union[int,list]=None)\n\nApplies a sliding window on an array-like input to generate a 3d X (and optionally y)\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ndata\n\n\nand array-like object with the input data\n\n\nwindow_len\nint | list\n\nsliding window length. When using a list, use negative numbers and 0.\n\n\nhorizon\nint | list\n0\nhorizon\n\n\nx_vars\nint | list\nNone\nindices of the independent variables\n\n\ny_vars\nint | list\nNone\nindices of the dependent variables (target). [] means no y will be created. None means all variables.\n\n\n\n\nsource\n\n\nprepare_sel_vars_and_steps\n\n prepare_sel_vars_and_steps (sel_vars=None, sel_steps=None, idxs=False)\n\n\nsource\n\n\nprepare_idxs\n\n prepare_idxs (o, shape=None)\n\n\ndata = np.arange(20).reshape(-1,1).repeat(3, 1) * np.array([1, 10, 100])\ndf = pd.DataFrame(data, columns=['feat_1', 'feat_2', 'feat_3'])\ndf.head()\n\n\n\n\n\n  \n    \n      \n      feat_1\n      feat_2\n      feat_3\n    \n  \n  \n    \n      0\n      0\n      0\n      0\n    \n    \n      1\n      1\n      10\n      100\n    \n    \n      2\n      2\n      20\n      200\n    \n    \n      3\n      3\n      30\n      300\n    \n    \n      4\n      4\n      40\n      400\n    \n  \n\n\n\n\n\nwindow_len = 8\nhorizon = 1\nx_vars = None\ny_vars = None\nX, y = apply_sliding_window(data, window_len, horizon=horizon, x_vars=x_vars, y_vars=y_vars)\nprint(np.shares_memory(X, data))\nprint(np.shares_memory(y, data))\nprint(X.shape, y.shape)\ntest_eq(X.shape, (len(df) - (window_len - 1 + horizon), df.shape[1], window_len))\ntest_eq(y.shape, (len(df) - (window_len - 1 + horizon), df.shape[1]))\nX[0], y[0]\n\nTrue\nTrue\n(12, 3, 8) (12, 3)\n\n\n(array([[  0,   1,   2,   3,   4,   5,   6,   7],\n        [  0,  10,  20,  30,  40,  50,  60,  70],\n        [  0, 100, 200, 300, 400, 500, 600, 700]]),\n array([  8,  80, 800]))\n\n\n\nwindow_len = 8\nhorizon = 1\nx_vars = None\ny_vars = 0\nX, y = apply_sliding_window(df, window_len, horizon=horizon, x_vars=x_vars, y_vars=y_vars)\nprint(np.shares_memory(X, df))\nprint(np.shares_memory(y, df))\nprint(X.shape, y.shape)\ntest_eq(X.shape, (len(df) - (window_len - 1 + horizon), df.shape[1], window_len))\ntest_eq(y.shape, (len(df) - (window_len - 1 + horizon),))\nX[0], y[0]\n\nTrue\nTrue\n(12, 3, 8) (12,)\n\n\n(array([[  0,   1,   2,   3,   4,   5,   6,   7],\n        [  0,  10,  20,  30,  40,  50,  60,  70],\n        [  0, 100, 200, 300, 400, 500, 600, 700]]),\n 8)\n\n\n\nwindow_len = 8\nhorizon = [1, 2]\nx_vars = 0\ny_vars = [1, 2]\nX, y = apply_sliding_window(df, window_len, horizon=horizon, x_vars=x_vars, y_vars=y_vars)\nprint(np.shares_memory(X, df))\nprint(np.shares_memory(y, df))\nprint(X.shape, y.shape)\ntest_eq(X.shape, (len(df) - (window_len - 1 + max(horizon)), 1, window_len))\ntest_eq(y.shape, (len(df) - (window_len - 1 + max(horizon)), len(y_vars), len(horizon)))\nX[0], y[0]\n\nTrue\nFalse\n(11, 1, 8) (11, 2, 2)\n\n\n(array([[0, 1, 2, 3, 4, 5, 6, 7]]),\n array([[ 80,  90],\n        [800, 900]]))\n\n\n\nwindow_len = [-4, -2, -1, 0]\nhorizon = [1, 2, 4]\nx_vars = 0\ny_vars = [1, 2]\nX, y = apply_sliding_window(df, window_len, horizon=horizon, x_vars=x_vars, y_vars=y_vars)\nprint(np.shares_memory(X, df))\nprint(np.shares_memory(y, df))\nprint(X.shape, y.shape)\ntest_eq(X.shape, (12, 1, 4))\ntest_eq(y.shape, (12, 2, 3))\nX[0], y[0]\n\nFalse\nFalse\n(12, 1, 4) (12, 2, 3)\n\n\n(array([[0, 2, 3, 4]]),\n array([[ 50,  60,  80],\n        [500, 600, 800]]))\n\n\n\nsource\n\n\ndf2Xy\n\n df2Xy (df, sample_col=None, feat_col=None, data_cols=None,\n        target_col=None, steps_in_rows=False, to3d=True, splits=None,\n        sort_by=None, ascending=True, y_func=None, return_names=False)\n\nThis function allows you to transform a pandas dataframe into X and y numpy arrays that can be used to create a TSDataset. sample_col: column that uniquely identifies each sample. feat_col: used for multivariate datasets. It indicates which is the column that indicates the feature by row. data_col: indicates ths column/s where the data is located. If None, it means all columns (except the sample_col, feat_col, and target_col) target_col: indicates the column/s where the target is. steps_in_rows: flag to indicate if each step is in a different row or in a different column (default). to3d: turns X to 3d (including univariate time series) sort_by: this is used to pass any colum/s that are needed to sort the steps in the sequence. If you pass a sample_col and/ or feat_col these will be automatically used before the sort_by column/s, and you don’t need to add them to the sort_by column/s list. y_func: function used to calculate y for each sample (and target_col) return_names: flag to return the names of the columns from where X was generated\n\nsource\n\n\nsplit_Xy\n\n split_Xy (X, y=None, splits=None)\n\n\ndf = pd.DataFrame()\ndf['sample_id'] = np.array([1,1,1,2,2,2,3,3,3])\ndf['var1'] = df['sample_id'] * 10 + df.index.values\ndf['var2'] = df['sample_id'] * 100 + df.index.values\ndf\n\n\n\n\n\n  \n    \n      \n      sample_id\n      var1\n      var2\n    \n  \n  \n    \n      0\n      1\n      10\n      100\n    \n    \n      1\n      1\n      11\n      101\n    \n    \n      2\n      1\n      12\n      102\n    \n    \n      3\n      2\n      23\n      203\n    \n    \n      4\n      2\n      24\n      204\n    \n    \n      5\n      2\n      25\n      205\n    \n    \n      6\n      3\n      36\n      306\n    \n    \n      7\n      3\n      37\n      307\n    \n    \n      8\n      3\n      38\n      308\n    \n  \n\n\n\n\n\nX_df, y_df = df2Xy(df, sample_col='sample_id', steps_in_rows=True)\ntest_eq(X_df[0], np.array([[10, 11, 12], [100, 101, 102]]))\n\n\nn_samples = 1_000\nn_rows = 10_000\n\nsample_ids = np.arange(n_samples).repeat(n_rows//n_samples).reshape(-1,1)\nfeat_ids = np.tile(np.arange(n_rows // n_samples), n_samples).reshape(-1,1)\ncont = np.random.randn(n_rows, 6)\nind_cat = np.random.randint(0, 3, (n_rows, 1))\ntarget = np.array([0,1,2])[ind_cat]\nind_cat2 = np.random.randint(0, 3, (n_rows, 1))\ntarget2 = np.array([100,200,300])[ind_cat2]\ndata = np.concatenate([sample_ids, feat_ids, cont, target, target], -1)\ncolumns = ['sample_id', 'feat_id'] + (np.arange(6) + 1).astype(str).tolist() + ['target'] + ['target2']\ndf = pd.DataFrame(data, columns=columns)\nidx = random_choice(np.arange(len(df)), len(df), False)\nnew_dtypes = {'sample_id':np.int32, 'feat_id':np.int32, '1':np.float32, '2':np.float32, '3':np.float32, '4':np.float32, '5':np.float32, '6':np.float32}\ndf = df.astype(dtype=new_dtypes)\ndf = df.loc[idx].reset_index(drop=True)\ndf\n\n\n\n\n\n  \n    \n      \n      sample_id\n      feat_id\n      1\n      2\n      3\n      4\n      5\n      6\n      target\n      target2\n    \n  \n  \n    \n      0\n      305\n      5\n      0.209939\n      0.158130\n      -0.970416\n      -0.965198\n      0.627537\n      -1.523784\n      2.0\n      2.0\n    \n    \n      1\n      848\n      2\n      0.309804\n      -0.392991\n      -0.698594\n      0.525771\n      -1.304203\n      0.644886\n      2.0\n      2.0\n    \n    \n      2\n      290\n      6\n      -0.328014\n      -0.290428\n      -0.071598\n      0.024535\n      0.354077\n      0.357194\n      1.0\n      1.0\n    \n    \n      3\n      634\n      4\n      0.885713\n      -0.792051\n      0.133696\n      -0.409837\n      -1.376452\n      1.322753\n      2.0\n      2.0\n    \n    \n      4\n      44\n      8\n      -1.037799\n      1.607723\n      -0.088286\n      -0.279098\n      0.267548\n      -0.385802\n      2.0\n      2.0\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      9995\n      173\n      1\n      0.346992\n      -0.736779\n      -0.763613\n      2.006004\n      -0.691721\n      -1.440827\n      0.0\n      0.0\n    \n    \n      9996\n      685\n      6\n      1.252947\n      0.151954\n      -0.151493\n      0.370482\n      -2.393744\n      -0.809685\n      0.0\n      0.0\n    \n    \n      9997\n      221\n      6\n      1.723383\n      0.401157\n      0.678439\n      -0.995303\n      -1.059554\n      0.149291\n      2.0\n      2.0\n    \n    \n      9998\n      635\n      7\n      1.850614\n      0.332822\n      1.563598\n      0.271247\n      -0.849911\n      0.655552\n      0.0\n      0.0\n    \n    \n      9999\n      535\n      1\n      0.550994\n      0.404426\n      0.112994\n      -0.481868\n      -0.511104\n      0.451547\n      1.0\n      1.0\n    \n  \n\n10000 rows × 10 columns\n\n\n\n\nfrom scipy.stats import mode\n\n\ndef y_func(o): return mode(o, axis=1).mode\nX, y = df2xy(df, sample_col='sample_id', feat_col='feat_id', target_col=['target', 'target2'], sort_by=['sample_id', 'feat_id'], y_func=y_func)\ntest_eq(X.shape, (1000, 10, 6))\ntest_eq(y.shape, (1000, 2))\nrand_idx = np.random.randint(0, np.max(df.sample_id))\nsorted_df = df.sort_values(by=['sample_id', 'feat_id'], kind='stable').reset_index(drop=True)\ntest_eq(X[rand_idx], sorted_df[sorted_df.sample_id == rand_idx][['1', '2', '3', '4', '5', '6']].values)\ntest_eq(np.squeeze(mode(sorted_df[sorted_df.sample_id == rand_idx][['target', 'target2']].values).mode), y[rand_idx])\n\n\n# Univariate\nfrom io import StringIO\n\n\nTESTDATA = StringIO(\"\"\"sample_id;value_0;value_1;target\n    rob;2;3;0\n    alice;6;7;1\n    eve;11;12;2\n    \"\"\")\n\ndf = pd.read_csv(TESTDATA, sep=\";\")\ndisplay(df)\nX, y = df2Xy(df, sample_col='sample_id', target_col='target', data_cols=['value_0', 'value_1'], sort_by='sample_id')\ntest_eq(X.shape, (3, 1, 2))\ntest_eq(y.shape, (3,))\nX, y\n\n\n\n\n\n  \n    \n      \n      sample_id\n      value_0\n      value_1\n      target\n    \n  \n  \n    \n      0\n      rob\n      2\n      3\n      0\n    \n    \n      1\n      alice\n      6\n      7\n      1\n    \n    \n      2\n      eve\n      11\n      12\n      2\n    \n  \n\n\n\n\n(array([[[ 6,  7]],\n \n        [[11, 12]],\n \n        [[ 2,  3]]]),\n array([1, 2, 0]))\n\n\n\n# Univariate\nTESTDATA = StringIO(\"\"\"sample_id;timestep;values;target\n    rob;1;2;0\n    alice;1;6;1\n    eve;1;11;2\n    \n    rob;2;3;0\n    alice;2;7;1\n    eve;2;12;2\n    \"\"\")\n\ndf = pd.read_csv(TESTDATA, sep=\";\")\ndisplay(df)\ndef y_func(o): return mode(o, axis=1).mode\nX, y = df2xy(df, sample_col='sample_id', target_col='target', data_cols=['values'], sort_by='timestep', to3d=True, y_func=y_func)\ntest_eq(X.shape, (3, 1, 2))\ntest_eq(y.shape, (3, ))\nprint(X, y)\n\n\n\n\n\n  \n    \n      \n      sample_id\n      timestep\n      values\n      target\n    \n  \n  \n    \n      0\n      rob\n      1\n      2\n      0\n    \n    \n      1\n      alice\n      1\n      6\n      1\n    \n    \n      2\n      eve\n      1\n      11\n      2\n    \n    \n      3\n      rob\n      2\n      3\n      0\n    \n    \n      4\n      alice\n      2\n      7\n      1\n    \n    \n      5\n      eve\n      2\n      12\n      2\n    \n  \n\n\n\n\n[[[ 6  7]]\n\n [[11 12]]\n\n [[ 2  3]]] [1 2 0]\n\n\n\n# Multivariate\nTESTDATA = StringIO(\"\"\"sample_id;trait;value_0;value_1;target\n    rob;green;2;3;0\n    rob;yellow;3;4;0\n    rob;blue;4;5;0\n    rob;red;5;6;0\n    alice;green;6;7;1\n    alice;yellow;7;8;1\n    alice;blue;8;9;1\n    alice;red;9;10;1\n    eve;yellow;11;12;2\n    eve;green;10;11;2\n    eve;blue;12;12;2\n    eve;red;13;14;2\n    \"\"\")\n\ndf = pd.read_csv(TESTDATA, sep=\";\")\nidx = random_choice(len(df), len(df), False)\ndf = df.iloc[idx]\ndisplay(df)\ndef y_func(o): return mode(o, axis=1).mode\nX, y = df2xy(df, sample_col='sample_id', feat_col='trait', target_col='target', data_cols=['value_0', 'value_1'], y_func=y_func)\nprint(X, y)\ntest_eq(X.shape, (3, 4, 2))\ntest_eq(y.shape, (3,))\n\n\n\n\n\n  \n    \n      \n      sample_id\n      trait\n      value_0\n      value_1\n      target\n    \n  \n  \n    \n      5\n      alice\n      yellow\n      7\n      8\n      1\n    \n    \n      3\n      rob\n      red\n      5\n      6\n      0\n    \n    \n      9\n      eve\n      green\n      10\n      11\n      2\n    \n    \n      6\n      alice\n      blue\n      8\n      9\n      1\n    \n    \n      11\n      eve\n      red\n      13\n      14\n      2\n    \n    \n      7\n      alice\n      red\n      9\n      10\n      1\n    \n    \n      2\n      rob\n      blue\n      4\n      5\n      0\n    \n    \n      4\n      alice\n      green\n      6\n      7\n      1\n    \n    \n      8\n      eve\n      yellow\n      11\n      12\n      2\n    \n    \n      1\n      rob\n      yellow\n      3\n      4\n      0\n    \n    \n      0\n      rob\n      green\n      2\n      3\n      0\n    \n    \n      10\n      eve\n      blue\n      12\n      12\n      2\n    \n  \n\n\n\n\n[[[ 8  9]\n  [ 6  7]\n  [ 9 10]\n  [ 7  8]]\n\n [[12 12]\n  [10 11]\n  [13 14]\n  [11 12]]\n\n [[ 4  5]\n  [ 2  3]\n  [ 5  6]\n  [ 3  4]]] [1 2 0]\n\n\n\n# Multivariate, multi-label\nTESTDATA = StringIO(\"\"\"sample_id;trait;value_0;value_1;target1;target2\n    rob;green;2;3;0;0\n    rob;yellow;3;4;0;0\n    rob;blue;4;5;0;0\n    rob;red;5;6;0;0\n    alice;green;6;7;1;0\n    alice;yellow;7;8;1;0\n    alice;blue;8;9;1;0\n    alice;red;9;10;1;0\n    eve;yellow;11;12;2;1\n    eve;green;10;11;2;1\n    eve;blue;12;12;2;1\n    eve;red;13;14;2;1\n    \"\"\")\n\ndf = pd.read_csv(TESTDATA, sep=\";\")\ndisplay(df)\ndef y_func(o): return mode(o, axis=1).mode\nX, y = df2xy(df, sample_col='sample_id', feat_col='trait', target_col=['target1', 'target2'], data_cols=['value_0', 'value_1'], y_func=y_func)\ntest_eq(X.shape, (3, 4, 2))\ntest_eq(y.shape, (3, 2))\nprint(X, y)\n\n\n\n\n\n  \n    \n      \n      sample_id\n      trait\n      value_0\n      value_1\n      target1\n      target2\n    \n  \n  \n    \n      0\n      rob\n      green\n      2\n      3\n      0\n      0\n    \n    \n      1\n      rob\n      yellow\n      3\n      4\n      0\n      0\n    \n    \n      2\n      rob\n      blue\n      4\n      5\n      0\n      0\n    \n    \n      3\n      rob\n      red\n      5\n      6\n      0\n      0\n    \n    \n      4\n      alice\n      green\n      6\n      7\n      1\n      0\n    \n    \n      5\n      alice\n      yellow\n      7\n      8\n      1\n      0\n    \n    \n      6\n      alice\n      blue\n      8\n      9\n      1\n      0\n    \n    \n      7\n      alice\n      red\n      9\n      10\n      1\n      0\n    \n    \n      8\n      eve\n      yellow\n      11\n      12\n      2\n      1\n    \n    \n      9\n      eve\n      green\n      10\n      11\n      2\n      1\n    \n    \n      10\n      eve\n      blue\n      12\n      12\n      2\n      1\n    \n    \n      11\n      eve\n      red\n      13\n      14\n      2\n      1\n    \n  \n\n\n\n\n[[[ 8  9]\n  [ 6  7]\n  [ 9 10]\n  [ 7  8]]\n\n [[12 12]\n  [10 11]\n  [13 14]\n  [11 12]]\n\n [[ 4  5]\n  [ 2  3]\n  [ 5  6]\n  [ 3  4]]] [[1 0]\n [2 1]\n [0 0]]\n\n\n\n# Multivariate, unlabeled\nTESTDATA = StringIO(\"\"\"sample_id;trait;value_0;value_1;target\n    rob;green;2;3;0\n    rob;yellow;3;4;0\n    rob;blue;4;5;0\n    rob;red;5;6;0\n    alice;green;6;7;1\n    alice;yellow;7;8;1\n    alice;blue;8;9;1\n    alice;red;9;10;1\n    eve;yellow;11;12;2\n    eve;green;10;11;2\n    eve;blue;12;12;2\n    eve;red;13;14;2\n    \"\"\")\n\ndf = pd.read_csv(TESTDATA, sep=\";\")\nidx = random_choice(len(df), len(df), False)\ndf = df.iloc[idx]\ndisplay(df)\ndef y_func(o): return mode(o, axis=1).mode\nX, y = df2xy(df, sample_col='sample_id', feat_col='trait', data_cols=['value_0', 'value_1'], y_func=y_func)\nprint(X, y)\ntest_eq(X.shape, (3, 4, 2))\ntest_eq(y, None)\n\n\n\n\n\n  \n    \n      \n      sample_id\n      trait\n      value_0\n      value_1\n      target\n    \n  \n  \n    \n      0\n      rob\n      green\n      2\n      3\n      0\n    \n    \n      7\n      alice\n      red\n      9\n      10\n      1\n    \n    \n      2\n      rob\n      blue\n      4\n      5\n      0\n    \n    \n      11\n      eve\n      red\n      13\n      14\n      2\n    \n    \n      6\n      alice\n      blue\n      8\n      9\n      1\n    \n    \n      10\n      eve\n      blue\n      12\n      12\n      2\n    \n    \n      8\n      eve\n      yellow\n      11\n      12\n      2\n    \n    \n      3\n      rob\n      red\n      5\n      6\n      0\n    \n    \n      1\n      rob\n      yellow\n      3\n      4\n      0\n    \n    \n      4\n      alice\n      green\n      6\n      7\n      1\n    \n    \n      9\n      eve\n      green\n      10\n      11\n      2\n    \n    \n      5\n      alice\n      yellow\n      7\n      8\n      1\n    \n  \n\n\n\n\n[[[ 8  9]\n  [ 6  7]\n  [ 9 10]\n  [ 7  8]]\n\n [[12 12]\n  [10 11]\n  [13 14]\n  [11 12]]\n\n [[ 4  5]\n  [ 2  3]\n  [ 5  6]\n  [ 3  4]]] None\n\n\n\nTESTDATA = StringIO(\"\"\"sample_id;trait;timestep;values;target\n    rob;green;1;2;0\n    rob;yellow;1;3;0\n    rob;blue;1;4;0\n    rob;red;1;5;0\n    alice;green;1;6;1\n    alice;yellow;1;7;1\n    alice;blue;1;8;1\n    alice;red;1;9;1\n    eve;yellow;1;11;2\n    eve;green;1;10;2\n    eve;blue;1;12;2\n    eve;red;1;13;2\n    \n    rob;green;2;3;0\n    rob;yellow;2;4;0\n    rob;blue;2;5;0\n    rob;red;2;6;0\n    alice;green;2;7;1\n    alice;yellow;2;8;1\n    alice;blue;2;9;1\n    alice;red;2;10;1\n    eve;yellow;2;12;2\n    eve;green;2;11;2\n    eve;blue;2;13;2\n    eve;red;2;14;2\n    \"\"\")\n\ndf = pd.read_csv(TESTDATA, sep=\";\")\ndisplay(df)\ndef y_func(o): return mode(o, axis=1).mode\nX, y = df2xy(df, sample_col='sample_id', feat_col='trait', sort_by='timestep', target_col='target', data_cols=['values'], y_func=y_func)\nprint(X, y)\ntest_eq(X.shape, (3, 4, 2))\ntest_eq(y.shape, (3, ))\n\n\n\n\n\n  \n    \n      \n      sample_id\n      trait\n      timestep\n      values\n      target\n    \n  \n  \n    \n      0\n      rob\n      green\n      1\n      2\n      0\n    \n    \n      1\n      rob\n      yellow\n      1\n      3\n      0\n    \n    \n      2\n      rob\n      blue\n      1\n      4\n      0\n    \n    \n      3\n      rob\n      red\n      1\n      5\n      0\n    \n    \n      4\n      alice\n      green\n      1\n      6\n      1\n    \n    \n      5\n      alice\n      yellow\n      1\n      7\n      1\n    \n    \n      6\n      alice\n      blue\n      1\n      8\n      1\n    \n    \n      7\n      alice\n      red\n      1\n      9\n      1\n    \n    \n      8\n      eve\n      yellow\n      1\n      11\n      2\n    \n    \n      9\n      eve\n      green\n      1\n      10\n      2\n    \n    \n      10\n      eve\n      blue\n      1\n      12\n      2\n    \n    \n      11\n      eve\n      red\n      1\n      13\n      2\n    \n    \n      12\n      rob\n      green\n      2\n      3\n      0\n    \n    \n      13\n      rob\n      yellow\n      2\n      4\n      0\n    \n    \n      14\n      rob\n      blue\n      2\n      5\n      0\n    \n    \n      15\n      rob\n      red\n      2\n      6\n      0\n    \n    \n      16\n      alice\n      green\n      2\n      7\n      1\n    \n    \n      17\n      alice\n      yellow\n      2\n      8\n      1\n    \n    \n      18\n      alice\n      blue\n      2\n      9\n      1\n    \n    \n      19\n      alice\n      red\n      2\n      10\n      1\n    \n    \n      20\n      eve\n      yellow\n      2\n      12\n      2\n    \n    \n      21\n      eve\n      green\n      2\n      11\n      2\n    \n    \n      22\n      eve\n      blue\n      2\n      13\n      2\n    \n    \n      23\n      eve\n      red\n      2\n      14\n      2\n    \n  \n\n\n\n\n[[[ 8  9]\n  [ 6  7]\n  [ 9 10]\n  [ 7  8]]\n\n [[12 13]\n  [10 11]\n  [13 14]\n  [11 12]]\n\n [[ 4  5]\n  [ 2  3]\n  [ 5  6]\n  [ 3  4]]] [1 2 0]\n\n\n\nsource\n\n\ndf2np3d\n\n df2np3d (df, groupby, data_cols=None)\n\nTransforms a df (with the same number of rows per group in groupby) to a 3d ndarray\n\nuser = np.array([1,2]).repeat(4).reshape(-1,1)\nval = np.random.rand(8, 3)\ndata = np.concatenate([user, val], axis=-1)\ndf = pd.DataFrame(data, columns=['user', 'x1', 'x2', 'x3'])\ntest_eq(df2np3d(df, ['user'], ['x1', 'x2', 'x3']).shape, (2, 3, 4))\n\n\nsource\n\n\nadd_missing_value_cols\n\n add_missing_value_cols (df, cols=None, dtype=<class 'float'>,\n                         fill_value=None)\n\n\ndata = np.random.randn(10, 2)\nmask = data > .8\ndata[mask] = np.nan\ndf = pd.DataFrame(data, columns=['A', 'B'])\ndf = add_missing_value_cols(df, cols=None, dtype=float)\ntest_eq(df['A'].isnull().sum(), df['missing_A'].sum())\ntest_eq(df['B'].isnull().sum(), df['missing_B'].sum())\ndf\n\n\n\n\n\n  \n    \n      \n      A\n      B\n      missing_A\n      missing_B\n    \n  \n  \n    \n      0\n      0.095985\n      0.360680\n      0.0\n      0.0\n    \n    \n      1\n      -0.610844\n      -1.609052\n      0.0\n      0.0\n    \n    \n      2\n      -0.067111\n      NaN\n      0.0\n      1.0\n    \n    \n      3\n      0.609950\n      0.458180\n      0.0\n      0.0\n    \n    \n      4\n      -0.440086\n      0.318771\n      0.0\n      0.0\n    \n    \n      5\n      -0.565956\n      0.471049\n      0.0\n      0.0\n    \n    \n      6\n      -0.516669\n      -0.613728\n      0.0\n      0.0\n    \n    \n      7\n      0.403820\n      -0.051065\n      0.0\n      0.0\n    \n    \n      8\n      NaN\n      NaN\n      1.0\n      1.0\n    \n    \n      9\n      NaN\n      -1.941093\n      1.0\n      0.0\n    \n  \n\n\n\n\n\nsource\n\n\nadd_missing_timestamps\n\n add_missing_timestamps (df, datetime_col=None, use_index=False,\n                         unique_id_cols=None, groupby=None,\n                         fill_value=nan, range_by_group=True,\n                         start_date=None, end_date=None, freq=None)\n\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ndf\n\n\npandas DataFrame\n\n\ndatetime_col\nNoneType\nNone\ncolumn that contains the datetime data (without duplicates within groups)\n\n\nuse_index\nbool\nFalse\nindicates if the index contains the datetime data\n\n\nunique_id_cols\nNoneType\nNone\ncolumn used to identify unique_ids\n\n\ngroupby\nNoneType\nNone\nsame as unique_id_cols. Will be deprecated. Kept for compatiblity.\n\n\nfill_value\nfloat\nnan\nvalues that will be insert where missing dates exist. Default:np.nan\n\n\nrange_by_group\nbool\nTrue\nif True, dates will be filled between min and max dates for each group. Otherwise, between the min and max dates in the df.\n\n\nstart_date\nNoneType\nNone\nstart date to fill in missing dates (same for all unique_ids)\n\n\nend_date\nNoneType\nNone\nend date to fill in missing dates (same for all unique_ids)\n\n\nfreq\nNoneType\nNone\nfrequency used to fill in the missing datetime\n\n\n\n\n# Filling dates between min and max dates\ndates = pd.date_range('2021-05-01', '2021-05-07').values\ndata = np.zeros((len(dates), 3))\ndata[:, 0] = dates\ndata[:, 1] = np.random.rand(len(dates))\ndata[:, 2] = np.random.rand(len(dates))\ncols = ['date', 'feature1', 'feature2']\ndate_df = pd.DataFrame(data, columns=cols).astype({'date': 'datetime64[ns]', 'feature1': float, 'feature2': float})\ndate_df_with_missing_dates = date_df.drop([1,3]).reset_index(drop=True)\ndate_df_with_missing_dates\n\n\n\n\n\n  \n    \n      \n      date\n      feature1\n      feature2\n    \n  \n  \n    \n      0\n      2021-05-01\n      0.189799\n      0.611582\n    \n    \n      1\n      2021-05-03\n      0.249404\n      0.788933\n    \n    \n      2\n      2021-05-05\n      0.342298\n      0.612203\n    \n    \n      3\n      2021-05-06\n      0.922762\n      0.908293\n    \n    \n      4\n      2021-05-07\n      0.641153\n      0.366178\n    \n  \n\n\n\n\n\n# No groups\nexpected_output_df = date_df.copy()\nexpected_output_df.loc[[1,3], ['feature1', 'feature2']] = np.nan\ndisplay(expected_output_df)\noutput_df = add_missing_timestamps(date_df_with_missing_dates.copy(), \n                                   'date', \n                                   unique_id_cols=None, \n                                   fill_value=np.nan, \n                                   range_by_group=False)\ntest_eq(output_df, expected_output_df)\n\n\n\n\n\n  \n    \n      \n      date\n      feature1\n      feature2\n    \n  \n  \n    \n      0\n      2021-05-01\n      0.189799\n      0.611582\n    \n    \n      1\n      2021-05-02\n      NaN\n      NaN\n    \n    \n      2\n      2021-05-03\n      0.249404\n      0.788933\n    \n    \n      3\n      2021-05-04\n      NaN\n      NaN\n    \n    \n      4\n      2021-05-05\n      0.342298\n      0.612203\n    \n    \n      5\n      2021-05-06\n      0.922762\n      0.908293\n    \n    \n      6\n      2021-05-07\n      0.641153\n      0.366178\n    \n  \n\n\n\n\n\n# Filling dates between min and max dates for each value in groupby column\ndates = pd.date_range('2021-05-01', '2021-05-07').values\ndates = np.concatenate((dates, dates))\ndata = np.zeros((len(dates), 4))\ndata[:, 0] = dates\ndata[:, 1] = np.array([0]*(len(dates)//2)+[1]*(len(dates)//2))\ndata[:, 2] = np.random.rand(len(dates))\ndata[:, 3] = np.random.rand(len(dates))\ncols = ['date', 'id', 'feature1', 'feature2']\ndate_df = pd.DataFrame(data, columns=cols).astype({'date': 'datetime64[ns]', 'id': int, 'feature1': float, 'feature2': float})\ndate_df_with_missing_dates = date_df.drop([0,1,3,8,11,13]).reset_index(drop=True)\ndate_df_with_missing_dates\n\n\n\n\n\n  \n    \n      \n      date\n      id\n      feature1\n      feature2\n    \n  \n  \n    \n      0\n      2021-05-03\n      0\n      0.594349\n      0.332724\n    \n    \n      1\n      2021-05-05\n      0\n      0.148300\n      0.320393\n    \n    \n      2\n      2021-05-06\n      0\n      0.682138\n      0.660869\n    \n    \n      3\n      2021-05-07\n      0\n      0.111529\n      0.660884\n    \n    \n      4\n      2021-05-01\n      1\n      0.397841\n      0.230159\n    \n    \n      5\n      2021-05-03\n      1\n      0.533729\n      0.930182\n    \n    \n      6\n      2021-05-04\n      1\n      0.144729\n      0.112380\n    \n    \n      7\n      2021-05-06\n      1\n      0.121597\n      0.413128\n    \n  \n\n\n\n\n\n# groupby='id', range_by_group=True\nexpected_output_df = date_df.drop([0,1,13]).reset_index(drop=True)  \nexpected_output_df.loc[[1,6,9], ['feature1', 'feature2']] = np.nan\ndisplay(expected_output_df)\noutput_df = add_missing_timestamps(date_df_with_missing_dates.copy(), \n                                   'date', \n                                   unique_id_cols='id', \n                                   fill_value=np.nan, \n                                   range_by_group=True)\ntest_eq(expected_output_df, output_df)\n\n\n\n\n\n  \n    \n      \n      date\n      id\n      feature1\n      feature2\n    \n  \n  \n    \n      0\n      2021-05-03\n      0\n      0.594349\n      0.332724\n    \n    \n      1\n      2021-05-04\n      0\n      NaN\n      NaN\n    \n    \n      2\n      2021-05-05\n      0\n      0.148300\n      0.320393\n    \n    \n      3\n      2021-05-06\n      0\n      0.682138\n      0.660869\n    \n    \n      4\n      2021-05-07\n      0\n      0.111529\n      0.660884\n    \n    \n      5\n      2021-05-01\n      1\n      0.397841\n      0.230159\n    \n    \n      6\n      2021-05-02\n      1\n      NaN\n      NaN\n    \n    \n      7\n      2021-05-03\n      1\n      0.533729\n      0.930182\n    \n    \n      8\n      2021-05-04\n      1\n      0.144729\n      0.112380\n    \n    \n      9\n      2021-05-05\n      1\n      NaN\n      NaN\n    \n    \n      10\n      2021-05-06\n      1\n      0.121597\n      0.413128\n    \n  \n\n\n\n\n\n# groupby='id', range_by_group=False\nexpected_output_df = date_df.copy() \nexpected_output_df.loc[[0,1,3,8,11,13], ['feature1', 'feature2']] = np.nan\ndisplay(expected_output_df)\noutput_df = add_missing_timestamps(date_df_with_missing_dates.copy(), \n                                   'date', \n                                   unique_id_cols='id', \n                                   fill_value=np.nan, \n                                   range_by_group=False)\ntest_eq(expected_output_df, output_df)\n\n\n\n\n\n  \n    \n      \n      date\n      id\n      feature1\n      feature2\n    \n  \n  \n    \n      0\n      2021-05-01\n      0\n      NaN\n      NaN\n    \n    \n      1\n      2021-05-02\n      0\n      NaN\n      NaN\n    \n    \n      2\n      2021-05-03\n      0\n      0.594349\n      0.332724\n    \n    \n      3\n      2021-05-04\n      0\n      NaN\n      NaN\n    \n    \n      4\n      2021-05-05\n      0\n      0.148300\n      0.320393\n    \n    \n      5\n      2021-05-06\n      0\n      0.682138\n      0.660869\n    \n    \n      6\n      2021-05-07\n      0\n      0.111529\n      0.660884\n    \n    \n      7\n      2021-05-01\n      1\n      0.397841\n      0.230159\n    \n    \n      8\n      2021-05-02\n      1\n      NaN\n      NaN\n    \n    \n      9\n      2021-05-03\n      1\n      0.533729\n      0.930182\n    \n    \n      10\n      2021-05-04\n      1\n      0.144729\n      0.112380\n    \n    \n      11\n      2021-05-05\n      1\n      NaN\n      NaN\n    \n    \n      12\n      2021-05-06\n      1\n      0.121597\n      0.413128\n    \n    \n      13\n      2021-05-07\n      1\n      NaN\n      NaN\n    \n  \n\n\n\n\n\n# Filling dates between min and max timestamps\ndates = pd.date_range('2021-05-01 000:00', '2021-05-01 20:00', freq='4H').values\ndata = np.zeros((len(dates), 3))\ndata[:, 0] = dates\ndata[:, 1] = np.random.rand(len(dates))\ndata[:, 2] = np.random.rand(len(dates))\ncols = ['date', 'feature1', 'feature2']\ndate_df = pd.DataFrame(data, columns=cols).astype({'date': 'datetime64[ns]', 'feature1': float, 'feature2': float})\ndate_df_with_missing_dates = date_df.drop([1,3]).reset_index(drop=True)\ndate_df_with_missing_dates\n\n\n\n\n\n  \n    \n      \n      date\n      feature1\n      feature2\n    \n  \n  \n    \n      0\n      2021-05-01 00:00:00\n      0.098458\n      0.591324\n    \n    \n      1\n      2021-05-01 08:00:00\n      0.507863\n      0.407036\n    \n    \n      2\n      2021-05-01 16:00:00\n      0.063839\n      0.595914\n    \n    \n      3\n      2021-05-01 20:00:00\n      0.310010\n      0.803500\n    \n  \n\n\n\n\n\n# No groups\nexpected_output_df = date_df.copy()\nexpected_output_df.loc[[1,3], ['feature1', 'feature2']] = np.nan\ndisplay(expected_output_df)\noutput_df = add_missing_timestamps(date_df_with_missing_dates.copy(), 'date', groupby=None, fill_value=np.nan, range_by_group=False, freq='4H')\ntest_eq(output_df, expected_output_df)\n\n\n\n\n\n  \n    \n      \n      date\n      feature1\n      feature2\n    \n  \n  \n    \n      0\n      2021-05-01 00:00:00\n      0.098458\n      0.591324\n    \n    \n      1\n      2021-05-01 04:00:00\n      NaN\n      NaN\n    \n    \n      2\n      2021-05-01 08:00:00\n      0.507863\n      0.407036\n    \n    \n      3\n      2021-05-01 12:00:00\n      NaN\n      NaN\n    \n    \n      4\n      2021-05-01 16:00:00\n      0.063839\n      0.595914\n    \n    \n      5\n      2021-05-01 20:00:00\n      0.310010\n      0.803500\n    \n  \n\n\n\n\n\n# Filling missing values between min and max timestamps for each value in groupby column\n\ndates = pd.date_range('2021-05-01 000:00', '2021-05-01 20:00', freq='4H').values\ndates = np.concatenate((dates, dates))\ndata = np.zeros((len(dates), 4))\ndata[:, 0] = dates\ndata[:, 1] = np.array([0]*(len(dates)//2)+[1]*(len(dates)//2))\ndata[:, 2] = np.random.rand(len(dates))\ndata[:, 3] = np.random.rand(len(dates))\ncols = ['date', 'id', 'feature1', 'feature2']\ndate_df = pd.DataFrame(data, columns=cols).astype({'date': 'datetime64[ns]', 'id': int, 'feature1': float, 'feature2': float})\ndate_df_with_missing_dates = date_df.drop([0,1,3,8,9,11]).reset_index(drop=True)\ndate_df_with_missing_dates\n\n\n\n\n\n  \n    \n      \n      date\n      id\n      feature1\n      feature2\n    \n  \n  \n    \n      0\n      2021-05-01 08:00:00\n      0\n      0.524909\n      0.531803\n    \n    \n      1\n      2021-05-01 16:00:00\n      0\n      0.512648\n      0.353095\n    \n    \n      2\n      2021-05-01 20:00:00\n      0\n      0.826829\n      0.985137\n    \n    \n      3\n      2021-05-01 00:00:00\n      1\n      0.347581\n      0.423076\n    \n    \n      4\n      2021-05-01 04:00:00\n      1\n      0.550143\n      0.103867\n    \n    \n      5\n      2021-05-01 16:00:00\n      1\n      0.156187\n      0.854512\n    \n  \n\n\n\n\n\n# groupby='id', range_by_group=True\nexpected_output_df = date_df.drop([0,1,11]).reset_index(drop=True)  \nexpected_output_df.loc[[1,6,7], ['feature1', 'feature2']] = np.nan\ndisplay(expected_output_df)\noutput_df = add_missing_timestamps(date_df_with_missing_dates.copy(),\n                                   'date', \n                                   groupby='id', \n                                   fill_value=np.nan, \n                                   range_by_group=True, \n                                   freq='4H')\ntest_eq(expected_output_df, output_df)\n\n\n\n\n\n  \n    \n      \n      date\n      id\n      feature1\n      feature2\n    \n  \n  \n    \n      0\n      2021-05-01 08:00:00\n      0\n      0.524909\n      0.531803\n    \n    \n      1\n      2021-05-01 12:00:00\n      0\n      NaN\n      NaN\n    \n    \n      2\n      2021-05-01 16:00:00\n      0\n      0.512648\n      0.353095\n    \n    \n      3\n      2021-05-01 20:00:00\n      0\n      0.826829\n      0.985137\n    \n    \n      4\n      2021-05-01 00:00:00\n      1\n      0.347581\n      0.423076\n    \n    \n      5\n      2021-05-01 04:00:00\n      1\n      0.550143\n      0.103867\n    \n    \n      6\n      2021-05-01 08:00:00\n      1\n      NaN\n      NaN\n    \n    \n      7\n      2021-05-01 12:00:00\n      1\n      NaN\n      NaN\n    \n    \n      8\n      2021-05-01 16:00:00\n      1\n      0.156187\n      0.854512\n    \n  \n\n\n\n\n\n# groupby='id', range_by_group=False\nexpected_output_df = date_df.copy() \nexpected_output_df.loc[[0,1,3,8,9,11], ['feature1', 'feature2']] = np.nan\ndisplay(expected_output_df)\noutput_df = add_missing_timestamps(date_df_with_missing_dates.copy(), \n                                   'date', \n                                   groupby='id', \n                                   fill_value=np.nan, \n                                   range_by_group=False, \n                                   freq='4H')\ntest_eq(expected_output_df, output_df)\n\n\n\n\n\n  \n    \n      \n      date\n      id\n      feature1\n      feature2\n    \n  \n  \n    \n      0\n      2021-05-01 00:00:00\n      0\n      NaN\n      NaN\n    \n    \n      1\n      2021-05-01 04:00:00\n      0\n      NaN\n      NaN\n    \n    \n      2\n      2021-05-01 08:00:00\n      0\n      0.524909\n      0.531803\n    \n    \n      3\n      2021-05-01 12:00:00\n      0\n      NaN\n      NaN\n    \n    \n      4\n      2021-05-01 16:00:00\n      0\n      0.512648\n      0.353095\n    \n    \n      5\n      2021-05-01 20:00:00\n      0\n      0.826829\n      0.985137\n    \n    \n      6\n      2021-05-01 00:00:00\n      1\n      0.347581\n      0.423076\n    \n    \n      7\n      2021-05-01 04:00:00\n      1\n      0.550143\n      0.103867\n    \n    \n      8\n      2021-05-01 08:00:00\n      1\n      NaN\n      NaN\n    \n    \n      9\n      2021-05-01 12:00:00\n      1\n      NaN\n      NaN\n    \n    \n      10\n      2021-05-01 16:00:00\n      1\n      0.156187\n      0.854512\n    \n    \n      11\n      2021-05-01 20:00:00\n      1\n      NaN\n      NaN\n    \n  \n\n\n\n\n\n# No groups, with duplicate dates ==> FAILS\ndates = pd.date_range('2021-05-01 000:00', '2021-05-01 20:00', freq='4H').values\ndata = np.zeros((len(dates), 3))\ndata[:, 0] = dates\ndata[:, 1] = np.random.rand(len(dates))\ndata[:, 2] = np.random.rand(len(dates))\ncols = ['date', 'feature1', 'feature2']\ndate_df = pd.DataFrame(data, columns=cols).astype({'date': 'datetime64[ns]', 'feature1': float, 'feature2': float})\ndate_df_with_missing_dates = date_df.drop([1,3]).reset_index(drop=True)\ndate_df_with_missing_dates.loc[3, 'date'] = date_df_with_missing_dates.loc[2, 'date']\ndisplay(date_df_with_missing_dates)\ntest_fail(add_missing_timestamps, args=[date_df_with_missing_dates, 'date'], kwargs=dict(groupby=None, fill_value=np.nan, range_by_group=False, freq='4H'), )\n\n\n\n\n\n  \n    \n      \n      date\n      feature1\n      feature2\n    \n  \n  \n    \n      0\n      2021-05-01 00:00:00\n      0.320709\n      0.139789\n    \n    \n      1\n      2021-05-01 08:00:00\n      0.574888\n      0.931896\n    \n    \n      2\n      2021-05-01 16:00:00\n      0.182917\n      0.292427\n    \n    \n      3\n      2021-05-01 16:00:00\n      0.438391\n      0.508668\n    \n  \n\n\n\n\n\n# groupby='id', range_by_group=True, with duplicate dates ==> FAILS\n\ndates = pd.date_range('2021-05-01 000:00', '2021-05-01 20:00', freq='4H').values\ndates = np.concatenate((dates, dates))\ndata = np.zeros((len(dates), 4))\ndata[:, 0] = dates\ndata[:, 1] = np.array([0]*(len(dates)//2)+[1]*(len(dates)//2))\ndata[:, 2] = np.random.rand(len(dates))\ndata[:, 3] = np.random.rand(len(dates))\ncols = ['date', 'id', 'feature1', 'feature2']\ndate_df = pd.DataFrame(data, columns=cols).astype({'date': 'datetime64[ns]', 'id': int, 'feature1': float, 'feature2': float})\ndate_df_with_missing_dates = date_df.drop([0,1,8,9,11]).reset_index(drop=True)\ndate_df_with_missing_dates.loc[3, 'date'] = date_df_with_missing_dates.loc[2, 'date']\ndisplay(date_df_with_missing_dates)\ntest_fail(add_missing_timestamps, args=[date_df_with_missing_dates, 'date'], kwargs=dict(groupby='id', fill_value=np.nan, range_by_group=True, freq='4H'), \n          contains='cannot handle a non-unique multi-index!')\n\n\n\n\n\n  \n    \n      \n      date\n      id\n      feature1\n      feature2\n    \n  \n  \n    \n      0\n      2021-05-01 08:00:00\n      0\n      0.066008\n      0.364110\n    \n    \n      1\n      2021-05-01 12:00:00\n      0\n      0.218200\n      0.896896\n    \n    \n      2\n      2021-05-01 16:00:00\n      0\n      0.191737\n      0.758464\n    \n    \n      3\n      2021-05-01 16:00:00\n      0\n      0.664730\n      0.097248\n    \n    \n      4\n      2021-05-01 00:00:00\n      1\n      0.872079\n      0.619036\n    \n    \n      5\n      2021-05-01 04:00:00\n      1\n      0.218482\n      0.717556\n    \n    \n      6\n      2021-05-01 16:00:00\n      1\n      0.410076\n      0.317791\n    \n  \n\n\n\n\n\n# groupby='id', range_by_group=FALSE, with duplicate dates ==> FAILS\n\ndates = pd.date_range('2021-05-01 000:00', '2021-05-01 20:00', freq='4H').values\ndates = np.concatenate((dates, dates))\ndata = np.zeros((len(dates), 4))\ndata[:, 0] = dates\ndata[:, 1] = np.array([0]*(len(dates)//2)+[1]*(len(dates)//2))\ndata[:, 2] = np.random.rand(len(dates))\ndata[:, 3] = np.random.rand(len(dates))\ncols = ['date', 'id', 'feature1', 'feature2']\ndate_df = pd.DataFrame(data, columns=cols).astype({'date': 'datetime64[ns]', 'id': int, 'feature1': float, 'feature2': float})\ndate_df_with_missing_dates = date_df.drop([0,1,8,9,11]).reset_index(drop=True)\ndate_df_with_missing_dates.loc[3, 'date'] = date_df_with_missing_dates.loc[2, 'date']\ndisplay(date_df_with_missing_dates)\ntest_fail(add_missing_timestamps, args=[date_df_with_missing_dates, 'date'], kwargs=dict(groupby='id', fill_value=np.nan, range_by_group=False, freq='4H'), \n          contains='cannot handle a non-unique multi-index!')\n\n\n\n\n\n  \n    \n      \n      date\n      id\n      feature1\n      feature2\n    \n  \n  \n    \n      0\n      2021-05-01 08:00:00\n      0\n      0.422054\n      0.160612\n    \n    \n      1\n      2021-05-01 12:00:00\n      0\n      0.672205\n      0.833167\n    \n    \n      2\n      2021-05-01 16:00:00\n      0\n      0.781480\n      0.381270\n    \n    \n      3\n      2021-05-01 16:00:00\n      0\n      0.369880\n      0.522789\n    \n    \n      4\n      2021-05-01 00:00:00\n      1\n      0.417485\n      0.350774\n    \n    \n      5\n      2021-05-01 04:00:00\n      1\n      0.572383\n      0.714410\n    \n    \n      6\n      2021-05-01 16:00:00\n      1\n      0.539457\n      0.366246\n    \n  \n\n\n\n\n\nsource\n\n\ntime_encoding\n\n time_encoding (series, freq, max_val=None)\n\nTransforms a pandas series of dtype datetime64 (of any freq) or DatetimeIndex into 2 float arrays\nAvailable options: microsecond, millisecond, second, minute, hour, day = day_of_month = dayofmonth, day_of_week = weekday = dayofweek, day_of_year = dayofyear, week = week_of_year = weekofyear, month and year\n\nfor freq in ['microsecond', 'second', 'minute', 'hour', 'day', 'dayofweek', 'dayofyear', 'month']:\n    tdf = pd.DataFrame(pd.date_range('2021-03-01', dt.datetime.today()), columns=['date'])\n    a,b = time_encoding(tdf.date, freq=freq)\n    plt.plot(a)\n    plt.plot(b)\n    plt.title(freq)\n    plt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfor freq in ['microsecond', 'second', 'minute', 'hour', 'day', 'dayofweek', 'dayofyear', 'month']:\n    dateindex = pd.date_range('2021-03-01', dt.datetime.today())\n    a,b = time_encoding(dateindex, freq=freq)\n    plt.plot(a)\n    plt.plot(b)\n    plt.title(freq)\n    plt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ndow_sin, dow_cos = time_encoding(date_df['date'], 'dayofweek')\nplt.plot(dow_sin)\nplt.plot(dow_cos)\nplt.title('DayOfWeek')\nplt.show()\ndate_df['dow_sin'] = dow_sin\ndate_df['dow_cos'] = dow_cos\ndate_df\n\n\n\n\n\n\n\n\n  \n    \n      \n      date\n      id\n      feature1\n      feature2\n      dow_sin\n      dow_cos\n    \n  \n  \n    \n      0\n      2021-05-01 00:00:00\n      0\n      0.217187\n      0.741391\n      -0.974928\n      -0.222521\n    \n    \n      1\n      2021-05-01 04:00:00\n      0\n      0.211557\n      0.354886\n      -0.974928\n      -0.222521\n    \n    \n      2\n      2021-05-01 08:00:00\n      0\n      0.422054\n      0.160612\n      -0.974928\n      -0.222521\n    \n    \n      3\n      2021-05-01 12:00:00\n      0\n      0.672205\n      0.833167\n      -0.974928\n      -0.222521\n    \n    \n      4\n      2021-05-01 16:00:00\n      0\n      0.781480\n      0.381270\n      -0.974928\n      -0.222521\n    \n    \n      5\n      2021-05-01 20:00:00\n      0\n      0.369880\n      0.522789\n      -0.974928\n      -0.222521\n    \n    \n      6\n      2021-05-01 00:00:00\n      1\n      0.417485\n      0.350774\n      -0.974928\n      -0.222521\n    \n    \n      7\n      2021-05-01 04:00:00\n      1\n      0.572383\n      0.714410\n      -0.974928\n      -0.222521\n    \n    \n      8\n      2021-05-01 08:00:00\n      1\n      0.906978\n      0.034521\n      -0.974928\n      -0.222521\n    \n    \n      9\n      2021-05-01 12:00:00\n      1\n      0.880333\n      0.226171\n      -0.974928\n      -0.222521\n    \n    \n      10\n      2021-05-01 16:00:00\n      1\n      0.539457\n      0.366246\n      -0.974928\n      -0.222521\n    \n    \n      11\n      2021-05-01 20:00:00\n      1\n      0.665987\n      0.162621\n      -0.974928\n      -0.222521\n    \n  \n\n\n\n\n\nsource\n\n\nget_gaps\n\n get_gaps (o:torch.Tensor, forward:bool=True, backward:bool=True,\n           nearest:bool=True, normalize:bool=True)\n\nNumber of sequence steps from previous, to next and/or to nearest real value along the last dimension of 3D arrays or tensors\n\nsource\n\n\nnearest_gaps\n\n nearest_gaps (o, normalize=True)\n\nNumber of sequence steps to nearest real value along the last dimension of 3D arrays or tensors\n\nsource\n\n\nbackward_gaps\n\n backward_gaps (o, normalize=True)\n\nNumber of sequence steps to next real value along the last dimension of 3D arrays or tensors\n\nsource\n\n\nforward_gaps\n\n forward_gaps (o, normalize=True)\n\nNumber of sequence steps since previous real value along the last dimension of 3D arrays or tensors\n\nt = torch.rand(1, 2, 8)\narr = t.numpy()\nt[t <.6] = np.nan\ntest_ge(nearest_gaps(t).min().item(), 0)\ntest_ge(nearest_gaps(arr).min(), 0)\ntest_le(nearest_gaps(t).min().item(), 1)\ntest_le(nearest_gaps(arr).min(), 1)\ntest_eq(torch.isnan(forward_gaps(t)).sum(), 0)\ntest_eq(np.isnan(forward_gaps(arr)).sum(), 0)\nag = get_gaps(t)\ntest_eq(ag.shape, (1,6,8))\ntest_eq(torch.isnan(ag).sum(), 0)\n\n\nsource\n\n\nadd_delta_timestamp_cols\n\n add_delta_timestamp_cols (df, cols=None, groupby=None, forward=True,\n                           backward=True, nearest=True, normalize=True)\n\n\n# Add delta timestamp features for the no groups setting\ndates = pd.date_range('2021-05-01', '2021-05-07').values\ndata = np.zeros((len(dates), 2))\ndata[:, 0] = dates\ndata[:, 1] = np.random.rand(len(dates))\n\ncols = ['date', 'feature1']\ndate_df = pd.DataFrame(data, columns=cols).astype({'date': 'datetime64[ns]', 'feature1': float})\ndate_df.loc[[1,3,4],'feature1'] = np.nan\ndate_df\n\n\n\n\n\n  \n    \n      \n      date\n      feature1\n    \n  \n  \n    \n      0\n      2021-05-01\n      0.362871\n    \n    \n      1\n      2021-05-02\n      NaN\n    \n    \n      2\n      2021-05-03\n      0.948324\n    \n    \n      3\n      2021-05-04\n      NaN\n    \n    \n      4\n      2021-05-05\n      NaN\n    \n    \n      5\n      2021-05-06\n      0.020711\n    \n    \n      6\n      2021-05-07\n      0.599574\n    \n  \n\n\n\n\n\n# No groups\nexpected_output_df = date_df.copy()\nexpected_output_df['feature1_dt_fwd'] = np.array([1,1,2,1,2,3,1])\nexpected_output_df['feature1_dt_bwd'] = np.array([2,1,3,2,1,1,1])\nexpected_output_df['feature1_dt_nearest'] = np.array([1,1,2,1,1,1,1])\n\ndisplay(expected_output_df)\noutput_df = add_delta_timestamp_cols(date_df, cols='feature1', normalize=False)\ntest_eq(expected_output_df, output_df)\n\n\n\n\n\n  \n    \n      \n      date\n      feature1\n      feature1_dt_fwd\n      feature1_dt_bwd\n      feature1_dt_nearest\n    \n  \n  \n    \n      0\n      2021-05-01\n      0.362871\n      1\n      2\n      1\n    \n    \n      1\n      2021-05-02\n      NaN\n      1\n      1\n      1\n    \n    \n      2\n      2021-05-03\n      0.948324\n      2\n      3\n      2\n    \n    \n      3\n      2021-05-04\n      NaN\n      1\n      2\n      1\n    \n    \n      4\n      2021-05-05\n      NaN\n      2\n      1\n      1\n    \n    \n      5\n      2021-05-06\n      0.020711\n      3\n      1\n      1\n    \n    \n      6\n      2021-05-07\n      0.599574\n      1\n      1\n      1\n    \n  \n\n\n\n\n\n# Add delta timestamp features within a group\ndates = pd.date_range('2021-05-01', '2021-05-07').values\ndates = np.concatenate((dates, dates))\ndata = np.zeros((len(dates), 3))\ndata[:, 0] = dates\ndata[:, 1] = np.array([0]*(len(dates)//2)+[1]*(len(dates)//2))\ndata[:, 2] = np.random.rand(len(dates))\n\ncols = ['date', 'id', 'feature1']\ndate_df = pd.DataFrame(data, columns=cols).astype({'date': 'datetime64[ns]', 'id': int, 'feature1': float})\ndate_df.loc[[1,3,4,8,9,11],'feature1'] = np.nan\ndate_df\n\n\n\n\n\n  \n    \n      \n      date\n      id\n      feature1\n    \n  \n  \n    \n      0\n      2021-05-01\n      0\n      0.564673\n    \n    \n      1\n      2021-05-02\n      0\n      NaN\n    \n    \n      2\n      2021-05-03\n      0\n      0.127592\n    \n    \n      3\n      2021-05-04\n      0\n      NaN\n    \n    \n      4\n      2021-05-05\n      0\n      NaN\n    \n    \n      5\n      2021-05-06\n      0\n      0.702692\n    \n    \n      6\n      2021-05-07\n      0\n      0.020405\n    \n    \n      7\n      2021-05-01\n      1\n      0.568840\n    \n    \n      8\n      2021-05-02\n      1\n      NaN\n    \n    \n      9\n      2021-05-03\n      1\n      NaN\n    \n    \n      10\n      2021-05-04\n      1\n      0.334893\n    \n    \n      11\n      2021-05-05\n      1\n      NaN\n    \n    \n      12\n      2021-05-06\n      1\n      0.949283\n    \n    \n      13\n      2021-05-07\n      1\n      0.574781\n    \n  \n\n\n\n\n\n# groupby='id'\nexpected_output_df = date_df.copy()\nexpected_output_df['feature1_dt_fwd'] = np.array([1,1,2,1,2,3,1,1,1,2,3,1,2,1])\nexpected_output_df['feature1_dt_bwd'] = np.array([2,1,3,2,1,1,1,3,2,1,2,1,1,1])\nexpected_output_df['feature1_dt_nearest'] = np.array([1,1,2,1,1,1,1,1,1,1,2,1,1,1])\n\ndisplay(expected_output_df)\noutput_df = add_delta_timestamp_cols(date_df, cols='feature1', groupby='id', normalize=False)\ntest_eq(expected_output_df, output_df)\n\n\n\n\n\n  \n    \n      \n      date\n      id\n      feature1\n      feature1_dt_fwd\n      feature1_dt_bwd\n      feature1_dt_nearest\n    \n  \n  \n    \n      0\n      2021-05-01\n      0\n      0.564673\n      1\n      2\n      1\n    \n    \n      1\n      2021-05-02\n      0\n      NaN\n      1\n      1\n      1\n    \n    \n      2\n      2021-05-03\n      0\n      0.127592\n      2\n      3\n      2\n    \n    \n      3\n      2021-05-04\n      0\n      NaN\n      1\n      2\n      1\n    \n    \n      4\n      2021-05-05\n      0\n      NaN\n      2\n      1\n      1\n    \n    \n      5\n      2021-05-06\n      0\n      0.702692\n      3\n      1\n      1\n    \n    \n      6\n      2021-05-07\n      0\n      0.020405\n      1\n      1\n      1\n    \n    \n      7\n      2021-05-01\n      1\n      0.568840\n      1\n      3\n      1\n    \n    \n      8\n      2021-05-02\n      1\n      NaN\n      1\n      2\n      1\n    \n    \n      9\n      2021-05-03\n      1\n      NaN\n      2\n      1\n      1\n    \n    \n      10\n      2021-05-04\n      1\n      0.334893\n      3\n      2\n      2\n    \n    \n      11\n      2021-05-05\n      1\n      NaN\n      1\n      1\n      1\n    \n    \n      12\n      2021-05-06\n      1\n      0.949283\n      2\n      1\n      1\n    \n    \n      13\n      2021-05-07\n      1\n      0.574781\n      1\n      1\n      1\n    \n  \n\n\n\n\nSlidingWindow and SlidingWindowPanel are 2 useful functions that will allow you to create an array with segments of a pandas dataframe based on multiple criteria.\n\nsource\n\n\nSlidingWindow\n\n SlidingWindow (window_len:int, stride:Optional[int]=1, start:int=0,\n                pad_remainder:bool=False, padding:str='post',\n                padding_value:float=nan, add_padding_feature:bool=True,\n                get_x:Union[NoneType,int,list]=None,\n                get_y:Union[NoneType,int,list]=None,\n                y_func:Optional[<built-infunctioncallable>]=None,\n                output_processor:Optional[<built-\n                infunctioncallable>]=None, copy:bool=False,\n                horizon:Union[int,list]=1, seq_first:bool=True,\n                sort_by:Optional[list]=None, ascending:bool=True,\n                check_leakage:bool=True)\n\nApplies a sliding window to a 1d or 2d input (np.ndarray, torch.Tensor or pd.DataFrame)\nInput:\n    You can use np.ndarray, pd.DataFrame or torch.Tensor as input\n\n    shape: (seq_len, ) or (seq_len, n_vars) if seq_first=True else (n_vars, seq_len)\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nwindow_len\nint\n\nlength of lookback window\n\n\nstride\nUnion[None, int]\n1\nn datapoints the window is moved ahead along the sequence. Default: 1. If None, stride=window_len (no overlap)\n\n\nstart\nint\n0\ndetermines the step where the first window is applied: 0 (default) or a given step (int). Previous steps will be discarded.\n\n\npad_remainder\nbool\nFalse\nallows to pad remainder subsequences when the sliding window is applied and get_y == [] (unlabeled data).\n\n\npadding\nstr\npost\n‘pre’ or ‘post’ (optional, defaults to ‘pre’): pad either before or after each sequence. If pad_remainder == False, it indicates the starting point to create the sequence (‘pre’ from the end, and ‘post’ from the beginning)\n\n\npadding_value\nfloat\nnan\nvalue (float) that will be used for padding. Default: np.nan\n\n\nadd_padding_feature\nbool\nTrue\nadd an additional feature indicating whether each timestep is padded (1) or not (0).\n\n\nget_x\nUnion[None, int, list]\nNone\nindices of columns that contain the independent variable (xs). If None, all data will be used as x.\n\n\nget_y\nUnion[None, int, list]\nNone\nindices of columns that contain the target (ys). If None, all data will be used as y. [] means no y data is created (unlabeled data).\n\n\ny_func\nOptional[callable]\nNone\noptional function to calculate the ys based on the get_y col/s and each y sub-window. y_func must be a function applied to axis=1!\n\n\noutput_processor\nOptional[callable]\nNone\noptional function to process the final output (X (and y if available)). This is useful when some values need to be removed.The function should take X and y (even if it’s None) as arguments.\n\n\ncopy\nbool\nFalse\ncopy the original object to avoid changes in it.\n\n\nhorizon\nUnion[int, list]\n1\nnumber of future datapoints to predict (y). If get_y is [] horizon will be set to 0.\n\n\nseq_first\nbool\nTrue\nTrue if input shape (seq_len, n_vars), False if input shape (n_vars, seq_len)\n\n\nsort_by\nOptional[list]\nNone\ncolumn/s used for sorting the array in ascending order\n\n\nascending\nbool\nTrue\nused in sorting\n\n\ncheck_leakage\nbool\nTrue\nchecks if there’s leakage in the output between X and y\n\n\n\n\nwl = 5\nstride = 5\n\nt = np.repeat(np.arange(13).reshape(-1,1), 3, axis=-1)\nprint('input shape:', t.shape)\nX, y = SlidingWindow(wl, stride=stride, pad_remainder=True, get_y=[])(t)\nX\n\ninput shape: (13, 3)\n\n\narray([[[ 0.,  1.,  2.,  3.,  4.],\n        [ 0.,  1.,  2.,  3.,  4.],\n        [ 0.,  1.,  2.,  3.,  4.],\n        [ 0.,  0.,  0.,  0.,  0.]],\n\n       [[ 5.,  6.,  7.,  8.,  9.],\n        [ 5.,  6.,  7.,  8.,  9.],\n        [ 5.,  6.,  7.,  8.,  9.],\n        [ 0.,  0.,  0.,  0.,  0.]],\n\n       [[10., 11., 12., nan, nan],\n        [10., 11., 12., nan, nan],\n        [10., 11., 12., nan, nan],\n        [ 0.,  0.,  0.,  1.,  1.]]])\n\n\n\nwl = 5\nt = np.arange(10)\nprint('input shape:', t.shape)\nX, y = SlidingWindow(wl)(t)\ntest_eq(X.shape[1:], (1, wl))\nitemify(X,)\n\ninput shape: (10,)\n\n\n(#5) [(array([[0, 1, 2, 3, 4]]),),(array([[1, 2, 3, 4, 5]]),),(array([[2, 3, 4, 5, 6]]),),(array([[3, 4, 5, 6, 7]]),),(array([[4, 5, 6, 7, 8]]),)]\n\n\n\nwl = 5\nh = 1\n\nt = np.arange(10)\nprint('input shape:', t.shape)\nX, y = SlidingWindow(wl, stride=1, horizon=h)(t)\nitems = itemify(X, y)\nprint(items)\ntest_eq(items[0][0].shape, (1, wl))\ntest_eq(items[0][1].shape, ())\n\ninput shape: (10,)\n[(array([[0, 1, 2, 3, 4]]), 5), (array([[1, 2, 3, 4, 5]]), 6), (array([[2, 3, 4, 5, 6]]), 7), (array([[3, 4, 5, 6, 7]]), 8), (array([[4, 5, 6, 7, 8]]), 9)]\n\n\n\nwl = 5\nh = 2 # 2 or more\n\nt = np.arange(10)\nprint('input shape:', t.shape)\nX, y = SlidingWindow(wl, horizon=h)(t)\nitems = itemify(X, y)\nprint(items)\ntest_eq(items[0][0].shape, (1, wl))\ntest_eq(items[0][1].shape, (2, ))\n\ninput shape: (10,)\n[(array([[0, 1, 2, 3, 4]]), array([5, 6])), (array([[1, 2, 3, 4, 5]]), array([6, 7])), (array([[2, 3, 4, 5, 6]]), array([7, 8])), (array([[3, 4, 5, 6, 7]]), array([8, 9]))]\n\n\n\nwl = 5\nh = 2 # 2 or more\n\nt = np.arange(10).reshape(1, -1)\nprint('input shape:', t.shape)\nX, y = SlidingWindow(wl, stride=1, horizon=h, get_y=None, seq_first=False)(t)\nitems = itemify(X, y)\nprint(items)\ntest_eq(items[0][0].shape, (1, wl))\ntest_eq(items[0][1].shape, (2, ))\n\ninput shape: (1, 10)\n[(array([[0, 1, 2, 3, 4]]), array([5, 6])), (array([[1, 2, 3, 4, 5]]), array([6, 7])), (array([[2, 3, 4, 5, 6]]), array([7, 8])), (array([[3, 4, 5, 6, 7]]), array([8, 9]))]\n\n\n\nwl = 5\nh = 2 # 2 or more\n\nt = np.arange(10).reshape(1, -1)\nprint('input shape:', t.shape)\nX, y = SlidingWindow(wl, stride=1, horizon=h, seq_first=False)(t)\nitems = itemify(X, y)\nprint(items)\ntest_eq(items[0][0].shape, (1, wl))\n\ninput shape: (1, 10)\n[(array([[0, 1, 2, 3, 4]]), array([5, 6])), (array([[1, 2, 3, 4, 5]]), array([6, 7])), (array([[2, 3, 4, 5, 6]]), array([7, 8])), (array([[3, 4, 5, 6, 7]]), array([8, 9]))]\n\n\n\nwl = 5\n\nt = np.arange(10).reshape(1, -1)\nprint('input shape:', t.shape)\nX, y = SlidingWindow(wl, stride=3, horizon=1, get_y=None, seq_first=False)(t)\nitems = itemify(X, y)\nprint(items)\ntest_eq(items[0][0].shape, (1, wl))\ntest_eq(items[0][1].shape, ())\n\ninput shape: (1, 10)\n[(array([[0, 1, 2, 3, 4]]), 5), (array([[3, 4, 5, 6, 7]]), 8)]\n\n\n\nwl = 5\nstart = 3\n\nt = np.arange(20)\nprint('input shape:', t.shape)\nX, y = SlidingWindow(wl, stride=None, horizon=1, start=start)(t)\nitems = itemify(X, y)\nprint(items)\ntest_eq(items[0][0].shape, (1, wl))\ntest_eq(items[0][1].shape, ())\n\ninput shape: (20,)\n[(array([[3, 4, 5, 6, 7]]), 8), (array([[ 8,  9, 10, 11, 12]]), 13), (array([[13, 14, 15, 16, 17]]), 18)]\n\n\n\nwl = 5\n\nt = np.arange(20)\nprint('input shape:', t.shape)\ndf = pd.DataFrame(t, columns=['var'])\ndisplay(df)\nX, y = SlidingWindow(wl, stride=None, horizon=1, get_y=None)(df)\nitems = itemify(X, y)\nprint(items)\ntest_eq(items[0][0].shape, (1, wl))\ntest_eq(items[0][1].shape, ())\n\ninput shape: (20,)\n\n\n\n\n\n\n  \n    \n      \n      var\n    \n  \n  \n    \n      0\n      0\n    \n    \n      1\n      1\n    \n    \n      2\n      2\n    \n    \n      3\n      3\n    \n    \n      4\n      4\n    \n    \n      5\n      5\n    \n    \n      6\n      6\n    \n    \n      7\n      7\n    \n    \n      8\n      8\n    \n    \n      9\n      9\n    \n    \n      10\n      10\n    \n    \n      11\n      11\n    \n    \n      12\n      12\n    \n    \n      13\n      13\n    \n    \n      14\n      14\n    \n    \n      15\n      15\n    \n    \n      16\n      16\n    \n    \n      17\n      17\n    \n    \n      18\n      18\n    \n    \n      19\n      19\n    \n  \n\n\n\n\n[(array([[0, 1, 2, 3, 4]]), 5), (array([[5, 6, 7, 8, 9]]), 10), (array([[10, 11, 12, 13, 14]]), 15)]\n\n\n\nwl = 5\n\nt = np.arange(20)\nprint('input shape:', t.shape)\ndf = pd.DataFrame(t, columns=['var'])\ndisplay(df)\nX, y = SlidingWindow(wl, stride=1, horizon=1, get_y=None)(df)\nitems = itemify(X, y)\nprint(items)\ntest_eq(items[0][0].shape, (1, wl))\ntest_eq(items[0][1].shape, ())\n\ninput shape: (20,)\n\n\n\n\n\n\n  \n    \n      \n      var\n    \n  \n  \n    \n      0\n      0\n    \n    \n      1\n      1\n    \n    \n      2\n      2\n    \n    \n      3\n      3\n    \n    \n      4\n      4\n    \n    \n      5\n      5\n    \n    \n      6\n      6\n    \n    \n      7\n      7\n    \n    \n      8\n      8\n    \n    \n      9\n      9\n    \n    \n      10\n      10\n    \n    \n      11\n      11\n    \n    \n      12\n      12\n    \n    \n      13\n      13\n    \n    \n      14\n      14\n    \n    \n      15\n      15\n    \n    \n      16\n      16\n    \n    \n      17\n      17\n    \n    \n      18\n      18\n    \n    \n      19\n      19\n    \n  \n\n\n\n\n[(array([[0, 1, 2, 3, 4]]), 5), (array([[1, 2, 3, 4, 5]]), 6), (array([[2, 3, 4, 5, 6]]), 7), (array([[3, 4, 5, 6, 7]]), 8), (array([[4, 5, 6, 7, 8]]), 9), (array([[5, 6, 7, 8, 9]]), 10), (array([[ 6,  7,  8,  9, 10]]), 11), (array([[ 7,  8,  9, 10, 11]]), 12), (array([[ 8,  9, 10, 11, 12]]), 13), (array([[ 9, 10, 11, 12, 13]]), 14), (array([[10, 11, 12, 13, 14]]), 15), (array([[11, 12, 13, 14, 15]]), 16), (array([[12, 13, 14, 15, 16]]), 17), (array([[13, 14, 15, 16, 17]]), 18), (array([[14, 15, 16, 17, 18]]), 19)]\n\n\n\nwl = 5\n\nt = np.arange(20)\nprint('input shape:', t.shape)\ndf = pd.DataFrame(t, columns=['var']).T\ndisplay(df)\nX, y = SlidingWindow(wl, stride=None, horizon=1, get_y=None, seq_first=False)(df)\nitems = itemify(X, y)\nprint(items)\ntest_eq(items[0][0].shape, (1, wl))\ntest_eq(items[0][1].shape, ())\n\ninput shape: (20,)\n\n\n\n\n\n\n  \n    \n      \n      0\n      1\n      2\n      3\n      4\n      5\n      6\n      7\n      8\n      9\n      10\n      11\n      12\n      13\n      14\n      15\n      16\n      17\n      18\n      19\n    \n  \n  \n    \n      var\n      0\n      1\n      2\n      3\n      4\n      5\n      6\n      7\n      8\n      9\n      10\n      11\n      12\n      13\n      14\n      15\n      16\n      17\n      18\n      19\n    \n  \n\n\n\n\n[(array([[0, 1, 2, 3, 4]]), 5), (array([[5, 6, 7, 8, 9]]), 10), (array([[10, 11, 12, 13, 14]]), 15)]\n\n\n\nwl = 5\nn_vars = 3\n\nt = (torch.stack(n_vars * [torch.arange(10)]).T * tensor([1, 10, 100]))\nprint('input shape:', t.shape)\ndf = pd.DataFrame(t, columns=[f'var_{i}' for i in range(n_vars)])\ndisplay(df)\nX, y = SlidingWindow(wl, horizon=1)(df)\nitems = itemify(X, y)\nprint(items)\ntest_eq(items[0][0].shape, (n_vars, wl))\n\ninput shape: torch.Size([10, 3])\n\n\n\n\n\n\n  \n    \n      \n      var_0\n      var_1\n      var_2\n    \n  \n  \n    \n      0\n      0\n      0\n      0\n    \n    \n      1\n      1\n      10\n      100\n    \n    \n      2\n      2\n      20\n      200\n    \n    \n      3\n      3\n      30\n      300\n    \n    \n      4\n      4\n      40\n      400\n    \n    \n      5\n      5\n      50\n      500\n    \n    \n      6\n      6\n      60\n      600\n    \n    \n      7\n      7\n      70\n      700\n    \n    \n      8\n      8\n      80\n      800\n    \n    \n      9\n      9\n      90\n      900\n    \n  \n\n\n\n\n[(array([[  0,   1,   2,   3,   4],\n       [  0,  10,  20,  30,  40],\n       [  0, 100, 200, 300, 400]]), array([  5,  50, 500])), (array([[  1,   2,   3,   4,   5],\n       [ 10,  20,  30,  40,  50],\n       [100, 200, 300, 400, 500]]), array([  6,  60, 600])), (array([[  2,   3,   4,   5,   6],\n       [ 20,  30,  40,  50,  60],\n       [200, 300, 400, 500, 600]]), array([  7,  70, 700])), (array([[  3,   4,   5,   6,   7],\n       [ 30,  40,  50,  60,  70],\n       [300, 400, 500, 600, 700]]), array([  8,  80, 800])), (array([[  4,   5,   6,   7,   8],\n       [ 40,  50,  60,  70,  80],\n       [400, 500, 600, 700, 800]]), array([  9,  90, 900]))]\n\n\n\nwl = 5\nn_vars = 3\n\nt = (torch.stack(n_vars * [torch.arange(10)]).T * tensor([1, 10, 100]))\nprint('input shape:', t.shape)\ndf = pd.DataFrame(t, columns=[f'var_{i}' for i in range(n_vars)])\ndisplay(df)\nX, y = SlidingWindow(wl, horizon=1, get_y=\"var_0\")(df)\nitems = itemify(X, y)\nprint(items)\ntest_eq(items[0][0].shape, (n_vars, wl))\n\ninput shape: torch.Size([10, 3])\n\n\n\n\n\n\n  \n    \n      \n      var_0\n      var_1\n      var_2\n    \n  \n  \n    \n      0\n      0\n      0\n      0\n    \n    \n      1\n      1\n      10\n      100\n    \n    \n      2\n      2\n      20\n      200\n    \n    \n      3\n      3\n      30\n      300\n    \n    \n      4\n      4\n      40\n      400\n    \n    \n      5\n      5\n      50\n      500\n    \n    \n      6\n      6\n      60\n      600\n    \n    \n      7\n      7\n      70\n      700\n    \n    \n      8\n      8\n      80\n      800\n    \n    \n      9\n      9\n      90\n      900\n    \n  \n\n\n\n\n[(array([[  0,   1,   2,   3,   4],\n       [  0,  10,  20,  30,  40],\n       [  0, 100, 200, 300, 400]]), 5), (array([[  1,   2,   3,   4,   5],\n       [ 10,  20,  30,  40,  50],\n       [100, 200, 300, 400, 500]]), 6), (array([[  2,   3,   4,   5,   6],\n       [ 20,  30,  40,  50,  60],\n       [200, 300, 400, 500, 600]]), 7), (array([[  3,   4,   5,   6,   7],\n       [ 30,  40,  50,  60,  70],\n       [300, 400, 500, 600, 700]]), 8), (array([[  4,   5,   6,   7,   8],\n       [ 40,  50,  60,  70,  80],\n       [400, 500, 600, 700, 800]]), 9)]\n\n\n\nwl = 5\nn_vars = 3\n\nt = (torch.stack(n_vars * [torch.arange(10)]).T * tensor([1, 10, 100]))\nprint('input shape:', t.shape)\ncolumns=[f'var_{i}' for i in range(n_vars-1)]+['target']\ndf = pd.DataFrame(t, columns=columns)\ndisplay(df)\nX, y = SlidingWindow(wl, horizon=1, get_x=columns[:-1], get_y='target')(df)\nitems = itemify(X, y)\nprint(items)\ntest_eq(items[0][0].shape, (n_vars-1, wl))\ntest_eq(items[0][1].shape, ())\n\ninput shape: torch.Size([10, 3])\n\n\n\n\n\n\n  \n    \n      \n      var_0\n      var_1\n      target\n    \n  \n  \n    \n      0\n      0\n      0\n      0\n    \n    \n      1\n      1\n      10\n      100\n    \n    \n      2\n      2\n      20\n      200\n    \n    \n      3\n      3\n      30\n      300\n    \n    \n      4\n      4\n      40\n      400\n    \n    \n      5\n      5\n      50\n      500\n    \n    \n      6\n      6\n      60\n      600\n    \n    \n      7\n      7\n      70\n      700\n    \n    \n      8\n      8\n      80\n      800\n    \n    \n      9\n      9\n      90\n      900\n    \n  \n\n\n\n\n[(array([[ 0,  1,  2,  3,  4],\n       [ 0, 10, 20, 30, 40]]), 500), (array([[ 1,  2,  3,  4,  5],\n       [10, 20, 30, 40, 50]]), 600), (array([[ 2,  3,  4,  5,  6],\n       [20, 30, 40, 50, 60]]), 700), (array([[ 3,  4,  5,  6,  7],\n       [30, 40, 50, 60, 70]]), 800), (array([[ 4,  5,  6,  7,  8],\n       [40, 50, 60, 70, 80]]), 900)]\n\n\n\nn_vars = 3\n\nt = (np.random.rand(1000, n_vars) - .5).cumsum(0)\nprint(t.shape)\nplt.plot(t)\nplt.show()\nX, y = SlidingWindow(5, stride=None, horizon=0, get_x=[0,1], get_y=2)(t)\ntest_eq(X[0].shape, (n_vars-1, wl))\ntest_eq(y[0].shape, ())\nprint(X.shape, y.shape)\n\n(1000, 3)\n\n\n\n\n\n(200, 2, 5) (200,)\n\n\n\nwl = 5\nn_vars = 3\n\nt = (np.random.rand(100, n_vars) - .5).cumsum(0)\nprint(t.shape)\ncolumns=[f'var_{i}' for i in range(n_vars-1)]+['target']\ndf = pd.DataFrame(t, columns=columns)\ndisplay(df)\nX, y = SlidingWindow(5, horizon=0, get_x=columns[:-1], get_y='target')(df)\ntest_eq(X[0].shape, (n_vars-1, wl))\ntest_eq(y[0].shape, ())\nprint(X.shape, y.shape)\n\n(100, 3)\n\n\n\n\n\n\n  \n    \n      \n      var_0\n      var_1\n      target\n    \n  \n  \n    \n      0\n      0.219731\n      -0.241554\n      0.156701\n    \n    \n      1\n      0.506130\n      -0.245832\n      -0.194691\n    \n    \n      2\n      0.270261\n      -0.069820\n      -0.495843\n    \n    \n      3\n      0.378622\n      -0.455200\n      -0.748986\n    \n    \n      4\n      -0.093547\n      -0.711579\n      -0.490699\n    \n    \n      ...\n      ...\n      ...\n      ...\n    \n    \n      95\n      2.437747\n      1.458150\n      -1.130901\n    \n    \n      96\n      2.361488\n      1.197764\n      -1.185382\n    \n    \n      97\n      1.905262\n      1.167854\n      -0.762819\n    \n    \n      98\n      1.653471\n      1.137911\n      -0.853101\n    \n    \n      99\n      1.491481\n      0.958854\n      -1.056193\n    \n  \n\n100 rows × 3 columns\n\n\n\n(96, 2, 5) (96,)\n\n\n\nseq_len = 100\nn_vars = 5\nt = (np.random.rand(seq_len, n_vars) - .5).cumsum(0)\nprint(t.shape)\ncolumns=[f'var_{i}' for i in range(n_vars-1)]+['target']\ndf = pd.DataFrame(t, columns=columns)\ndisplay(df)\nX, y = SlidingWindow(5, stride=1, horizon=0, get_x=columns[:-1], get_y='target', seq_first=True)(df)\ntest_eq(X[0].shape, (n_vars-1, wl))\ntest_eq(y[0].shape, ())\nprint(X.shape, y.shape)\n\n(100, 5)\n\n\n\n\n\n\n  \n    \n      \n      var_0\n      var_1\n      var_2\n      var_3\n      target\n    \n  \n  \n    \n      0\n      -0.283635\n      -0.272063\n      0.353320\n      -0.315063\n      -0.309275\n    \n    \n      1\n      -0.165789\n      -0.130168\n      0.370109\n      -0.556931\n      -0.081643\n    \n    \n      2\n      0.275993\n      -0.194410\n      0.248333\n      -0.111052\n      -0.120682\n    \n    \n      3\n      0.031166\n      -0.002200\n      0.269169\n      -0.552053\n      -0.510375\n    \n    \n      4\n      -0.064207\n      0.435825\n      0.567562\n      -0.539118\n      -0.711120\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      95\n      -5.172653\n      -2.081473\n      -1.078278\n      1.754230\n      0.210967\n    \n    \n      96\n      -4.953655\n      -2.216942\n      -0.850038\n      1.582219\n      0.296310\n    \n    \n      97\n      -4.908402\n      -2.555048\n      -1.024953\n      1.638236\n      -0.028197\n    \n    \n      98\n      -4.754680\n      -2.953755\n      -0.901179\n      1.643445\n      -0.495625\n    \n    \n      99\n      -4.332660\n      -3.217639\n      -0.714610\n      1.948983\n      -0.002574\n    \n  \n\n100 rows × 5 columns\n\n\n\n(96, 4, 5) (96,)\n\n\n\nseq_len = 100\nn_vars = 5\n\nt = (np.random.rand(seq_len, n_vars) - .5).cumsum(0)\nprint(t.shape)\ncolumns=[f'var_{i}' for i in range(n_vars-1)] + ['target']\ndf = pd.DataFrame(t, columns=columns).T\ndisplay(df)\nX, y = SlidingWindow(5, stride=1, horizon=0, get_x=columns[:-1], get_y='target', seq_first=False)(df)\ntest_eq(X[0].shape, (n_vars-1, wl))\ntest_eq(y[0].shape, ())\nprint(X.shape, y.shape)\n\n(100, 5)\n\n\n\n\n\n\n  \n    \n      \n      0\n      1\n      2\n      3\n      4\n      5\n      6\n      7\n      8\n      9\n      ...\n      90\n      91\n      92\n      93\n      94\n      95\n      96\n      97\n      98\n      99\n    \n  \n  \n    \n      var_0\n      -0.148541\n      -0.638995\n      -0.771698\n      -0.641033\n      -1.131360\n      -1.185967\n      -1.109805\n      -1.198873\n      -1.311733\n      -1.557146\n      ...\n      1.931363\n      1.574373\n      1.696122\n      2.028446\n      1.817409\n      1.660194\n      1.385937\n      0.944191\n      1.117168\n      1.186324\n    \n    \n      var_1\n      -0.275498\n      -0.349152\n      -0.732845\n      -1.137754\n      -1.298116\n      -0.887117\n      -0.840864\n      -0.352913\n      -0.305306\n      -0.212787\n      ...\n      -2.497891\n      -2.555227\n      -2.338915\n      -2.026424\n      -2.104412\n      -2.107611\n      -1.977381\n      -2.176756\n      -2.130357\n      -2.328706\n    \n    \n      var_2\n      0.322848\n      -0.017649\n      0.370618\n      0.423482\n      0.912222\n      0.794922\n      0.562113\n      0.978956\n      0.538988\n      0.845996\n      ...\n      1.194526\n      1.305628\n      1.603021\n      1.175247\n      0.773061\n      0.326706\n      0.686670\n      0.477293\n      0.332445\n      0.821803\n    \n    \n      var_3\n      0.405491\n      0.221544\n      -0.224301\n      -0.572749\n      -0.320719\n      -0.785067\n      -0.300023\n      -0.230970\n      -0.145901\n      -0.237308\n      ...\n      1.520324\n      1.255344\n      1.129787\n      0.916642\n      0.590165\n      0.685978\n      0.373407\n      0.542141\n      0.534386\n      0.647745\n    \n    \n      target\n      -0.116065\n      0.080274\n      0.346056\n      0.737782\n      0.536293\n      0.618542\n      0.615581\n      1.054234\n      1.518854\n      1.709095\n      ...\n      -0.641459\n      -0.275772\n      -0.556412\n      -0.220890\n      -0.507841\n      -0.388330\n      -0.303392\n      0.105113\n      0.260270\n      0.064639\n    \n  \n\n5 rows × 100 columns\n\n\n\n(96, 4, 5) (96,)\n\n\n\nseq_len = 100\nn_vars = 5\nt = (np.random.rand(seq_len, n_vars) - .5).cumsum(0)\nprint(t.shape)\ncolumns=[f'var_{i}' for i in range(n_vars-1)] + ['target']\ndf = pd.DataFrame(t, columns=columns).T\ndisplay(df)\nX, y = SlidingWindow(5, stride=None, horizon=0, get_x=columns[:-1], get_y='target', seq_first=False)(df)\ntest_eq(X[0].shape, (n_vars-1, wl))\ntest_eq(y[0].shape, ())\nprint(X.shape, y.shape)\n\n(100, 5)\n\n\n\n\n\n\n  \n    \n      \n      0\n      1\n      2\n      3\n      4\n      5\n      6\n      7\n      8\n      9\n      ...\n      90\n      91\n      92\n      93\n      94\n      95\n      96\n      97\n      98\n      99\n    \n  \n  \n    \n      var_0\n      0.115435\n      0.430686\n      0.614773\n      0.792695\n      1.155368\n      1.225898\n      1.452182\n      1.476069\n      1.919852\n      1.507091\n      ...\n      4.815501\n      4.645090\n      4.455031\n      4.008223\n      3.866602\n      3.724032\n      3.247002\n      3.677510\n      3.490404\n      3.721697\n    \n    \n      var_1\n      0.042986\n      0.015359\n      0.062029\n      -0.146050\n      -0.639816\n      -0.225595\n      -0.472021\n      -0.448978\n      -0.465124\n      -0.286368\n      ...\n      1.468102\n      1.784592\n      2.126125\n      2.446136\n      2.904345\n      3.225461\n      3.617937\n      3.597883\n      3.232807\n      3.000932\n    \n    \n      var_2\n      -0.065309\n      -0.080171\n      -0.058904\n      0.234621\n      0.565133\n      0.628112\n      0.951859\n      0.583779\n      0.967554\n      0.769830\n      ...\n      -0.984463\n      -0.653632\n      -0.248285\n      -0.338787\n      -0.181091\n      -0.285292\n      -0.532943\n      -0.605790\n      -0.502086\n      -0.914128\n    \n    \n      var_3\n      0.105143\n      0.494115\n      0.469126\n      0.338364\n      0.464081\n      0.253782\n      0.452462\n      0.217943\n      0.484594\n      0.130015\n      ...\n      0.894124\n      0.552519\n      0.166379\n      0.149776\n      -0.209523\n      -0.386212\n      -0.866623\n      -0.785561\n      -0.788361\n      -0.457148\n    \n    \n      target\n      0.318705\n      0.278762\n      0.644054\n      0.484539\n      0.305155\n      0.750238\n      0.713616\n      1.213125\n      1.356425\n      1.694658\n      ...\n      5.366727\n      5.251513\n      5.586758\n      5.843976\n      5.536114\n      5.640595\n      6.108106\n      6.188888\n      6.098045\n      6.158165\n    \n  \n\n5 rows × 100 columns\n\n\n\n(20, 4, 5) (20,)\n\n\n\nfrom tsai.data.validation import TrainValidTestSplitter\n\n\nseq_len = 100\nn_vars = 5\nt = (np.random.rand(seq_len, n_vars) - .5).cumsum(0)\nprint(t.shape)\ncolumns=[f'var_{i}' for i in range(n_vars-1)]+['target']\ndf = pd.DataFrame(t, columns=columns)\ndisplay(df)\nX, y = SlidingWindow(5, stride=1, horizon=0, get_x=columns[:-1], get_y='target', seq_first=True)(df)\nsplits = TrainValidTestSplitter(valid_size=.2, shuffle=False)(y)\nX.shape, y.shape, splits\n\n(100, 5)\n\n\n\n\n\n\n  \n    \n      \n      var_0\n      var_1\n      var_2\n      var_3\n      target\n    \n  \n  \n    \n      0\n      -0.306396\n      0.014741\n      -0.261777\n      -0.305477\n      -0.229333\n    \n    \n      1\n      -0.382064\n      0.381658\n      -0.508205\n      0.083926\n      -0.352223\n    \n    \n      2\n      -0.462788\n      -0.075632\n      -0.918972\n      0.111431\n      -0.255939\n    \n    \n      3\n      -0.329056\n      -0.512973\n      -1.412868\n      -0.076777\n      -0.394450\n    \n    \n      4\n      -0.463039\n      -0.590885\n      -1.047386\n      -0.406276\n      -0.113335\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      95\n      -1.681212\n      -2.162603\n      -2.962027\n      -1.873796\n      2.443179\n    \n    \n      96\n      -1.443838\n      -2.590472\n      -3.326444\n      -2.338978\n      2.447001\n    \n    \n      97\n      -1.941318\n      -2.978520\n      -2.928146\n      -1.891379\n      2.435833\n    \n    \n      98\n      -1.512874\n      -3.369047\n      -3.267633\n      -2.262135\n      2.695891\n    \n    \n      99\n      -1.016829\n      -3.056351\n      -3.155080\n      -2.654061\n      3.189971\n    \n  \n\n100 rows × 5 columns\n\n\n\n((96, 4, 5),\n (96,),\n ((#77) [0,1,2,3,4,5,6,7,8,9...], (#19) [77,78,79,80,81,82,83,84,85,86...]))\n\n\n\ndata = np.concatenate([np.linspace(0, 1, 11).reshape(-1,1).repeat(2, 1), np.arange(11).reshape(-1,1)], -1)\ndf_test = pd.DataFrame(data, columns=['col1', 'col2', 'target'])\ndf_test['target'] = df_test['target'].astype(int)\ndf_test\n\n\n\n\n\n  \n    \n      \n      col1\n      col2\n      target\n    \n  \n  \n    \n      0\n      0.0\n      0.0\n      0\n    \n    \n      1\n      0.1\n      0.1\n      1\n    \n    \n      2\n      0.2\n      0.2\n      2\n    \n    \n      3\n      0.3\n      0.3\n      3\n    \n    \n      4\n      0.4\n      0.4\n      4\n    \n    \n      5\n      0.5\n      0.5\n      5\n    \n    \n      6\n      0.6\n      0.6\n      6\n    \n    \n      7\n      0.7\n      0.7\n      7\n    \n    \n      8\n      0.8\n      0.8\n      8\n    \n    \n      9\n      0.9\n      0.9\n      9\n    \n    \n      10\n      1.0\n      1.0\n      10\n    \n  \n\n\n\n\n\ndef _y_func(o): return o[:, 0]\n\n\nfor wl in np.arange(1, 20):\n    x, y = SlidingWindow(wl, None, pad_remainder=True, get_x=['col1', 'col2'], get_y=['target'], horizon=-wl, y_func=_y_func)(df_test)\n    test_eq(x.shape[0], math.ceil((len(df_test))/wl))\n    test_eq(x.shape[0], y.shape[0])\n    test_eq(x.shape[2], wl)\n    test_close(x[:, 0, 0]*10, y)\n\n\nfor wl in np.arange(1, 20):\n    x, y = SlidingWindow(wl, None, pad_remainder=True, get_x=['col1', 'col2'], get_y=['target'], horizon=-wl, y_func=None)(df_test)\n    test_eq(x.shape[0], math.ceil((len(df_test))/ wl))\n    test_eq(x.shape[0], y.shape[0])\n    test_eq(x.shape[2], wl)\n\n\nfor wl in np.arange(1, len(df_test)+1):\n    x, y = SlidingWindow(wl, None, pad_remainder=False, get_x=['col1', 'col2'], get_y=['target'], horizon=-wl, y_func=None)(df_test)\n    test_eq(x.shape[0], len(df_test) // wl)\n    test_eq(x.shape[0], y.shape[0])\n    test_eq(x.shape[2], wl)\n\n\nfor wl in np.arange(1, 20):\n    x, _ = SlidingWindow(wl, None, pad_remainder=True, get_x=['col1', 'col2'], get_y=[], horizon=0)(df_test)\n    test_eq(x.shape[0], math.ceil((len(df_test))/wl))\n    test_eq(x.shape[2], wl)\n\n\nfor wl in np.arange(2, len(df_test)):\n    x, _ = SlidingWindow(wl, wl, pad_remainder=False, get_x=['col1', 'col2'], get_y=[], horizon=0)(df_test)\n    test_eq(x.shape[0], len(df_test) // wl)\n    test_eq(x.shape[2], wl)\n\n\ndf = pd.DataFrame()\ndf['sample_id'] = np.concatenate([np.ones(n)*(i + 1) for i,n in enumerate([13])])\ndf['var1'] = df['sample_id'] + df.index.values - 1\ndf['var2'] = df['var1'] * 10\ndf['target'] = (df['var1']).astype(int)\ndf['sample_id'] = df['sample_id'].astype(int)\ndf\n\n\n\n\n\n  \n    \n      \n      sample_id\n      var1\n      var2\n      target\n    \n  \n  \n    \n      0\n      1\n      0.0\n      0.0\n      0\n    \n    \n      1\n      1\n      1.0\n      10.0\n      1\n    \n    \n      2\n      1\n      2.0\n      20.0\n      2\n    \n    \n      3\n      1\n      3.0\n      30.0\n      3\n    \n    \n      4\n      1\n      4.0\n      40.0\n      4\n    \n    \n      5\n      1\n      5.0\n      50.0\n      5\n    \n    \n      6\n      1\n      6.0\n      60.0\n      6\n    \n    \n      7\n      1\n      7.0\n      70.0\n      7\n    \n    \n      8\n      1\n      8.0\n      80.0\n      8\n    \n    \n      9\n      1\n      9.0\n      90.0\n      9\n    \n    \n      10\n      1\n      10.0\n      100.0\n      10\n    \n    \n      11\n      1\n      11.0\n      110.0\n      11\n    \n    \n      12\n      1\n      12.0\n      120.0\n      12\n    \n  \n\n\n\n\n\nX, y = SlidingWindow(window_len=3, stride=2, start=3, pad_remainder=False, padding=\"pre\", padding_value=np.nan, add_padding_feature=False,\n                     get_x=[\"var1\", \"var2\"], get_y=[\"target\"], y_func=None, output_processor=None, copy=False, horizon=4, seq_first=True, sort_by=None,\n                     ascending=True, check_leakage=True)(df)\ntest_eq(X.shape, (2, 2, 3))\ntest_eq(y.shape, (2, 4))\nX, y\n\n(array([[[ 4.,  5.,  6.],\n         [40., 50., 60.]],\n \n        [[ 6.,  7.,  8.],\n         [60., 70., 80.]]]),\n array([[ 7,  8,  9, 10],\n        [ 9, 10, 11, 12]]))\n\n\n\nX, y = SlidingWindow(window_len=3, stride=2, start=3, pad_remainder=True, padding=\"pre\", padding_value=np.nan, add_padding_feature=False,\n                     get_x=[\"var1\", \"var2\"], get_y=[\"target\"], y_func=None, output_processor=None, copy=False, horizon=4, seq_first=True, sort_by=None,\n                     ascending=True, check_leakage=True)(df)\ntest_eq(X.shape, (3, 2, 3))\ntest_eq(y.shape, (3, 4))\nX, y\n\n(array([[[nan,  3.,  4.],\n         [nan, 30., 40.]],\n \n        [[ 4.,  5.,  6.],\n         [40., 50., 60.]],\n \n        [[ 6.,  7.,  8.],\n         [60., 70., 80.]]]),\n array([[ 5,  6,  7,  8],\n        [ 7,  8,  9, 10],\n        [ 9, 10, 11, 12]]))\n\n\n\nX, y = SlidingWindow(window_len=3, stride=2, start=3, pad_remainder=False, padding=\"post\", padding_value=np.nan, add_padding_feature=False,\n                     get_x=[\"var1\", \"var2\"], get_y=[\"target\"], y_func=None, output_processor=None, copy=False, horizon=4, seq_first=True, sort_by=None,\n                     ascending=True, check_leakage=True)(df)\ntest_eq(X.shape, (2, 2, 3))\ntest_eq(y.shape, (2, 4))\nX, y\n\n(array([[[ 3.,  4.,  5.],\n         [30., 40., 50.]],\n \n        [[ 5.,  6.,  7.],\n         [50., 60., 70.]]]),\n array([[ 6,  7,  8,  9],\n        [ 8,  9, 10, 11]]))\n\n\n\nX, y = SlidingWindow(window_len=3, stride=2, start=3, pad_remainder=True, padding=\"post\", padding_value=np.nan, add_padding_feature=False,\n                     get_x=[\"var1\", \"var2\"], get_y=[\"target\"], y_func=None, output_processor=None, copy=False, horizon=4, seq_first=True, sort_by=None,\n                     ascending=True, check_leakage=True)(df)\ntest_eq(X.shape, (3, 2, 3))\ntest_eq(y.shape, (3, 4))\nX, y\n\n(array([[[ 3.,  4.,  5.],\n         [30., 40., 50.]],\n \n        [[ 5.,  6.,  7.],\n         [50., 60., 70.]],\n \n        [[ 7.,  8.,  9.],\n         [70., 80., 90.]]]),\n array([[ 6.,  7.,  8.,  9.],\n        [ 8.,  9., 10., 11.],\n        [10., 11., 12., nan]]))\n\n\n\nX, y = SlidingWindow(window_len=10, stride=2, start=3, pad_remainder=True, padding=\"pre\", padding_value=np.nan, add_padding_feature=False,\n                     get_x=[\"var1\", \"var2\"], get_y=[\"target\"], y_func=None, output_processor=None, copy=False, horizon=4, seq_first=True, sort_by=None,\n                     ascending=True, check_leakage=True)(df)\ntest_eq(X.shape, (1, 2, 10))\ntest_eq(y.shape, (1, 4))\nX, y\n\n(array([[[nan, nan, nan, nan,  3.,  4.,  5.,  6.,  7.,  8.],\n         [nan, nan, nan, nan, 30., 40., 50., 60., 70., 80.]]]),\n array([[ 9, 10, 11, 12]]))\n\n\n\nX, y = SlidingWindow(window_len=10, stride=2, start=3, pad_remainder=True, padding=\"post\", padding_value=np.nan, add_padding_feature=False,\n                     get_x=[\"var1\", \"var2\"], get_y=[\"target\"], y_func=None, output_processor=None, copy=False, horizon=4, seq_first=True, sort_by=None,\n                     ascending=True, check_leakage=True)(df)\ntest_eq(X.shape, (1, 2, 10))\ntest_eq(y.shape, (1, 4))\nX, y\n\n(array([[[  3.,   4.,   5.,   6.,   7.,   8.,   9.,  10.,  11.,  12.],\n         [ 30.,  40.,  50.,  60.,  70.,  80.,  90., 100., 110., 120.]]]),\n array([[nan, nan, nan, nan]]))\n\n\n\nsource\n\n\nSlidingWindowPanel\n\n SlidingWindowPanel (window_len:int, unique_id_cols:list,\n                     stride:Optional[int]=1, start:int=0,\n                     pad_remainder:bool=False, padding:str='post',\n                     padding_value:float=nan,\n                     add_padding_feature:bool=True,\n                     get_x:Union[NoneType,int,list]=None,\n                     get_y:Union[NoneType,int,list]=None,\n                     y_func:Optional[<built-infunctioncallable>]=None,\n                     output_processor:Optional[<built-\n                     infunctioncallable>]=None, copy:bool=False,\n                     horizon:Union[int,list]=1, seq_first:bool=True,\n                     sort_by:Optional[list]=None, ascending:bool=True,\n                     check_leakage:bool=True, return_key:bool=False,\n                     verbose:bool=True)\n\nApplies a sliding window to a pd.DataFrame.\nArgs: window_len = length of lookback window unique_id_cols = pd.DataFrame columns that will be used to identify a time series for each entity. stride = n datapoints the window is moved ahead along the sequence. Default: 1. If None, stride=window_len (no overlap) start = determines the step where the first window is applied: 0 (default), a given step (int), or random within the 1st stride (None). pad_remainder = allows to pad remainder subsequences when the sliding window is applied and get_y == [] (unlabeled data). padding = ‘pre’ or ‘post’ (optional, defaults to ‘pre’): pad either before or after each sequence. If pad_remainder == False, it indicates the starting point to create the sequence (‘pre’ from the end, and ‘post’ from the beginning) padding_value = value (float) that will be used for padding. Default: np.nan add_padding_feature = add an additional feature indicating whether each timestep is padded (1) or not (0). horizon = number of future datapoints to predict (y). If get_y is [] horizon will be set to 0. * 0 for last step in each sub-window. * n > 0 for a range of n future steps (1 to n). * n < 0 for a range of n past steps (-n + 1 to 0). * list : for those exact timesteps. get_x = indices of columns that contain the independent variable (xs). If None, all data will be used as x. get_y = indices of columns that contain the target (ys). If None, all data will be used as y. [] means no y data is created (unlabeled data). y_func = function to calculate the ys based on the get_y col/s and each y sub-window. y_func must be a function applied to axis=1! output_processor = optional function to filter output (X (and y if available)). This is useful when some values need to be removed. The function should take X and y (even if it’s None) as arguments. copy = copy the original object to avoid changes in it. seq_first = True if input shape (seq_len, n_vars), False if input shape (n_vars, seq_len) sort_by = column/s used for sorting the array in ascending order ascending = used in sorting check_leakage = checks if there’s leakage in the output between X and y return_key = when True, the key corresponsing to unique_id_cols for each sample is returned verbose = controls verbosity. True or 1 displays progress bar. 2 or more show records that cannot be created due to its length.\nInput: You can use np.ndarray, pd.DataFrame or torch.Tensor as input shape: (seq_len, ) or (seq_len, n_vars) if seq_first=True else (n_vars, seq_len)\n\nsamples = 100_000\nwl = 5\nn_vars = 10\n\nt = (torch.stack(n_vars * [torch.arange(samples)]).T * tensor([10**i for i in range(n_vars)]))\ndf = pd.DataFrame(t, columns=[f'var_{i}' for i in range(n_vars)])\ndf['time'] = np.arange(len(t))\ndf['device'] = 0\ndf['target'] = np.random.randint(0, 2, len(df))\ndf2 = df.copy()\ndf3 = df.copy()\ncols = ['var_0', 'var_1', 'var_2', 'device', 'target']\ndf2[cols] = df2[cols] + 1\ndf3[cols] = df3[cols] + 2\ndf2 = df2.loc[:3]\ndf['region'] = 'A'\ndf2['region'] = 'A'\ndf3['region'] = 'B'\ndf = df.append(df2).append(df3).reset_index(drop=True)\ndf['index'] = np.arange(len(df))\ndf = df.sample(frac=1).reset_index(drop=True)\ndisplay(df.head())\ndf.shape\n\n\n\n\n\n  \n    \n      \n      var_0\n      var_1\n      var_2\n      var_3\n      var_4\n      var_5\n      var_6\n      var_7\n      var_8\n      var_9\n      time\n      device\n      target\n      region\n      index\n    \n  \n  \n    \n      0\n      82686\n      826860\n      8268600\n      82686000\n      826860000\n      8268600000\n      82686000000\n      826860000000\n      8268600000000\n      82686000000000\n      82686\n      0\n      0\n      A\n      82686\n    \n    \n      1\n      63255\n      632532\n      6325302\n      63253000\n      632530000\n      6325300000\n      63253000000\n      632530000000\n      6325300000000\n      63253000000000\n      63253\n      2\n      3\n      B\n      163257\n    \n    \n      2\n      12357\n      123552\n      1235502\n      12355000\n      123550000\n      1235500000\n      12355000000\n      123550000000\n      1235500000000\n      12355000000000\n      12355\n      2\n      2\n      B\n      112359\n    \n    \n      3\n      87463\n      874630\n      8746300\n      87463000\n      874630000\n      8746300000\n      87463000000\n      874630000000\n      8746300000000\n      87463000000000\n      87463\n      0\n      1\n      A\n      87463\n    \n    \n      4\n      89683\n      896830\n      8968300\n      89683000\n      896830000\n      8968300000\n      89683000000\n      896830000000\n      8968300000000\n      89683000000000\n      89683\n      0\n      0\n      A\n      89683\n    \n  \n\n\n\n\n(200004, 15)\n\n\n\nX, y = SlidingWindowPanel(window_len=5, unique_id_cols=['device'], stride=1, start=0, get_x=df.columns[:n_vars], get_y=['target'], \n                          horizon=0, seq_first=True, sort_by=['time'], ascending=True, return_key=False)(df)\nX.shape, y.shape\n\n...data processed\nconcatenating X...\n...X concatenated\nconcatenating y...\n...y concatenated\n\n\n((199992, 10, 5), (199992,))\n\n\n\nX, y, key = SlidingWindowPanel(window_len=5, unique_id_cols=['device'], stride=1, start=0, get_x=df.columns[:n_vars], get_y=['target'], \n                               horizon=0, seq_first=True, sort_by=['time'], ascending=True, return_key=True)(df)\nX.shape, y.shape, key.shape\n\n...data processed\nconcatenating X...\n...X concatenated\nconcatenating y...\n...y concatenated\n\n\n((199992, 10, 5), (199992,), (199992,))\n\n\n\nX, y = SlidingWindowPanel(window_len=5, unique_id_cols=['device', 'region'], stride=1, start=0, get_x=df.columns[:n_vars], get_y=['target'], \n                          horizon=0, seq_first=True, sort_by=['time'], ascending=True)(df)\nX.shape, y.shape\n\n...data processed\nconcatenating X...\n...X concatenated\nconcatenating y...\n...y concatenated\n\n\n((199992, 10, 5), (199992,))\n\n\n\n# y_func must be a function applied to axis=1!\ndef y_max(o): return np.max(o, axis=1)\n\n\nX, y = SlidingWindowPanel(window_len=5, unique_id_cols=['device', 'region'], stride=1, start=0, get_x=df.columns[:n_vars], get_y=['target'], \n                          y_func=y_max, horizon=5, seq_first=True, sort_by=['time'], ascending=True)(df)\nX.shape, y.shape\n\nprocessing data...\n\n\n\n\n\n\n...data processed\nconcatenating X...\n...X concatenated\nconcatenating y...\n...y concatenated\n\n\n((199982, 10, 5), (199982,))\n\n\n\nsource\n\n\nidentify_padding\n\n identify_padding (float_mask, value=-1)\n\nIdentifies padded subsequences in a mask of type float\nThis function identifies as padded subsequences those where all values == nan from the end of the sequence (last dimension) across all channels, and sets those values to the selected value (default = -1)\nArgs: mask: boolean or float mask value: scalar that will be used to identify padded subsequences\n\nwl = 5\nstride = 5\n\nt = np.repeat(np.arange(13).reshape(-1,1), 3, axis=-1)\nprint('input shape:', t.shape)\nX, _ = SlidingWindow(wl, stride=stride, pad_remainder=True, get_y=[])(t)\nX = tensor(X)\nX[0, 1, -2:] = np.nan\nX[1,..., :3] = np.nan\nprint(X)\nidentify_padding(torch.isnan(X).float())\n\ninput shape: (13, 3)\ntensor([[[ 0.,  1.,  2.,  3.,  4.],\n         [ 0.,  1.,  2., nan, nan],\n         [ 0.,  1.,  2.,  3.,  4.],\n         [ 0.,  0.,  0.,  0.,  0.]],\n\n        [[nan, nan, nan,  8.,  9.],\n         [nan, nan, nan,  8.,  9.],\n         [nan, nan, nan,  8.,  9.],\n         [nan, nan, nan,  0.,  0.]],\n\n        [[10., 11., 12., nan, nan],\n         [10., 11., 12., nan, nan],\n         [10., 11., 12., nan, nan],\n         [ 0.,  0.,  0.,  1.,  1.]]])\n\n\ntensor([[[0., 0., 0., 0., 0.],\n         [0., 0., 0., 1., 1.],\n         [0., 0., 0., 0., 0.],\n         [0., 0., 0., 0., 0.]],\n\n        [[1., 1., 1., 0., 0.],\n         [1., 1., 1., 0., 0.],\n         [1., 1., 1., 0., 0.],\n         [1., 1., 1., 0., 0.]],\n\n        [[0., 0., 0., 1., 1.],\n         [0., 0., 0., 1., 1.],\n         [0., 0., 0., 1., 1.],\n         [0., 0., 0., 0., 0.]]])\n\n\n\n\nForecasting data preparation\n\nsource\n\nbasic_data_preparation_fn\n\n basic_data_preparation_fn (df, drop_duplicates=True, datetime_col=None,\n                            use_index=False, keep='last',\n                            add_missing_datetimes=True, freq='1D',\n                            method=None, sort_by=None)\n\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ndf\n\n\ndataframe to preprocess\n\n\ndrop_duplicates\nbool\nTrue\nflag to indicate if rows with duplicate datetime info should be removed\n\n\ndatetime_col\nNoneType\nNone\nstr indicating the name of the column/s that contains the datetime info\n\n\nuse_index\nbool\nFalse\nflag to indicate if the datetime info is in the index\n\n\nkeep\nstr\nlast\nstr to indicate what data should be kept in case of duplicate rows\n\n\nadd_missing_datetimes\nbool\nTrue\nflaf to indicate if missing datetimes should be added\n\n\nfreq\nstr\n1D\nstr to indicate the frequency used in the datetime info. Used in case missing timestamps exists\n\n\nmethod\nNoneType\nNone\nstr indicating the method used to fill data for missing timestamps: None, ‘bfill’, ‘ffill’\n\n\nsort_by\nNoneType\nNone\nstr or list of str to indicate if how to sort data. If use_index=True the index will be used to sort the dataframe.\n\n\n\n\ndf_len = 100\ndatetime_col = 'datetime' \ndf = pd.DataFrame(np.arange(df_len), columns=['value'])\ndf['datetime'] = pd.date_range(pd.to_datetime('1749-03-31'), periods=df_len, freq='1D')\ndf['type'] = 1\n# drop 10 rows at random\ndf = df.drop(df.sample(10).index)\n# add 2 duplicated rows\ndf = df.append(df.sample(2))\ndisplay(df)\n\nnew_df = basic_data_preparation_fn(df, drop_duplicates=True, datetime_col=datetime_col, use_index=False, keep='last', \n                                   add_missing_datetimes=True, freq='1D', method='ffill', sort_by=datetime_col)\ndisplay(new_df)\n\n\n\n\n\n  \n    \n      \n      value\n      datetime\n      type\n    \n  \n  \n    \n      0\n      0\n      1749-03-31\n      1\n    \n    \n      1\n      1\n      1749-04-01\n      1\n    \n    \n      2\n      2\n      1749-04-02\n      1\n    \n    \n      3\n      3\n      1749-04-03\n      1\n    \n    \n      4\n      4\n      1749-04-04\n      1\n    \n    \n      ...\n      ...\n      ...\n      ...\n    \n    \n      97\n      97\n      1749-07-06\n      1\n    \n    \n      98\n      98\n      1749-07-07\n      1\n    \n    \n      99\n      99\n      1749-07-08\n      1\n    \n    \n      90\n      90\n      1749-06-29\n      1\n    \n    \n      24\n      24\n      1749-04-24\n      1\n    \n  \n\n92 rows × 3 columns\n\n\n\n\n\n\n\n  \n    \n      \n      value\n      datetime\n      type\n    \n  \n  \n    \n      0\n      0\n      1749-03-31\n      1\n    \n    \n      1\n      1\n      1749-04-01\n      1\n    \n    \n      2\n      2\n      1749-04-02\n      1\n    \n    \n      3\n      3\n      1749-04-03\n      1\n    \n    \n      4\n      4\n      1749-04-04\n      1\n    \n    \n      ...\n      ...\n      ...\n      ...\n    \n    \n      95\n      95\n      1749-07-04\n      1\n    \n    \n      96\n      96\n      1749-07-05\n      1\n    \n    \n      97\n      97\n      1749-07-06\n      1\n    \n    \n      98\n      98\n      1749-07-07\n      1\n    \n    \n      99\n      99\n      1749-07-08\n      1\n    \n  \n\n100 rows × 3 columns\n\n\n\n\nsource\n\n\ncheck_safe_conversion\n\n check_safe_conversion (o, dtype='float32', cols=None)\n\nChecks if the conversion to float is safe\n\nassert check_safe_conversion(-2**11, 'float16') == True\nassert check_safe_conversion(-2**11 - 1, 'float16') == False\nassert check_safe_conversion(2**24, 'float32') == True\nassert check_safe_conversion(2**24+1, 'float32') == False\nassert check_safe_conversion(2**53, 'float64') == True\nassert check_safe_conversion(2**53+1, 'float64') == False\n\ndf = pd.DataFrame({'a': [1, 2, 3], 'b': [2**24, 2**24+1, 2**24+2]})\nassert not check_safe_conversion(df, 'float32')\nassert check_safe_conversion(df, 'int32')\nassert check_safe_conversion(df, 'float32', cols='a')\nassert not check_safe_conversion(df, 'float32', cols='b')\n\n-2147483648 1 3 2147483647\n-2147483648 16777216 16777218 2147483647\n\n\n/Users/nacho/opt/anaconda3/envs/py37torch113/lib/python3.7/site-packages/ipykernel_launcher.py:39: UserWarning: Unsafe conversion to float32: {'a': True, 'b': False}\n/Users/nacho/opt/anaconda3/envs/py37torch113/lib/python3.7/site-packages/ipykernel_launcher.py:39: UserWarning: Unsafe conversion to float32: {'b': False}\n\n\n\nsource\n\n\nprepare_forecasting_data\n\nfrom tsai.data.validation import get_forecasting_splits\n\n\nfcst_history = 10 \nfcst_horizon = 5\nstride = 1\nvalid_size=0.2\ntest_size=0.2\n\ndf = pd.DataFrame()\ndf['target'] = np.arange(50)\n\nX, y = prepare_forecasting_data(df, fcst_history, fcst_horizon)\nsplits = get_forecasting_splits(df, fcst_history, fcst_horizon, valid_size=valid_size, test_size=test_size, stride=stride, show_plot=False)\nassert y[splits[0]][-1][0][-1] == y[splits[1]][0][0][0] - stride\nassert y[splits[1]][-1][0][-1] == y[splits[2]][0][0][0] - stride\nfor s,t in zip(splits, ['\\ntrain_split:', '\\nvalid_split:', '\\ntest_split :']):\n    print(t)\n    for xi, yi in zip(X[s], y[s]):\n        print(xi, yi)\n\n\ntrain_split:\n[[0 1 2 3 4 5 6 7 8 9]] [[10 11 12 13 14]]\n[[ 1  2  3  4  5  6  7  8  9 10]] [[11 12 13 14 15]]\n[[ 2  3  4  5  6  7  8  9 10 11]] [[12 13 14 15 16]]\n[[ 3  4  5  6  7  8  9 10 11 12]] [[13 14 15 16 17]]\n[[ 4  5  6  7  8  9 10 11 12 13]] [[14 15 16 17 18]]\n[[ 5  6  7  8  9 10 11 12 13 14]] [[15 16 17 18 19]]\n[[ 6  7  8  9 10 11 12 13 14 15]] [[16 17 18 19 20]]\n[[ 7  8  9 10 11 12 13 14 15 16]] [[17 18 19 20 21]]\n[[ 8  9 10 11 12 13 14 15 16 17]] [[18 19 20 21 22]]\n[[ 9 10 11 12 13 14 15 16 17 18]] [[19 20 21 22 23]]\n[[10 11 12 13 14 15 16 17 18 19]] [[20 21 22 23 24]]\n[[11 12 13 14 15 16 17 18 19 20]] [[21 22 23 24 25]]\n[[12 13 14 15 16 17 18 19 20 21]] [[22 23 24 25 26]]\n[[13 14 15 16 17 18 19 20 21 22]] [[23 24 25 26 27]]\n[[14 15 16 17 18 19 20 21 22 23]] [[24 25 26 27 28]]\n[[15 16 17 18 19 20 21 22 23 24]] [[25 26 27 28 29]]\n\nvalid_split:\n[[20 21 22 23 24 25 26 27 28 29]] [[30 31 32 33 34]]\n[[21 22 23 24 25 26 27 28 29 30]] [[31 32 33 34 35]]\n[[22 23 24 25 26 27 28 29 30 31]] [[32 33 34 35 36]]\n[[23 24 25 26 27 28 29 30 31 32]] [[33 34 35 36 37]]\n[[24 25 26 27 28 29 30 31 32 33]] [[34 35 36 37 38]]\n[[25 26 27 28 29 30 31 32 33 34]] [[35 36 37 38 39]]\n\ntest_split :\n[[30 31 32 33 34 35 36 37 38 39]] [[40 41 42 43 44]]\n[[31 32 33 34 35 36 37 38 39 40]] [[41 42 43 44 45]]\n[[32 33 34 35 36 37 38 39 40 41]] [[42 43 44 45 46]]\n[[33 34 35 36 37 38 39 40 41 42]] [[43 44 45 46 47]]\n[[34 35 36 37 38 39 40 41 42 43]] [[44 45 46 47 48]]\n[[35 36 37 38 39 40 41 42 43 44]] [[45 46 47 48 49]]\n\n\n/Users/nacho/opt/anaconda3/envs/py37torch113/lib/python3.7/site-packages/ipykernel_launcher.py:46: DeprecationWarning: elementwise comparison failed; this will raise an error in the future.\n\n\n\nfcst_history = 10 \nfcst_horizon = 5\nstride = 1\nvalid_size=0.2\ntest_size=0.2\n\ndf = pd.DataFrame()\ndf['target'] = np.arange(50)\n\nX, y = prepare_forecasting_data(df, fcst_history, fcst_horizon, x_vars=None, y_vars=[])\nsplits = get_forecasting_splits(df, fcst_history, fcst_horizon, valid_size=valid_size, test_size=test_size, stride=stride, show_plot=False)\nassert y is None\n\n\ndf_len = 100\nn_values = 3\ndatetime_col = 'datetime' \ndf = pd.DataFrame()\nfor i in range(n_values):\n    df[f\"value_{i}\"] = (np.arange(df_len) * 10**i).astype(np.float32)\ndisplay(df)\n\nfcst_history = 10\nfcst_horizon = 5\nx_vars = df.columns\ny_vars = None\ndtype = None\n\nX, y = prepare_forecasting_data(df, fcst_history=fcst_history, fcst_horizon=fcst_horizon, x_vars=x_vars, y_vars=y_vars, dtype=dtype)\nassert np.shares_memory(df, X)\nassert np.shares_memory(df, y)\ntest_eq(X.shape, (86, 3, 10))\ntest_eq(y.shape, (86, 3, 5))\ntest_eq(y[:3, :, 0],  X[:3, :, -1] + np.array([1, 10, 100]).reshape(1, 1, -1))\nprint(X[:3].astype(int))\nprint(y[:3].astype(int))\n\n\n\n\n\n  \n    \n      \n      value_0\n      value_1\n      value_2\n    \n  \n  \n    \n      0\n      0.0\n      0.0\n      0.0\n    \n    \n      1\n      1.0\n      10.0\n      100.0\n    \n    \n      2\n      2.0\n      20.0\n      200.0\n    \n    \n      3\n      3.0\n      30.0\n      300.0\n    \n    \n      4\n      4.0\n      40.0\n      400.0\n    \n    \n      ...\n      ...\n      ...\n      ...\n    \n    \n      95\n      95.0\n      950.0\n      9500.0\n    \n    \n      96\n      96.0\n      960.0\n      9600.0\n    \n    \n      97\n      97.0\n      970.0\n      9700.0\n    \n    \n      98\n      98.0\n      980.0\n      9800.0\n    \n    \n      99\n      99.0\n      990.0\n      9900.0\n    \n  \n\n100 rows × 3 columns\n\n\n\n[[[   0    1    2    3    4    5    6    7    8    9]\n  [   0   10   20   30   40   50   60   70   80   90]\n  [   0  100  200  300  400  500  600  700  800  900]]\n\n [[   1    2    3    4    5    6    7    8    9   10]\n  [  10   20   30   40   50   60   70   80   90  100]\n  [ 100  200  300  400  500  600  700  800  900 1000]]\n\n [[   2    3    4    5    6    7    8    9   10   11]\n  [  20   30   40   50   60   70   80   90  100  110]\n  [ 200  300  400  500  600  700  800  900 1000 1100]]]\n[[[  10   11   12   13   14]\n  [ 100  110  120  130  140]\n  [1000 1100 1200 1300 1400]]\n\n [[  11   12   13   14   15]\n  [ 110  120  130  140  150]\n  [1100 1200 1300 1400 1500]]\n\n [[  12   13   14   15   16]\n  [ 120  130  140  150  160]\n  [1200 1300 1400 1500 1600]]]\n\n\n/Users/nacho/opt/anaconda3/envs/py37torch113/lib/python3.7/site-packages/ipykernel_launcher.py:46: DeprecationWarning: elementwise comparison failed; this will raise an error in the future.\n\n\n\ndf_len = 100\nn_values = 3\ndatetime_col = 'datetime' \ndf = pd.DataFrame()\nfor i in range(n_values):\n    df[f\"value_{i}\"] = (np.arange(df_len) * 10**(i + 1)).astype(np.float32)\n\ndf['datetime'] = pd.date_range(pd.to_datetime('1749-03-31'), periods=df_len, freq='1D')\ndf['type'] = np.random.randint(0, 4, df_len)\ndf['target'] = np.arange(df_len)\ndisplay(df)\n\nfcst_history = 10\nfcst_horizon = 5\nx_vars = ['value_0', 'value_1', 'value_2', 'target']\ny_vars = 'target'\ndtype = np.float32\n\nX, y = prepare_forecasting_data(df, fcst_history=fcst_history, fcst_horizon=fcst_horizon, x_vars=x_vars, y_vars=y_vars, dtype=dtype)\ntest_eq(X.shape, (86, 4, 10))\ntest_eq(y.shape, (86, 1, 5))\nprint(X[:3].astype(int))\nprint(y[:3])\n\n\n\n\n\n  \n    \n      \n      value_0\n      value_1\n      value_2\n      datetime\n      type\n      target\n    \n  \n  \n    \n      0\n      0.0\n      0.0\n      0.0\n      1749-03-31\n      0\n      0\n    \n    \n      1\n      10.0\n      100.0\n      1000.0\n      1749-04-01\n      3\n      1\n    \n    \n      2\n      20.0\n      200.0\n      2000.0\n      1749-04-02\n      0\n      2\n    \n    \n      3\n      30.0\n      300.0\n      3000.0\n      1749-04-03\n      2\n      3\n    \n    \n      4\n      40.0\n      400.0\n      4000.0\n      1749-04-04\n      3\n      4\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      95\n      950.0\n      9500.0\n      95000.0\n      1749-07-04\n      3\n      95\n    \n    \n      96\n      960.0\n      9600.0\n      96000.0\n      1749-07-05\n      2\n      96\n    \n    \n      97\n      970.0\n      9700.0\n      97000.0\n      1749-07-06\n      2\n      97\n    \n    \n      98\n      980.0\n      9800.0\n      98000.0\n      1749-07-07\n      3\n      98\n    \n    \n      99\n      990.0\n      9900.0\n      99000.0\n      1749-07-08\n      2\n      99\n    \n  \n\n100 rows × 6 columns\n\n\n\n[[[    0    10    20    30    40    50    60    70    80    90]\n  [    0   100   200   300   400   500   600   700   800   900]\n  [    0  1000  2000  3000  4000  5000  6000  7000  8000  9000]\n  [    0     1     2     3     4     5     6     7     8     9]]\n\n [[   10    20    30    40    50    60    70    80    90   100]\n  [  100   200   300   400   500   600   700   800   900  1000]\n  [ 1000  2000  3000  4000  5000  6000  7000  8000  9000 10000]\n  [    1     2     3     4     5     6     7     8     9    10]]\n\n [[   20    30    40    50    60    70    80    90   100   110]\n  [  200   300   400   500   600   700   800   900  1000  1100]\n  [ 2000  3000  4000  5000  6000  7000  8000  9000 10000 11000]\n  [    2     3     4     5     6     7     8     9    10    11]]]\n[[[10. 11. 12. 13. 14.]]\n\n [[11. 12. 13. 14. 15.]]\n\n [[12. 13. 14. 15. 16.]]]\n\n\n/Users/nacho/opt/anaconda3/envs/py37torch113/lib/python3.7/site-packages/ipykernel_launcher.py:46: DeprecationWarning: elementwise comparison failed; this will raise an error in the future.\n\n\n\nsource\n\n\nget_today\n\n get_today (datetime_format='%Y-%m-%d')\n\n\ntest_eq(get_today(), dt.datetime.today().strftime(\"%Y-%m-%d\"))\n\n\nsource\n\n\nsplit_fcst_datetime\n\n split_fcst_datetime (fcst_datetime)\n\nDefine fcst start and end dates\n\n\n\n\nDetails\n\n\n\n\nfcst_datetime\nstr or list of str with datetime\n\n\n\n\ntest_eq(split_fcst_datetime(None), (None, None))\ntest_eq(split_fcst_datetime('2020-01-01'), ('2020-01-01', '2020-01-01'))\ntest_eq(split_fcst_datetime(['2019-01-01', '2020-01-01']), ['2019-01-01', '2020-01-01'])\n\n\nsource\n\n\nset_df_datetime\n\n set_df_datetime (df, datetime_col=None, use_index=False)\n\nMake sure datetime column or index is of the right date type.\n\n# Test\ndf_len = 100\nn_values = 3\ndatetime_col = 'datetime'\ndf = pd.DataFrame()\nfor i in range(n_values):\n    df[f\"value_{i}\"] = (np.arange(df_len) * 10**(i + 1)).astype(np.float32)\ndf['datetime'] = pd.date_range(pd.to_datetime('1749-03-31'), periods=df_len, freq='1D')\nset_df_datetime(df, datetime_col=datetime_col)\ntest_eq(df['datetime'].dtypes, np.dtype('datetime64[ns]'))\ndf_index = df.set_index('datetime')\nset_df_datetime(df_index, use_index=True)\ntest_eq(df_index.index.dtype, np.dtype('datetime64[ns]'))\n\n\nsource\n\n\nget_df_datetime_bounds\n\n get_df_datetime_bounds (df, datetime_col=None, use_index=False)\n\nReturns the start date and and dates used by the forecast\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ndf\n\n\ndataframe containing forecasting data\n\n\ndatetime_col\nNoneType\nNone\nstr data column containing the datetime\n\n\nuse_index\nbool\nFalse\nbool flag to indicate if index should be used to get column\n\n\n\n\n# Test\ndf_len = 100\nn_values = 3\ndatetime_col = 'datetime'\ndf = pd.DataFrame()\nfor i in range(n_values):\n    df[f\"value_{i}\"] = (np.arange(df_len) * 10**(i + 1)).astype(np.float32)\ndf['datetime'] = pd.date_range(pd.to_datetime('1749-03-31'), periods=df_len, freq='1D')\ntest_eq(get_df_datetime_bounds(df, datetime_col=datetime_col), (df['datetime'].min(), df['datetime'].max()))\ndf_index = df.set_index('datetime')\ntest_eq(get_df_datetime_bounds(df_index, use_index=True), (df_index.index.min(), df_index.index.max()))\n\n\nsource\n\n\nget_fcst_bounds\n\n get_fcst_bounds (df, fcst_datetime, fcst_history=None, fcst_horizon=None,\n                  freq='D', datetime_format='%Y-%m-%d', datetime_col=None,\n                  use_index=False)\n\nReturns the start and end datetimes used by the forecast\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ndf\n\n\ndataframe containing forecasting data\n\n\nfcst_datetime\n\n\ndatetime for which a fcst is created. Optionally tuple of datatimes if the fcst is created for a range of dates.\n\n\nfcst_history\nNoneType\nNone\n# steps used as input\n\n\nfcst_horizon\nNoneType\nNone\n# predicted steps\n\n\nfreq\nstr\nD\ndatetime units. May contain a letters only or a combination of ints + letters: eg. “7D”\n\n\ndatetime_format\nstr\n%Y-%m-%d\nformat used to convert “today”\n\n\ndatetime_col\nNoneType\nNone\nstr data column containing the datetime\n\n\nuse_index\nbool\nFalse\nbool flag to indicate if index should be used to get column\n\n\n\n\n# Test\ndf_len = 100\nn_values = 3\ndatetime_col = 'datetime'\ndf = pd.DataFrame()\nfor i in range(n_values):\n    df[f\"value_{i}\"] = (np.arange(df_len) * 10**(i + 1)).astype(np.float32)\nfreq = \"7D\"\ndf['datetime'] = pd.date_range(None, pd.Timestamp(get_today(), freq=freq), periods=df_len, freq=freq)\ndisplay(df)\nmax_dt = pd.Timestamp(df['datetime'].max(), freq=freq)\nfcst_history = 30\nfcst_horizon = 10\nfcst_datetime = max_dt - fcst_horizon * max_dt.freq\nstart_datetime, end_datetime = get_fcst_bounds(df, fcst_datetime, datetime_col=datetime_col, fcst_history=fcst_history, fcst_horizon=fcst_horizon, freq=freq)\ndates = pd.date_range(start_datetime, end_datetime, freq=freq)\ntest_eq(len(dates), fcst_history + fcst_horizon)\ntest_eq(end_datetime, max_dt)\n\n\n\n\n\n  \n    \n      \n      value_0\n      value_1\n      value_2\n      datetime\n    \n  \n  \n    \n      0\n      0.0\n      0.0\n      0.0\n      2021-04-08\n    \n    \n      1\n      10.0\n      100.0\n      1000.0\n      2021-04-15\n    \n    \n      2\n      20.0\n      200.0\n      2000.0\n      2021-04-22\n    \n    \n      3\n      30.0\n      300.0\n      3000.0\n      2021-04-29\n    \n    \n      4\n      40.0\n      400.0\n      4000.0\n      2021-05-06\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      95\n      950.0\n      9500.0\n      95000.0\n      2023-02-02\n    \n    \n      96\n      960.0\n      9600.0\n      96000.0\n      2023-02-09\n    \n    \n      97\n      970.0\n      9700.0\n      97000.0\n      2023-02-16\n    \n    \n      98\n      980.0\n      9800.0\n      98000.0\n      2023-02-23\n    \n    \n      99\n      990.0\n      9900.0\n      99000.0\n      2023-03-02\n    \n  \n\n100 rows × 4 columns\n\n\n\n\nsource\n\n\nfilter_df_by_datetime\n\n filter_df_by_datetime (df, start_datetime=None, end_datetime=None,\n                        datetime_col=None, use_index=False)\n\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ndf\n\n\ndataframe containing forecasting data\n\n\nstart_datetime\nNoneType\nNone\nlower datetime bound\n\n\nend_datetime\nNoneType\nNone\nupper datetime bound\n\n\ndatetime_col\nNoneType\nNone\nstr data column containing the datetime\n\n\nuse_index\nbool\nFalse\nbool flag to indicate if index should be used to get column\n\n\n\n\n# Test\ndf_len = 100\nn_values = 3\ndatetime_col = 'datetime'\ndf = pd.DataFrame()\nfor i in range(n_values):\n    df[f\"value_{i}\"] = (np.arange(df_len) * 10**(i + 1)).astype(np.float32)\nfreq = \"7D\"\ndf['datetime'] = pd.date_range(None, pd.Timestamp(get_today(), freq=freq), periods=df_len, freq=freq)\ndisplay(df)\nmax_dt = pd.Timestamp(df['datetime'].max(), freq=freq)\nfcst_history = 30\nfcst_horizon = 10\nfcst_datetime = max_dt - fcst_horizon * max_dt.freq\nstart_datetime, end_datetime = get_fcst_bounds(df, fcst_datetime, datetime_col=datetime_col, fcst_history=fcst_history, fcst_horizon=fcst_horizon, freq=freq)\ntest_eq(len(filter_df_by_datetime(df, start_datetime=start_datetime, end_datetime=end_datetime, datetime_col=datetime_col)), fcst_history + fcst_horizon)\n\n\n\n\n\n  \n    \n      \n      value_0\n      value_1\n      value_2\n      datetime\n    \n  \n  \n    \n      0\n      0.0\n      0.0\n      0.0\n      2021-04-08\n    \n    \n      1\n      10.0\n      100.0\n      1000.0\n      2021-04-15\n    \n    \n      2\n      20.0\n      200.0\n      2000.0\n      2021-04-22\n    \n    \n      3\n      30.0\n      300.0\n      3000.0\n      2021-04-29\n    \n    \n      4\n      40.0\n      400.0\n      4000.0\n      2021-05-06\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      95\n      950.0\n      9500.0\n      95000.0\n      2023-02-02\n    \n    \n      96\n      960.0\n      9600.0\n      96000.0\n      2023-02-09\n    \n    \n      97\n      970.0\n      9700.0\n      97000.0\n      2023-02-16\n    \n    \n      98\n      980.0\n      9800.0\n      98000.0\n      2023-02-23\n    \n    \n      99\n      990.0\n      9900.0\n      99000.0\n      2023-03-02\n    \n  \n\n100 rows × 4 columns\n\n\n\n\nsource\n\n\nget_fcst_data_from_df\n\n get_fcst_data_from_df (df, fcst_datetime, fcst_history=None,\n                        fcst_horizon=None, freq='D',\n                        datetime_format='%Y-%m-%d', datetime_col=None,\n                        use_index=False)\n\nGet forecasting data from a dataframe\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ndf\n\n\ndataframe containing forecasting data\n\n\nfcst_datetime\n\n\ndatetime for which a fcst is created. Optionally tuple of datatimes if the fcst is created for a range of dates.\n\n\nfcst_history\nNoneType\nNone\n# steps used as input\n\n\nfcst_horizon\nNoneType\nNone\n# predicted steps\n\n\nfreq\nstr\nD\ndatetime units. May contain a letters only or a combination of ints + letters: eg. “7D”\n\n\ndatetime_format\nstr\n%Y-%m-%d\nformat used to convert “today”\n\n\ndatetime_col\nNoneType\nNone\nstr data column containing the datetime\n\n\nuse_index\nbool\nFalse\nbool flag to indicate if index should be used to get column\n\n\n\n\n# Test\ndf_len = 100\nn_values = 3\ndatetime_col = 'datetime'\ndf = pd.DataFrame()\nfor i in range(n_values):\n    df[f\"value_{i}\"] = (np.arange(df_len) * 10**(i + 1)).astype(np.float32)\nfreq = \"7D\"\ndf['datetime'] = pd.date_range(None, pd.Timestamp(get_today(), freq=freq), periods=df_len, freq=freq)\ndisplay(df)\nmax_dt = pd.Timestamp(df['datetime'].max(), freq=freq)\nfcst_history = 30\nfcst_horizon = 10\nfcst_datetime = max_dt - fcst_horizon * max_dt.freq\ntest_eq(len(get_fcst_data_from_df(df, fcst_datetime, fcst_history=fcst_history, fcst_horizon=fcst_horizon, freq=freq, datetime_col=datetime_col)), \n                                  fcst_history + fcst_horizon)\n\n\n\n\n\n  \n    \n      \n      value_0\n      value_1\n      value_2\n      datetime\n    \n  \n  \n    \n      0\n      0.0\n      0.0\n      0.0\n      2021-04-08\n    \n    \n      1\n      10.0\n      100.0\n      1000.0\n      2021-04-15\n    \n    \n      2\n      20.0\n      200.0\n      2000.0\n      2021-04-22\n    \n    \n      3\n      30.0\n      300.0\n      3000.0\n      2021-04-29\n    \n    \n      4\n      40.0\n      400.0\n      4000.0\n      2021-05-06\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      95\n      950.0\n      9500.0\n      95000.0\n      2023-02-02\n    \n    \n      96\n      960.0\n      9600.0\n      96000.0\n      2023-02-09\n    \n    \n      97\n      970.0\n      9700.0\n      97000.0\n      2023-02-16\n    \n    \n      98\n      980.0\n      9800.0\n      98000.0\n      2023-02-23\n    \n    \n      99\n      990.0\n      9900.0\n      99000.0\n      2023-03-02\n    \n  \n\n100 rows × 4 columns"
  },
  {
    "objectID": "callback.experimental.html",
    "href": "callback.experimental.html",
    "title": "Experimental Callbacks",
    "section": "",
    "text": "Miscellaneous experimental callbacks for timeseriesAI."
  },
  {
    "objectID": "callback.experimental.html#gamblers-loss-noisy-labels",
    "href": "callback.experimental.html#gamblers-loss-noisy-labels",
    "title": "Experimental Callbacks",
    "section": "Gambler’s loss: noisy labels",
    "text": "Gambler’s loss: noisy labels\n\nsource\n\ngambler_loss\n\n gambler_loss (reward=2)\n\n\nsource\n\n\nGamblersCallback\n\n GamblersCallback (after_create=None, before_fit=None, before_epoch=None,\n                   before_train=None, before_batch=None, after_pred=None,\n                   after_loss=None, before_backward=None,\n                   after_cancel_backward=None, after_backward=None,\n                   before_step=None, after_cancel_step=None,\n                   after_step=None, after_cancel_batch=None,\n                   after_batch=None, after_cancel_train=None,\n                   after_train=None, before_validate=None,\n                   after_cancel_validate=None, after_validate=None,\n                   after_cancel_epoch=None, after_epoch=None,\n                   after_cancel_fit=None, after_fit=None)\n\nA callback to use metrics with gambler’s loss\n\nfrom tsai.data.external import *\nfrom tsai.data.core import *\nfrom tsai.models.InceptionTime import *\nfrom tsai.models.layers import *\nfrom tsai.learner import *\nfrom fastai.metrics import *\nfrom tsai.metrics import *\n\n\nX, y, splits = get_UCR_data('NATOPS', return_split=False)\ntfms = [None, TSCategorize()]\ndsets = TSDatasets(X, y, tfms=tfms, splits=splits)\ndls = TSDataLoaders.from_dsets(dsets.train, dsets.valid, bs=[64, 128])\nloss_func = gambler_loss()\nlearn = ts_learner(dls, InceptionTime(dls.vars, dls.c + 1), loss_func=loss_func, cbs=GamblersCallback, metrics=[accuracy])\nlearn.fit_one_cycle(1)\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      accuracy\n      time\n    \n  \n  \n    \n      0\n      1.840055\n      1.945397\n      0.166667\n      00:05"
  },
  {
    "objectID": "callback.experimental.html#uncertainty-based-data-augmentation",
    "href": "callback.experimental.html#uncertainty-based-data-augmentation",
    "title": "Experimental Callbacks",
    "section": "Uncertainty-based data augmentation",
    "text": "Uncertainty-based data augmentation\n\nsource\n\nUBDAug\n\n UBDAug (batch_tfms:list, N:int=2, C:int=4, S:int=1)\n\nA callback to implement the uncertainty-based data augmentation.\n\nfrom tsai.models.utils import *\n\n\nX, y, splits = get_UCR_data('NATOPS', return_split=False)\ntfms = [None, TSCategorize()]\ndsets = TSDatasets(X, y, tfms=tfms, splits=splits)\ndls = TSDataLoaders.from_dsets(dsets.train, dsets.valid, batch_tfms=[TSStandardize()])\nmodel = build_ts_model(InceptionTime, dls=dls)\nTS_tfms = [TSMagScale(.75, p=.5), TSMagWarp(.1, p=0.5),  TSWindowWarp(.25, p=.5), \n           TSSmooth(p=0.5), TSRandomResizedCrop(.1, p=.5), \n           TSRandomCropPad(.3, p=0.5), \n           TSMagAddNoise(.5, p=.5)]\n\nubda_cb = UBDAug(TS_tfms, N=2, C=4, S=2)\nlearn = ts_learner(dls, model, cbs=ubda_cb, metrics=accuracy)\nlearn.fit_one_cycle(1)\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      accuracy\n      time\n    \n  \n  \n    \n      0\n      1.817080\n      1.791119\n      0.077778\n      00:14"
  },
  {
    "objectID": "index.html#description",
    "href": "index.html#description",
    "title": "tsai",
    "section": "Description",
    "text": "Description\n\nState-of-the-art Deep Learning library for Time Series and Sequences.\n\ntsai is an open-source deep learning package built on top of Pytorch & fastai focused on state-of-the-art techniques for time series tasks like classification, regression, forecasting, imputation…\ntsai is currently under active development by timeseriesAI."
  },
  {
    "objectID": "index.html#whats-new",
    "href": "index.html#whats-new",
    "title": "tsai",
    "section": "What’s new:",
    "text": "What’s new:\n\nMarch 2022\n\n⚡️ Starting with tsai 0.3.0 you’ll get faster installs and imports through a better use of dependencies.\nNew visualization methods: learn.feature_importance() and learn.step_importance() will help you gain better insights on how your models works.\nNew calibration model: learn.calibrate_model() for time series classification tasks.\n\n\n\nNovember, 2021\n\n✅ Implemented some of the learnings from reviewing Kaggle’s latest time series competition (see Medium blog post for more details) like:\n\nimproved RNN initialization (based on a kernel shared by https://www.kaggle.com/junkoda)\nadded the option to pass a feature extractor to RNNPlus & TSiT (Transformer) models.\n\ncreated a MultiConv layer that allows the concatenation of original features with the output of one or multiple convolution layers in parallel.\n\n\n\n\nSeptember, 2021\n\nSee our new tutorial notebook on how to track your experiments with Weights & Biases \ntsai just got easier to use with the new sklearn-like APIs: TSClassifier, TSRegressor, and TSForecaster!! See this for more info.\nNew tutorial notebook on how to train your model with larger-than-memory datasets in less time achieving up to 100% GPU usage!! \ntsai supports now more input formats: np.array, np.memmap, zarr, xarray, dask, list, L, …\n\n\n\nPreviously\n\nMINIROCKET a SOTA Time Series Classification model (now available in Pytorch): You can now check MiniRocket’s performance in our new tutorial notebook \n\n\n“Using this method, it is possible to train and test a classifier on all of 109 datasets from the UCR archive to state-of-the-art accuracy in less than 10 minutes.” A. Dempster et al. (Dec 2020)\n\n\nMulti-class and multi-label time series classification notebook: you can also check our new tutorial notebook: \nSelf-supervised learning: Learn how to leverage your unlabeled datasets \nNew visualization: We’ve also added a new PredictionDynamics callback that will display the predictions during training. This is the type of output you would get in a classification task for example:"
  },
  {
    "objectID": "index.html#installation",
    "href": "index.html#installation",
    "title": "tsai",
    "section": "Installation",
    "text": "Installation\n\nPip install\nYou can install the latest stable version from pip using:\npip install tsai\nIf you plan to develop tsai yourself, or want to be on the cutting edge, you can use an editable install. First install PyTorch, and then:\ngit clone https://github.com/timeseriesAI/tsai\npip install -e \"tsai[dev]\"\nNote: starting with tsai 0.3.0 tsai will only install hard dependencies. Other soft dependencies (which are only required for selected tasks) will not be installed by default (this is the recommended approach. If you require any of the dependencies that is not installed, tsai will ask you to install it when necessary). If you still want to install tsai with all its dependencies you can do it by running:\npip install tsai[extras]\n\n\nConda install\nYou can also install tsai using conda (note that if you replace conda with mamba the install process will be much faster and more reliable):\nconda install -c timeseriesai tsai"
  },
  {
    "objectID": "index.html#documentation",
    "href": "index.html#documentation",
    "title": "tsai",
    "section": "Documentation",
    "text": "Documentation\nHere’s the link to the documentation."
  },
  {
    "objectID": "index.html#available-models",
    "href": "index.html#available-models",
    "title": "tsai",
    "section": "Available models:",
    "text": "Available models:\nHere’s a list with some of the state-of-the-art models available in tsai:\n\nLSTM (Hochreiter, 1997) (paper)\nGRU (Cho, 2014) (paper)\nMLP - Multilayer Perceptron (Wang, 2016) (paper)\nFCN - Fully Convolutional Network (Wang, 2016) (paper)\nResNet - Residual Network (Wang, 2016) (paper)\nLSTM-FCN (Karim, 2017) (paper)\nGRU-FCN (Elsayed, 2018) (paper)\nmWDN - Multilevel wavelet decomposition network (Wang, 2018) (paper)\nTCN - Temporal Convolutional Network (Bai, 2018) (paper)\nMLSTM-FCN - Multivariate LSTM-FCN (Karim, 2019) (paper)\nInceptionTime (Fawaz, 2019) (paper)\nRocket (Dempster, 2019) (paper)\nXceptionTime (Rahimian, 2019) (paper)\nResCNN - 1D-ResCNN (Zou , 2019) (paper)\nTabModel - modified from fastai’s TabularModel\nOmniScale - Omni-Scale 1D-CNN (Tang, 2020) (paper)\nTST - Time Series Transformer (Zerveas, 2020) (paper)\nTabTransformer (Huang, 2020) (paper)\nMiniRocket (Dempster, 2021) (paper)\nXCM - An Explainable Convolutional Neural Network (Fauvel, 2021) (paper)\ngMLP - Gated Multilayer Perceptron (Liu, 2021) (paper)\nGatedTabTransformer (Cholakov, 2022) (paper)\n\namong others!"
  },
  {
    "objectID": "index.html#how-to-start-using-tsai",
    "href": "index.html#how-to-start-using-tsai",
    "title": "tsai",
    "section": "How to start using tsai?",
    "text": "How to start using tsai?\nTo get to know the tsai package, we’d suggest you start with this notebook in Google Colab: 01_Intro_to_Time_Series_Classification It provides an overview of a time series classification task.\nWe have also develop many other tutorial notebooks.\nTo use tsai in your own notebooks, the only thing you need to do after you have installed the package is to run this:\nfrom tsai.all import *"
  },
  {
    "objectID": "index.html#examples",
    "href": "index.html#examples",
    "title": "tsai",
    "section": "Examples",
    "text": "Examples\nThese are just a few examples of how you can use tsai:\n\nBinary, univariate classification\nTraining:\nfrom tsai.all import *\nX, y, splits = get_classification_data('ECG200', split_data=False)\nbatch_tfms = TSStandardize()\nclf = TSClassifier(X, y, splits=splits, path='models', arch=InceptionTimePlus, batch_tfms=batch_tfms, metrics=accuracy, cbs=ShowGraph())\nclf.fit_one_cycle(100, 3e-4)\nclf.export(\"clf.pkl\") \nInference:\nfrom tsai.inference import load_learner\nclf = load_learner(\"models/clf.pkl\")\nprobas, target, preds = clf.get_X_preds(X[splits[0]], y[splits[0]])\n\n\nMulti-class, multivariate classification\nTraining:\nfrom tsai.all import *\nX, y, splits = get_classification_data('LSST', split_data=False)\nbatch_tfms = TSStandardize(by_sample=True)\nmv_clf = TSClassifier(X, y, splits=splits, path='models', arch=InceptionTimePlus, batch_tfms=batch_tfms, metrics=accuracy, cbs=ShowGraph())\nmv_clf.fit_one_cycle(10, 1e-2)\nmv_clf.export(\"mv_clf.pkl\")\nInference:\nfrom tsai.inference import load_learner\nmv_clf = load_learner(\"models/mv_clf.pkl\")\nprobas, target, preds = mv_clf.get_X_preds(X[splits[0]], y[splits[0]])\n\n\nMultivariate Regression\nTraining:\nfrom tsai.all import *\nX, y, splits = get_regression_data('AppliancesEnergy', split_data=False)\nbatch_tfms = TSStandardize(by_sample=True)\nreg = TSRegressor(X, y, splits=splits, path='models', arch=TSTPlus, batch_tfms=batch_tfms, metrics=rmse, cbs=ShowGraph(), verbose=True)\nreg.fit_one_cycle(100, 3e-4)\nreg.export(\"reg.pkl\")\nInference:\nfrom tsai.inference import load_learner\nreg = load_learner(\"models/reg.pkl\")\nraw_preds, target, preds = reg.get_X_preds(X[splits[0]], y[splits[0]])\nThe ROCKETs (RocketClassifier, RocketRegressor, MiniRocketClassifier, MiniRocketRegressor, MiniRocketVotingClassifier or MiniRocketVotingRegressor) are somewhat different models. They are not actually deep learning models (although they use convolutions) and are used in a different way.\n⚠️ You’ll also need to install sktime to be able to use them. You can install it separately:\npip install sktime\nor use:\npip install tsai[extras]\nTraining:\nfrom sklearn.metrics import mean_squared_error, make_scorer\nfrom tsai.all import *\nfrom tsai.models.MINIROCKET import *\nX_train, y_train, X_test, y_test = get_regression_data('AppliancesEnergy')\nrmse_scorer = make_scorer(mean_squared_error, greater_is_better=False)\nmr_reg = MiniRocketRegressor(scoring=rmse_scorer)\nmr_reg.fit(X_train, y_train)\nmr_reg.save(\"minirocket_regressor\")\nInference:\nmr_reg = load_rocket(\"minirocket_regressor\")\ny_pred = mr_reg.predict(X_test)\nmean_squared_error(y_test, y_pred, squared=False)\n\n\nForecasting\nYou can use tsai for forecast in the following scenarios:\n\nunivariate or multivariate time series input\nunivariate or multivariate time series output\nsingle or multi-step ahead\n\nYou’ll need to: * prepare X (time series input) and the target y (see documentation) * select one of tsai’s models ending in Plus (TSTPlus, InceptionTimePlus, TSiTPlus, etc). The model will auto-configure a head to yield an output with the same shape as the target input y.\n\nSingle step\nTraining:\nfrom tsai.all import *\nts = get_forecasting_time_series(\"Sunspots\").values\nX, y = SlidingWindow(60, horizon=1)(ts)\nsplits = TimeSplitter(235)(y) \nbatch_tfms = TSStandardize()\nfcst = TSForecaster(X, y, splits=splits, path='models', batch_tfms=batch_tfms, bs=512, arch=TSTPlus, metrics=mae, cbs=ShowGraph())\nfcst.fit_one_cycle(50, 1e-3)\nfcst.export(\"fcst.pkl\")\nInference:\nfrom tsai.inference import load_learner\nfcst = load_learner(\"models/fcst.pkl\", cpu=False)\nraw_preds, target, preds = fcst.get_X_preds(X[splits[0]], y[splits[0]])\nraw_preds.shape\noutput: torch.Size([2940, 1])\n\n\nMulti-step\nThis example show how to build a 3-step ahead univariate forecast.\nTraining:\nfrom tsai.all import *\nts = get_forecasting_time_series(\"Sunspots\").values\nX, y = SlidingWindow(60, horizon=3)(ts)\nsplits = TimeSplitter(235)(y) \nbatch_tfms = TSStandardize()\nfcst = TSForecaster(X, y, splits=splits, path='models', batch_tfms=batch_tfms, bs=512, arch=TSTPlus, metrics=mae, cbs=ShowGraph())\nfcst.fit_one_cycle(50, 1e-3)\nfcst.export(\"fcst.pkl\")\nInference:\nfrom tsai.inference import load_learner\nfcst = load_learner(\"models/fcst.pkl\", cpu=False)\nraw_preds, target, preds = fcst.get_X_preds(X[splits[0]], y[splits[0]])\nraw_preds.shape\noutput: torch.Size([2938, 3])"
  },
  {
    "objectID": "index.html#input-data-format",
    "href": "index.html#input-data-format",
    "title": "tsai",
    "section": "Input data format",
    "text": "Input data format\nThe input format for all time series models and image models in tsai is the same. An np.ndarray (or array-like object like zarr, etc) with 3 dimensions:\n[# samples x # variables x sequence length]\nThe input format for tabular models in tsai (like TabModel, TabTransformer and TabFusionTransformer) is a pandas dataframe. See example."
  },
  {
    "objectID": "index.html#how-to-contribute-to-tsai",
    "href": "index.html#how-to-contribute-to-tsai",
    "title": "tsai",
    "section": "How to contribute to tsai?",
    "text": "How to contribute to tsai?\nWe welcome contributions of all kinds. Development of enhancements, bug fixes, documentation, tutorial notebooks, …\nWe have created a guide to help you start contributing to tsai. You can read it here."
  },
  {
    "objectID": "index.html#citing-tsai",
    "href": "index.html#citing-tsai",
    "title": "tsai",
    "section": "Citing tsai",
    "text": "Citing tsai\nIf you use tsai in your research please use the following BibTeX entry:\n@Misc{tsai,\n    author =       {Ignacio Oguiza},\n    title =        {tsai - A state-of-the-art deep learning library for time series and sequential data},\n    howpublished = {Github},\n    year =         {2022},\n    url =          {https://github.com/timeseriesAI/tsai}\n}"
  },
  {
    "objectID": "models.mwdn.html",
    "href": "models.mwdn.html",
    "title": "mWDN",
    "section": "",
    "text": "multilevel Wavelet Decomposition Network (mWDN)\n\nThis is an unofficial PyTorch implementation created by Ignacio Oguiza - oguiza@timeseriesAI.co\n\nsource\n\nWaveBlock\n\n WaveBlock (c_in, c_out, seq_len, wavelet=None)\n\nSame as nn.Module, but no need for subclasses to call super().__init__\n\nsource\n\n\nmWDNPlus\n\n mWDNPlus (c_in, c_out, seq_len, levels=3, wavelet=None, base_model=None,\n           base_arch=<class\n           'tsai.models.InceptionTimePlus.InceptionTimePlus'>, **kwargs)\n\nA sequential container. Modules will be added to it in the order they are passed in the constructor. Alternatively, an OrderedDict of modules can be passed in. The forward() method of [Sequential](https://timeseriesAI.github.io/models.layers.html#sequential) accepts any input and forwards it to the first module it contains. It then “chains” outputs to inputs sequentially for each subsequent module, finally returning the output of the last module.\nThe value a [Sequential](https://timeseriesAI.github.io/models.layers.html#sequential) provides over manually calling a sequence of modules is that it allows treating the whole container as a single module, such that performing a transformation on the [Sequential](https://timeseriesAI.github.io/models.layers.html#sequential) applies to each of the modules it stores (which are each a registered submodule of the [Sequential](https://timeseriesAI.github.io/models.layers.html#sequential)).\nWhat’s the difference between a [Sequential](https://timeseriesAI.github.io/models.layers.html#sequential) and a :class:torch.nn.ModuleList? A ModuleList is exactly what it sounds like–a list for storing Module s! On the other hand, the layers in a [Sequential](https://timeseriesAI.github.io/models.layers.html#sequential) are connected in a cascading way.\nExample::\n# Using Sequential to create a small model. When `model` is run,\n# input will first be passed to `Conv2d(1,20,5)`. The output of\n# `Conv2d(1,20,5)` will be used as the input to the first\n# `ReLU`; the output of the first `ReLU` will become the input\n# for `Conv2d(20,64,5)`. Finally, the output of\n# `Conv2d(20,64,5)` will be used as input to the second `ReLU`\nmodel = nn.Sequential(\n          nn.Conv2d(1,20,5),\n          nn.ReLU(),\n          nn.Conv2d(20,64,5),\n          nn.ReLU()\n        )\n\n# Using Sequential with OrderedDict. This is functionally the\n# same as the above code\nmodel = nn.Sequential(OrderedDict([\n          ('conv1', nn.Conv2d(1,20,5)),\n          ('relu1', nn.ReLU()),\n          ('conv2', nn.Conv2d(20,64,5)),\n          ('relu2', nn.ReLU())\n        ]))\n\nsource\n\n\nmWDNBlocks\n\n mWDNBlocks (c_in, c_out, seq_len, levels=3, wavelet=None)\n\nSame as nn.Module, but no need for subclasses to call super().__init__\n\nsource\n\n\nmWDN\n\n mWDN (c_in, c_out, seq_len, levels=3, wavelet=None, base_arch=<class\n       'tsai.models.InceptionTimePlus.InceptionTimePlus'>, **kwargs)\n\nSame as nn.Module, but no need for subclasses to call super().__init__\n\nfrom tsai.models.TSTPlus import TSTPlus\n\n\nbs = 16\nc_in = 3\nseq_len = 12\nc_out = 2\nxb = torch.rand(bs, c_in, seq_len).to(default_device())\ntest_eq(mWDN(c_in, c_out, seq_len).to(xb.device)(xb).shape, [bs, c_out])\nmodel = mWDNPlus(c_in, c_out, seq_len, fc_dropout=.5)\ntest_eq(model.to(xb.device)(xb).shape, [bs, c_out])\nmodel = mWDNPlus(c_in, c_out, seq_len, base_arch=TSTPlus, fc_dropout=.5)\ntest_eq(model.to(xb.device)(xb).shape, [bs, c_out])\n\n\nmodel.head, model.head_nf\n\n(Sequential(\n   (0): GELU(approximate='none')\n   (1): fastai.layers.Flatten(full=False)\n   (2): LinBnDrop(\n     (0): Dropout(p=0.5, inplace=False)\n     (1): Linear(in_features=1536, out_features=2, bias=True)\n   )\n ),\n 128)"
  },
  {
    "objectID": "models.tstplus.html",
    "href": "models.tstplus.html",
    "title": "TSTPlus",
    "section": "",
    "text": "This is an unofficial PyTorch implementation by Ignacio Oguiza of - oguiza@timeseriesAI.co based on:\nThis implementation is adapted to work with the rest of the tsai library, and contain some hyperparameters that are not available in the original implementation. I included them for experimenting."
  },
  {
    "objectID": "models.tstplus.html#imports",
    "href": "models.tstplus.html#imports",
    "title": "TSTPlus",
    "section": "Imports",
    "text": "Imports"
  },
  {
    "objectID": "models.tstplus.html#tst",
    "href": "models.tstplus.html#tst",
    "title": "TSTPlus",
    "section": "TST",
    "text": "TST\n\nt = torch.rand(16, 50, 128)\nattn_mask = torch.triu(torch.ones(50, 50)) # shape: q_len x q_len\nkey_padding_mask = torch.zeros(16, 50)\nkey_padding_mask[[1, 3, 6, 15], -10:] = 1\nkey_padding_mask = key_padding_mask.bool()\nprint('attn_mask', attn_mask.shape, 'key_padding_mask', key_padding_mask.shape)\nencoder = _TSTEncoderLayer(q_len=50, d_model=128, n_heads=8, d_k=None, d_v=None, d_ff=512, attn_dropout=0., dropout=0.1, store_attn=True, activation='gelu')\noutput = encoder(t, key_padding_mask=key_padding_mask, attn_mask=attn_mask)\noutput.shape\n\nattn_mask torch.Size([50, 50]) key_padding_mask torch.Size([16, 50])\n\n\ntorch.Size([16, 50, 128])\n\n\n\ncmap='viridis'\nfigsize=(6,5)\nplt.figure(figsize=figsize)\nplt.pcolormesh(encoder.attn[0][0].detach().cpu().numpy(), cmap=cmap)\nplt.title('Self-attention map')\nplt.colorbar()\nplt.show()\n\n\n\n\n\nsource\n\nTSTPlus\n\n TSTPlus (c_in:int, c_out:int, seq_len:int, max_seq_len:Optional[int]=512,\n          n_layers:int=3, d_model:int=128, n_heads:int=16,\n          d_k:Optional[int]=None, d_v:Optional[int]=None, d_ff:int=256,\n          norm:str='BatchNorm', attn_dropout:float=0.0, dropout:float=0.0,\n          act:str='gelu', key_padding_mask:bool='auto',\n          padding_var:Optional[int]=None,\n          attn_mask:Optional[torch.Tensor]=None, res_attention:bool=True,\n          pre_norm:bool=False, store_attn:bool=False, pe:str='zeros',\n          learn_pe:bool=True, flatten:bool=True, fc_dropout:float=0.0,\n          concat_pool:bool=False, bn:bool=False,\n          custom_head:Optional[Callable]=None,\n          y_range:Optional[tuple]=None, verbose:bool=False, **kwargs)\n\nTST (Time Series Transformer) is a Transformer that takes continuous time series as inputs\n\nfrom tsai.models.utils import build_ts_model\n\n\nbs = 8\nc_in = 9  # aka channels, features, variables, dimensions\nc_out = 2\nseq_len = 1_500\n\nxb = torch.randn(bs, c_in, seq_len).to(device)\n\n# standardize by channel by_var based on the training set\nxb = (xb - xb.mean((0, 2), keepdim=True)) / xb.std((0, 2), keepdim=True)\n\n# Settings\nmax_seq_len = 256\nd_model = 128\nn_heads = 16\nd_k = d_v = None  # if None --> d_model // n_heads\nd_ff = 256\nnorm = \"BatchNorm\"\ndropout = 0.1\nactivation = \"gelu\"\nn_layers = 3\nfc_dropout = 0.1\npe = None\nlearn_pe = True\nkwargs = {}\n\nmodel = TSTPlus(c_in, c_out, seq_len, max_seq_len=max_seq_len, d_model=d_model, n_heads=n_heads,\n                d_k=d_k, d_v=d_v, d_ff=d_ff, norm=norm, dropout=dropout, activation=activation, n_layers=n_layers,\n                fc_dropout=fc_dropout, pe=pe, learn_pe=learn_pe, **kwargs).to(device)\ntest_eq(model(xb).shape, [bs, c_out])\ntest_eq(model[0], model.backbone)\ntest_eq(model[1], model.head)\nmodel2 = build_ts_model(TSTPlus, c_in, c_out, seq_len, max_seq_len=max_seq_len, d_model=d_model, n_heads=n_heads,\n                           d_k=d_k, d_v=d_v, d_ff=d_ff, norm=norm, dropout=dropout, activation=activation, n_layers=n_layers,\n                           fc_dropout=fc_dropout, pe=pe, learn_pe=learn_pe, **kwargs).to(device)\ntest_eq(model2(xb).shape, [bs, c_out])\ntest_eq(model2[0], model2.backbone)\ntest_eq(model2[1], model2.head)\nprint(f'model parameters: {count_parameters(model)}')\n\nmodel parameters: 470018\n\n\n\nkey_padding_mask = torch.sort(torch.randint(0, 2, (bs, max_seq_len))).values.bool().to(device)\nkey_padding_mask[0]\n\ntensor([False, False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,  True,\n         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n         True,  True,  True,  True,  True,  True])\n\n\n\nmodel2.key_padding_mask = True\nmodel2.to(device)((xb, key_padding_mask)).shape\n\ntorch.Size([8, 2])\n\n\n\nmodel.head\n\nSequential(\n  (0): GELU(approximate='none')\n  (1): fastai.layers.Flatten(full=False)\n  (2): LinBnDrop(\n    (0): Dropout(p=0.1, inplace=False)\n    (1): Linear(in_features=32768, out_features=2, bias=True)\n  )\n)\n\n\n\nmodel = TSTPlus(c_in, c_out, seq_len, pre_norm=True)\ntest_eq(model.to(xb.device)(xb).shape, [bs, c_out])\n\n\nbs = 8\nc_in = 9  # aka channels, features, variables, dimensions\nc_out = 2\nseq_len = 5000\n\nxb = torch.randn(bs, c_in, seq_len)\n\n# standardize by channel by_var based on the training set\nxb = (xb - xb.mean((0, 2), keepdim=True)) / xb.std((0, 2), keepdim=True)\n\nmodel = TSTPlus(c_in, c_out, seq_len, res_attention=True)\ntest_eq(model.to(xb.device)(xb).shape, [bs, c_out])\nprint(f'model parameters: {count_parameters(model)}')\n\nmodel parameters: 605698\n\n\n\ncustom_head = partial(create_pool_head, concat_pool=True)\nmodel = TSTPlus(c_in, c_out, seq_len, max_seq_len=max_seq_len, d_model=d_model, n_heads=n_heads,\n            d_k=d_k, d_v=d_v, d_ff=d_ff, dropout=dropout, activation=activation, n_layers=n_layers,\n            fc_dropout=fc_dropout, pe=pe, learn_pe=learn_pe, flatten=False, custom_head=custom_head, **kwargs)\ntest_eq(model.to(xb.device)(xb).shape, [bs, c_out])\nprint(f'model parameters: {count_parameters(model)}')\n\nmodel parameters: 421122\n\n\n\ncustom_head = partial(create_pool_plus_head, concat_pool=True)\nmodel = TSTPlus(c_in, c_out, seq_len, max_seq_len=max_seq_len, d_model=d_model, n_heads=n_heads,\n            d_k=d_k, d_v=d_v, d_ff=d_ff, dropout=dropout, activation=activation, n_layers=n_layers,\n            fc_dropout=fc_dropout, pe=pe, learn_pe=learn_pe, flatten=False, custom_head=custom_head, **kwargs)\ntest_eq(model.to(xb.device)(xb).shape, [bs, c_out])\nprint(f'model parameters: {count_parameters(model)}')\n\nmodel parameters: 554240\n\n\n\nbs = 8\nc_in = 9  # aka channels, features, variables, dimensions\nc_out = 2\nseq_len = 60\n\nxb = torch.randn(bs, c_in, seq_len)\n\n# standardize by channel by_var based on the training set\nxb = (xb - xb.mean((0, 2), keepdim=True)) / xb.std((0, 2), keepdim=True)\n\n# Settings\nmax_seq_len = 120\nd_model = 128\nn_heads = 16\nd_k = d_v = None # if None --> d_model // n_heads\nd_ff = 256\ndropout = 0.1\nact = \"gelu\"\nn_layers = 3\nfc_dropout = 0.1\npe='zeros'\nlearn_pe=True\nkwargs = {}\n# kwargs = dict(kernel_size=5, padding=2)\n\nmodel = TSTPlus(c_in, c_out, seq_len, max_seq_len=max_seq_len, d_model=d_model, n_heads=n_heads,\n            d_k=d_k, d_v=d_v, d_ff=d_ff, dropout=dropout, act=act, n_layers=n_layers,\n            fc_dropout=fc_dropout, pe=pe, learn_pe=learn_pe, **kwargs)\ntest_eq(model.to(xb.device)(xb).shape, [bs, c_out])\nprint(f'model parameters: {count_parameters(model)}')\nbody, head = model[0], model[1]\ntest_eq(body.to(xb.device)(xb).ndim, 3)\ntest_eq(head.to(xb.device)(body.to(xb.device)(xb)).ndim, 2)\nhead\n\nmodel parameters: 421762\n\n\nSequential(\n  (0): GELU(approximate='none')\n  (1): fastai.layers.Flatten(full=False)\n  (2): LinBnDrop(\n    (0): Dropout(p=0.1, inplace=False)\n    (1): Linear(in_features=7680, out_features=2, bias=True)\n  )\n)\n\n\n\nmodel.show_pe()\n\n\n\n\n\n\n\n\nmodel = TSTPlus(3, 2, 10)\nxb = torch.randn(4, 3, 10)\nyb = torch.randint(0, 2, (4,))\ntest_eq(model.backbone._key_padding_mask(xb)[1], None)\nrandom_idxs = random_choice(len(xb), 2, False)\nxb[random_idxs, :, -5:] = np.nan\nxb[random_idxs, 0, 1] = np.nan\ntest_eq(model.backbone._key_padding_mask(xb.clone())[1].data, (torch.isnan(xb).float().mean(1)==1).bool())\ntest_eq(model.backbone._key_padding_mask(xb.clone())[1].data.shape, (4,10))\nprint(torch.isnan(xb).sum())\npred = model.to(xb.device)(xb.clone())\nloss = CrossEntropyLossFlat()(pred, yb)\nloss.backward()\nmodel.to(xb.device).backbone._key_padding_mask(xb)[1].data.shape\n\ntensor(32)\n\n\ntorch.Size([4, 10])\n\n\n\nbs = 4\nc_in = 3\nseq_len = 10\nc_out = 2\nxb = torch.randn(bs, c_in, seq_len)\nxb[:, -1] = torch.randint(0, 2, (bs, seq_len)).sort()[0]\nmodel = TSTPlus(c_in, c_out, seq_len).to(xb.device)\ntest_eq(model.backbone._key_padding_mask(xb)[1], None)\nmodel = TSTPlus(c_in, c_out, seq_len, padding_var=-1).to(xb.device)\ntest_eq(model.backbone._key_padding_mask(xb)[1], (xb[:, -1]==1))\nmodel = TSTPlus(c_in, c_out, seq_len, padding_var=2).to(xb.device)\ntest_eq(model.backbone._key_padding_mask(xb)[1], (xb[:, -1]==1))\ntest_eq(model(xb).shape, (bs, c_out))\n\n\nbs = 4\nc_in = 3\nseq_len = 10\nc_out = 2\nxb = torch.randn(bs, c_in, seq_len)\nmodel = TSTPlus(c_in, c_out, seq_len, act='smelu')\n\n\nsource\n\n\nMultiTSTPlus\n\n MultiTSTPlus (feat_list, c_out, seq_len, max_seq_len:Optional[int]=512,\n               custom_head=None, n_layers:int=3, d_model:int=128,\n               n_heads:int=16, d_k:Optional[int]=None,\n               d_v:Optional[int]=None, d_ff:int=256, norm:str='BatchNorm',\n               attn_dropout:float=0.0, dropout:float=0.0, act:str='gelu',\n               key_padding_mask:bool='auto',\n               padding_var:Optional[int]=None,\n               attn_mask:Optional[torch.Tensor]=None,\n               res_attention:bool=True, pre_norm:bool=False,\n               store_attn:bool=False, pe:str='zeros', learn_pe:bool=True,\n               flatten:bool=True, fc_dropout:float=0.0,\n               concat_pool:bool=False, bn:bool=False,\n               y_range:Optional[tuple]=None, verbose:bool=False)\n\nA sequential container. Modules will be added to it in the order they are passed in the constructor. Alternatively, an OrderedDict of modules can be passed in. The forward() method of [Sequential](https://timeseriesAI.github.io/models.layers.html#sequential) accepts any input and forwards it to the first module it contains. It then “chains” outputs to inputs sequentially for each subsequent module, finally returning the output of the last module.\nThe value a [Sequential](https://timeseriesAI.github.io/models.layers.html#sequential) provides over manually calling a sequence of modules is that it allows treating the whole container as a single module, such that performing a transformation on the [Sequential](https://timeseriesAI.github.io/models.layers.html#sequential) applies to each of the modules it stores (which are each a registered submodule of the [Sequential](https://timeseriesAI.github.io/models.layers.html#sequential)).\nWhat’s the difference between a [Sequential](https://timeseriesAI.github.io/models.layers.html#sequential) and a :class:torch.nn.ModuleList? A ModuleList is exactly what it sounds like–a list for storing Module s! On the other hand, the layers in a [Sequential](https://timeseriesAI.github.io/models.layers.html#sequential) are connected in a cascading way.\nExample::\n# Using Sequential to create a small model. When `model` is run,\n# input will first be passed to `Conv2d(1,20,5)`. The output of\n# `Conv2d(1,20,5)` will be used as the input to the first\n# `ReLU`; the output of the first `ReLU` will become the input\n# for `Conv2d(20,64,5)`. Finally, the output of\n# `Conv2d(20,64,5)` will be used as input to the second `ReLU`\nmodel = nn.Sequential(\n          nn.Conv2d(1,20,5),\n          nn.ReLU(),\n          nn.Conv2d(20,64,5),\n          nn.ReLU()\n        )\n\n# Using Sequential with OrderedDict. This is functionally the\n# same as the above code\nmodel = nn.Sequential(OrderedDict([\n          ('conv1', nn.Conv2d(1,20,5)),\n          ('relu1', nn.ReLU()),\n          ('conv2', nn.Conv2d(20,64,5)),\n          ('relu2', nn.ReLU())\n        ]))\n\nbs = 8\nc_in = 7  # aka channels, features, variables, dimensions\nc_out = 2\nseq_len = 10\nxb2 = torch.randn(bs, c_in, seq_len)\nmodel1 = MultiTSTPlus([2, 5], c_out, seq_len)\nmodel2 = MultiTSTPlus(7, c_out, seq_len)\ntest_eq(model1.to(xb2.device)(xb2).shape, (bs, c_out))\ntest_eq(model1.to(xb2.device)(xb2).shape, model2.to(xb2.device)(xb2).shape)\ntest_eq(count_parameters(model1) > count_parameters(model2), True)\n\n\nbs = 8\nc_in = 7  # aka channels, features, variables, dimensions\nc_out = 2\nseq_len = 10\nxb2 = torch.randn(bs, c_in, seq_len)\nmodel1 = MultiTSTPlus([2, 5], c_out, seq_len, )\nmodel2 = MultiTSTPlus([[0,2,5], [0,1,3,4,6]], c_out, seq_len)\ntest_eq(model1.to(xb2.device)(xb2).shape, (bs, c_out))\ntest_eq(model1.to(xb2.device)(xb2).shape, model2.to(xb2.device)(xb2).shape)\n\n\nmodel1 = MultiTSTPlus([2, 5], c_out, seq_len, y_range=(0.5, 5.5))\nbody, head = split_model(model1)\ntest_eq(body.to(xb2.device)(xb2).ndim, 3)\ntest_eq(head.to(xb2.device)(body.to(xb2.device)(xb2)).ndim, 2)\nhead\n\nSequential(\n  (0): Sequential(\n    (0): GELU(approximate='none')\n    (1): fastai.layers.Flatten(full=False)\n    (2): LinBnDrop(\n      (0): Linear(in_features=2560, out_features=2, bias=True)\n    )\n  )\n)\n\n\n\nmodel = MultiTSTPlus([2, 5], c_out, seq_len, pre_norm=True)\n\n\nbs = 8\nn_vars = 3\nseq_len = 12\nc_out = 2\nxb = torch.rand(bs, n_vars, seq_len)\nnet = MultiTSTPlus(n_vars, c_out, seq_len)\nchange_model_head(net, create_pool_plus_head, concat_pool=False)\nprint(net.to(xb.device)(xb).shape)\nnet.head\n\ntorch.Size([8, 2])\n\n\nSequential(\n  (0): AdaptiveAvgPool1d(output_size=1)\n  (1): Reshape(bs)\n  (2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (3): Linear(in_features=128, out_features=512, bias=False)\n  (4): ReLU(inplace=True)\n  (5): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (6): Linear(in_features=512, out_features=2, bias=False)\n)\n\n\n\nbs = 8\nn_vars = 3\nseq_len = 12\nc_out = 10\nxb = torch.rand(bs, n_vars, seq_len)\nnew_head = partial(conv_lin_nd_head, d=(5 ,2))\nnet = MultiTSTPlus(n_vars, c_out, seq_len, custom_head=new_head)\nprint(net.to(xb.device)(xb).shape)\nnet.head\n\ntorch.Size([8, 5, 2, 10])\n\n\nSequential(\n  (0): create_conv_lin_nd_head(\n    (0): Conv1d(128, 10, kernel_size=(1,), stride=(1,))\n    (1): Linear(in_features=12, out_features=10, bias=True)\n    (2): Transpose(-1, -2)\n    (3): Reshape(bs, 5, 2, 10)\n  )\n)"
  },
  {
    "objectID": "models.explainability.html",
    "href": "models.explainability.html",
    "title": "Explainability",
    "section": "",
    "text": "Functionality to help with both global and local explainability.\n\n\nsource\n\nget_attribution_map\n\n get_attribution_map (model, modules, x, y=None, detach=True, cpu=False,\n                      apply_relu=True)\n\n\nsource\n\n\nget_acts_and_grads\n\n get_acts_and_grads (model, modules, x, y=None, detach=True, cpu=False)\n\nReturns activations and gradients for given modules in a model and a single input or a batch. Gradients require y value(s). If they are not provided, it will use the predictions."
  },
  {
    "objectID": "models.tabfusiontransformer.html",
    "href": "models.tabfusiontransformer.html",
    "title": "TabFusionTransformer",
    "section": "",
    "text": "This is a a Pytorch implementeation of TabTransformerTransformer created by Ignacio Oguiza (oguiza@timeseriesAI.co)\nThis implementation is inspired by:\nHuang, X., Khetan, A., Cvitkovic, M., & Karnin, Z. (2020). TabTransformer: Tabular Data Modeling Using Contextual Embeddings. arXiv preprint https://arxiv.org/pdf/2012.06678\nOfficial repo: https://github.com/awslabs/autogluon/tree/master/tabular/src/autogluon/tabular/models/tab_transformer\n\nsource\n\nTabFusionTransformer\n\n TabFusionTransformer (classes, cont_names, c_out, d_model=32, n_layers=6,\n                       n_heads=8, d_k=None, d_v=None, d_ff=None,\n                       res_attention=True, attention_act='gelu',\n                       res_dropout=0.0, fc_mults=(4, 2), fc_dropout=0.0,\n                       fc_act=None, fc_skip=False, fc_bn=False,\n                       bn_final=False, init=True)\n\nClass that allows you to pass one or multiple inputs\n\nsource\n\n\nTabFusionBackbone\n\n TabFusionBackbone (classes, cont_names, d_model=32, n_layers=6,\n                    n_heads=8, d_k=None, d_v=None, d_ff=None, init=True,\n                    res_attention=True, attention_act='gelu',\n                    res_dropout=0.0)\n\nBase class for all neural network modules.\nYour models should also subclass this class.\nModules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes::\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 20, 5)\n        self.conv2 = nn.Conv2d(20, 20, 5)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        return F.relu(self.conv2(x))\nSubmodules assigned in this way will be registered, and will have their parameters converted too when you call :meth:to, etc.\n.. note:: As per the example above, an __init__() call to the parent class must be made before assignment on the child.\n:ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool\n\nsource\n\n\nSequential\n\n Sequential (*args)\n\nClass that allows you to pass one or multiple inputs\n\nsource\n\n\nifnone\n\n ifnone (a, b)\n\nb if a is None else a\n\nfrom fastai.tabular.all import *\n\n\npath = untar_data(URLs.ADULT_SAMPLE)\ndf = pd.read_csv(path/'adult.csv')\ndls = TabularDataLoaders.from_csv(path/'adult.csv', path=path, y_names=\"salary\",\n    cat_names = ['workclass', 'education', 'marital-status', 'occupation', 'relationship', 'race'],\n    cont_names = ['age', 'fnlwgt', 'education-num'],\n    procs = [Categorify, FillMissing, Normalize])\nx_cat, x_cont, yb = first(dls.train)\nmodel = TabFusionTransformer(dls.classes, dls.cont_names, dls.c)\ntest_eq(model(x_cat, x_cont).shape, (dls.train.bs, dls.c))\n\n\nsource\n\n\nTSTabFusionTransformer\n\n TSTabFusionTransformer (c_in, c_out, seq_len, classes, cont_names,\n                         d_model=32, n_layers=6, n_heads=8, d_k=None,\n                         d_v=None, d_ff=None, res_attention=True,\n                         attention_act='gelu', res_dropout=0.0,\n                         fc_mults=(1, 0.5), fc_dropout=0.0, fc_act=None,\n                         fc_skip=False, fc_bn=False, bn_final=False,\n                         init=True)\n\nBase class for all neural network modules.\nYour models should also subclass this class.\nModules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes::\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 20, 5)\n        self.conv2 = nn.Conv2d(20, 20, 5)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        return F.relu(self.conv2(x))\nSubmodules assigned in this way will be registered, and will have their parameters converted too when you call :meth:to, etc.\n.. note:: As per the example above, an __init__() call to the parent class must be made before assignment on the child.\n:ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool\n\nclasses = {'education': ['#na#', '10th', '11th', '12th', '1st-4th', '5th-6th', '7th-8th', '9th', 'Assoc-acdm', 'Assoc-voc', 'Bachelors', 'Doctorate', \n                         'HS-grad', 'Masters', 'Preschool', 'Prof-school', 'Some-college'],\n 'education-num_na': ['#na#', False, True],\n 'marital-status': ['#na#', 'Divorced', 'Married-AF-spouse', 'Married-civ-spouse', 'Married-spouse-absent', 'Never-married', 'Separated', 'Widowed'],\n 'occupation': ['#na#', '?', 'Adm-clerical', 'Armed-Forces', 'Craft-repair', 'Exec-managerial', 'Farming-fishing', 'Handlers-cleaners', 'Machine-op-inspct', \n                'Other-service', 'Priv-house-serv', 'Prof-specialty', 'Protective-serv', 'Sales', 'Tech-support', 'Transport-moving'],\n 'race': ['#na#', 'Amer-Indian-Eskimo', 'Asian-Pac-Islander', 'Black', 'Other', 'White'],\n 'relationship': ['#na#', 'Husband', 'Not-in-family', 'Other-relative', 'Own-child', 'Unmarried', 'Wife'],\n 'workclass': ['#na#', '?', 'Federal-gov', 'Local-gov', 'Never-worked', 'Private', 'Self-emp-inc', 'Self-emp-not-inc', 'State-gov', 'Without-pay']}\n\ncont_names = ['a', 'b', 'c']\nc_out = 6\nx_ts = torch.randn(64, 3, 10)\nx_cat = torch.randint(0,3,(64,7))\nx_cont = torch.randn(64,3)\nmodel = TSTabFusionTransformer(x_ts.shape[1], c_out, x_ts.shape[-1], classes, cont_names)\nx = (x_ts, (x_cat, x_cont))\ntest_eq(model(x).shape, (x_ts.shape[0], c_out))"
  },
  {
    "objectID": "analysis.html",
    "href": "analysis.html",
    "title": "Analysis",
    "section": "",
    "text": "fastai Learner extensions useful to perform prediction analysis.\nsource"
  },
  {
    "objectID": "analysis.html#permutation-importance",
    "href": "analysis.html#permutation-importance",
    "title": "Analysis",
    "section": "Permutation importance",
    "text": "Permutation importance\nWe’ve also introduced 2 methods to help you better understand how important certain features or certain steps are for your model. Both methods use permutation importance.\n⚠️The permutation feature or step importance is defined as the decrease in a model score when a single feature or step value is randomly shuffled.\nSo if you using accuracy (higher is better), the most important features or steps will be those with a lower value on the chart (as randomly shuffling them reduces performance).\nThe opposite occurs for metrics like mean squared error (lower is better). In this case, the most important features or steps will be those with a higher value on the chart.\nThere are 2 issues with step importance:\n\nthere may be many steps and the analysis could take very long\nsteps will likely have a high autocorrelation\n\nFor those reasons, we’ve introduced an argument (n_steps) to group steps. In this way you’ll be able to know which part of the time series is the most important.\nFeature importance has been adapted from https://www.kaggle.com/cdeotte/lstm-feature-importance by Chris Deotte (Kaggle GrandMaster).\n\nsource\n\nLearner.feature_importance\n\n Learner.feature_importance (X=None, y=None,\n                             partial_n:(<class'int'>,<class'float'>)=None,\n                             method:str='permutation',\n                             feature_names:list=None, sel_classes:(<class'\n                             str'>,<class'list'>)=None,\n                             key_metric_idx:int=0, show_chart:bool=True,\n                             figsize:tuple=None, title:str=None,\n                             return_df:bool=True,\n                             save_df_path:pathlib.Path=None,\n                             random_state:int=23, verbose:bool=True)\n\nCalculates feature importance as the drop in the model’s validation loss or metric when a feature value is randomly shuffled\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nX\nNoneType\nNone\narray-like object containing the time series. If None, all data in the validation set will be used.\n\n\ny\nNoneType\nNone\narray-like object containing the targets. If None, all targets in the validation set will be used.\n\n\npartial_n\n(<class ‘int’>, <class ‘float’>)\nNone\n# (int) or % (float) of used to measure feature importance. If None, all data will be used.\n\n\nmethod\nstr\npermutation\nMethod used to invalidate feature. Use ‘permutation’ for shuffling or ‘ablation’ for setting values to np.nan.\n\n\nfeature_names\nlist\nNone\nOptional list of feature names that will be displayed if available. Otherwise var_0, var_1, etc.\n\n\nsel_classes\n(<class ‘str’>, <class ‘list’>)\nNone\nclasses for which the analysis will be made\n\n\nkey_metric_idx\nint\n0\nOptional position of the metric used. If None or no metric is available, the loss will be used.\n\n\nshow_chart\nbool\nTrue\nFlag to indicate if a chart showing permutation feature importance will be plotted.\n\n\nfigsize\ntuple\nNone\nSize of the chart.\n\n\ntitle\nstr\nNone\nOptional string that will be used as the chart title. If None ‘Permutation Feature Importance’.\n\n\nreturn_df\nbool\nTrue\nFlag to indicate if the dataframe with feature importance will be returned.\n\n\nsave_df_path\nPath\nNone\nPath where dataframe containing the permutation feature importance results will be saved.\n\n\nrandom_state\nint\n23\nOptional int that controls the shuffling applied to the data.\n\n\nverbose\nbool\nTrue\nFlag that controls verbosity.\n\n\n\n\nsource\n\n\nLearner.step_importance\n\n Learner.step_importance (X=None, y=None,\n                          partial_n:(<class'int'>,<class'float'>)=None,\n                          method:str='permutation', step_names:list=None,\n                          sel_classes:(<class'str'>,<class'list'>)=None,\n                          n_steps:int=1, key_metric_idx:int=0,\n                          show_chart:bool=True, figsize:tuple=(10, 5),\n                          title:str=None, xlabel=None,\n                          return_df:bool=True,\n                          save_df_path:pathlib.Path=None,\n                          random_state:int=23, verbose:bool=True)\n\nCalculates step importance as the drop in the model’s validation loss or metric when a step/s value/s is/are randomly shuffled\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nX\nNoneType\nNone\narray-like object containing the time series. If None, all data in the validation set will be used.\n\n\ny\nNoneType\nNone\narray-like object containing the targets. If None, all targets in the validation set will be used.\n\n\npartial_n\n(<class ‘int’>, <class ‘float’>)\nNone\n# (int) or % (float) of used to measure feature importance. If None, all data will be used.\n\n\nmethod\nstr\npermutation\nMethod used to invalidate feature. Use ‘permutation’ for shuffling or ‘ablation’ for setting values to np.nan.\n\n\nstep_names\nlist\nNone\nOptional list of step names that will be displayed if available. Otherwise 0, 1, 2, etc.\n\n\nsel_classes\n(<class ‘str’>, <class ‘list’>)\nNone\nclasses for which the analysis will be made\n\n\nn_steps\nint\n1\n# of steps that will be analyzed at a time. Default is 1.\n\n\nkey_metric_idx\nint\n0\nOptional position of the metric used. If None or no metric is available, the loss will be used.\n\n\nshow_chart\nbool\nTrue\nFlag to indicate if a chart showing permutation feature importance will be plotted.\n\n\nfigsize\ntuple\n(10, 5)\nSize of the chart.\n\n\ntitle\nstr\nNone\nOptional string that will be used as the chart title. If None ‘Permutation Feature Importance’.\n\n\nxlabel\nNoneType\nNone\nOptional string that will be used as the chart xlabel. If None ‘steps’.\n\n\nreturn_df\nbool\nTrue\nFlag to indicate if the dataframe with feature importance will be returned.\n\n\nsave_df_path\nPath\nNone\nPath where dataframe containing the permutation feature importance results will be saved.\n\n\nrandom_state\nint\n23\nOptional int that controls the shuffling applied to the data.\n\n\nverbose\nbool\nTrue\nFlag that controls verbosity.\n\n\n\n\nfrom tsai.data.external import get_UCR_data\nfrom tsai.data.preprocessing import TSRobustScale, TSStandardize\nfrom tsai.learner import ts_learner\nfrom tsai.models.FCNPlus import FCNPlus\nfrom tsai.metrics import accuracy\n\n\ndsid = 'NATOPS'\nX, y, splits = get_UCR_data(dsid, split_data=False)\ntfms  = [None, [TSClassification()]]\nbatch_tfms = TSRobustScale()\nbatch_tfms = TSStandardize()\ndls = get_ts_dls(X, y, splits=splits, sel_vars=[0, 3, 5, 8, 10], sel_steps=slice(-30, None), tfms=tfms, batch_tfms=batch_tfms)\nlearn = ts_learner(dls, FCNPlus, metrics=accuracy, train_metrics=True)\nlearn.fit_one_cycle(2)\nlearn.plot_metrics()\nlearn.show_probas()\nlearn.plot_confusion_matrix()\nlearn.plot_top_losses(X[splits[1]], y[splits[1]], largest=True)\nlearn.top_losses(X[splits[1]], y[splits[1]], largest=True)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(TensorBase([2.3490, 2.3218, 2.2956, 2.2231, 2.1906, 2.1742, 2.1699, 2.1641,\n             2.1579]),\n [129, 149, 56, 9, 166, 116, 154, 110, 104])\n\n\n\n\n\n\n\n\n\nlearn.feature_importance()\n\nX.shape: (180, 24, 51)\ny.shape: (180,)\nSelected metric: accuracy\nComputing feature importance (permutation method)...\n\n\n\n\n\n\n\n\n\n\n\n    \n      \n      0.00% [0/3 00:00<?]\n    \n    \n\n\n\n\n\n\n\n    \n      \n      0.00% [0/3 00:00<?]\n    \n    \n\n\n\n\n\n\n\n\n\n\n\n    \n      \n      0.00% [0/3 00:00<?]\n    \n    \n\n\n\n\n\n\n\n    \n      \n      0.00% [0/3 00:00<?]\n    \n    \n\n\n\n\n\n\n\n    \n      \n      0.00% [0/3 00:00<?]\n    \n    \n\n\n\n\n\n\n\n\n\n  \n    \n      \n      Feature\n      accuracy\n      accuracy_change\n    \n  \n  \n    \n      0\n      var_0\n      0.261111\n      0.066667\n    \n    \n      1\n      var_3\n      0.300000\n      0.027778\n    \n    \n      2\n      var_5\n      0.322222\n      0.005556\n    \n    \n      3\n      BASELINE\n      0.327778\n      -0.000000\n    \n    \n      4\n      var_8\n      0.327778\n      -0.000000\n    \n    \n      5\n      var_10\n      0.327778\n      -0.000000\n    \n  \n\n\n\n\n\nlearn.step_importance(n_steps=5);\n\n  0 step: BASELINE             accuracy: 0.327778\n\n\n\n\n\n\n\n    \n      \n      0.00% [0/3 00:00<?]\n    \n    \n\n\n  1 step: 21 to 25             accuracy: 0.322222\n\n\n\n\n\n\n  2 step: 26 to 30             accuracy: 0.327778\n\n\n\n\n\n\n\n    \n      \n      0.00% [0/3 00:00<?]\n    \n    \n\n\n  3 step: 31 to 35             accuracy: 0.311111\n\n\n\n\n\n\n  4 step: 36 to 40             accuracy: 0.272222\n\n\n\n\n\n\n\n    \n      \n      0.00% [0/3 00:00<?]\n    \n    \n\n\n  5 step: 41 to 45             accuracy: 0.288889\n\n\n\n\n\n\n  6 step: 46 to 50             accuracy: 0.305556\n\n\n\n\n\n\nYou may pass an X and y if you want to analyze a particular group of samples:\nlearn.feature_importance(X=X[splits[1]], y=y[splits[1]])\nIf you have a large validation dataset, you may also use the partial_n argument to select a fixed amount of samples (integer) or a percentage of the validation dataset (float):\nlearn.feature_importance(partial_n=.1)\nlearn.feature_importance(partial_n=100)"
  },
  {
    "objectID": "models.rnn.html",
    "href": "models.rnn.html",
    "title": "RNNs",
    "section": "",
    "text": "These are RNN, LSTM and GRU PyTorch implementations created by Ignacio Oguiza - oguiza@timeseriesAI.co\nsource"
  },
  {
    "objectID": "models.rnn.html#converting-a-model-to-torchscript",
    "href": "models.rnn.html#converting-a-model-to-torchscript",
    "title": "RNNs",
    "section": "Converting a model to TorchScript",
    "text": "Converting a model to TorchScript\n\nmodel = LSTM(c_in, c_out, hidden_size=100, n_layers=2, bidirectional=True, rnn_dropout=.5, fc_dropout=.5)\nmodel.eval()\ninp = torch.rand(1, c_in, 50)\noutput = model(inp)\nprint(output)\n\ntensor([[-0.0287, -0.0105]], grad_fn=<AddmmBackward0>)\n\n\n\nTracing\n\n# save to gpu, cpu or both\ntraced_cpu = torch.jit.trace(model.cpu(), inp)\nprint(traced_cpu)\ntorch.jit.save(traced_cpu, \"cpu.pt\")\n\n# load cpu or gpu model\ntraced_cpu = torch.jit.load(\"cpu.pt\")\ntest_eq(traced_cpu(inp), output)\n\n!rm \"cpu.pt\"\n\nLSTM(\n  original_name=LSTM\n  (rnn): LSTM(original_name=LSTM)\n  (dropout): Dropout(original_name=Dropout)\n  (fc): Linear(original_name=Linear)\n)\n\n\n\n\nScripting\n\n# save to gpu, cpu or both\nscripted_cpu = torch.jit.script(model.cpu())\nprint(scripted_cpu)\ntorch.jit.save(scripted_cpu, \"cpu.pt\")\n\n# load cpu or gpu model\nscripted_cpu = torch.jit.load(\"cpu.pt\")\ntest_eq(scripted_cpu(inp), output)\n\n!rm \"cpu.pt\"\n\nRecursiveScriptModule(\n  original_name=LSTM\n  (rnn): RecursiveScriptModule(original_name=LSTM)\n  (dropout): RecursiveScriptModule(original_name=Dropout)\n  (fc): RecursiveScriptModule(original_name=Linear)\n)"
  },
  {
    "objectID": "models.rnn.html#converting-a-model-to-onnx",
    "href": "models.rnn.html#converting-a-model-to-onnx",
    "title": "RNNs",
    "section": "Converting a model to ONNX",
    "text": "Converting a model to ONNX\nimport onnx\n\n# Export the model\ntorch.onnx.export(model.cpu(),               # model being run\n                  inp,                       # model input (or a tuple for multiple inputs)\n                  \"cpu.onnx\",                # where to save the model (can be a file or file-like object)\n                  export_params=True,        # store the trained parameter weights inside the model file\n                  verbose=False,\n                  opset_version=13,          # the ONNX version to export the model to\n                  do_constant_folding=True,  # whether to execute constant folding for optimization\n                  input_names = ['input'],   # the model's input names\n                  output_names = ['output'], # the model's output names\n                  dynamic_axes={\n                      'input'  : {0 : 'batch_size'}, \n                      'output' : {0 : 'batch_size'}} # variable length axes\n                 )\n\n# Load the model and check it's ok\nonnx_model = onnx.load(\"cpu.onnx\")\nonnx.checker.check_model(onnx_model)\n\n# You can ignore the WARNINGS below\nimport onnxruntime as ort\n\nort_sess = ort.InferenceSession('cpu.onnx')\nout = ort_sess.run(None, {'input': inp.numpy()})\n\n# input & output names\ninput_name = ort_sess.get_inputs()[0].name\noutput_name = ort_sess.get_outputs()[0].name\n\n# input dimensions\ninput_dims = ort_sess.get_inputs()[0].shape\nprint(input_name, output_name, input_dims)\n\ntest_close(out, output.detach().numpy())\n!rm \"cpu.onnx\""
  },
  {
    "objectID": "models.misc.html",
    "href": "models.misc.html",
    "title": "Miscellaneous",
    "section": "",
    "text": "This contains a set of experiments.\n\n\nsource\n\nInputWrapper\n\n InputWrapper (arch, c_in, c_out, seq_len, new_c_in=None,\n               new_seq_len=None, **kwargs)\n\nSame as nn.Module, but no need for subclasses to call super().__init__\n\nfrom tsai.models.TST import *\n\n\nxb = torch.randn(16, 1, 1000)\nmodel = InputWrapper(TST, 1, 4, 1000, 10, 224)\ntest_eq(model.to(xb.device)(xb).shape, (16,4))\n\n\nsource\n\n\nResidualWrapper\n\n ResidualWrapper (model)\n\nSame as nn.Module, but no need for subclasses to call super().__init__\n\nsource\n\n\nRecursiveWrapper\n\n RecursiveWrapper (model, n_steps, anchored=False)\n\nSame as nn.Module, but no need for subclasses to call super().__init__\n\nxb = torch.randn(16, 1, 20)\nmodel = RecursiveWrapper(TST(1, 1, 20), 5)\ntest_eq(model.to(xb.device)(xb).shape, (16, 5))"
  },
  {
    "objectID": "data.image.html",
    "href": "data.image.html",
    "title": "Imaging Time Series",
    "section": "",
    "text": "Main functions used to transform time series into TSImage tensors.\n\n\ndsid = 'NATOPS'\nX, y, splits = get_UCR_data(dsid, return_split=False)\n\n\nsource\n\nToTSImage\n\n ToTSImage (enc=None, dec=None, split_idx=None, order=None)\n\nDelegates (__call__,decode,setup) to (encodes,decodes,setups) if split_idx matches\n\nsource\n\n\nTSImage\n\n TSImage (x, **kwargs)\n\nA Tensor which support subclass pickling, and maintains metadata when casting or after methods\n\nsource\n\n\nTSToPlot\n\n TSToPlot (size:Optional[int]=224, dpi:int=100, lw=1, **kwargs)\n\nTransforms a time series batch to a 4d TSImage (bs, n_vars, size, size) by creating a matplotlib plot.\n\nout = TSToPlot()(TSTensor(X[:2]), split_idx=0)\nprint(out.shape)\nout[0].show()\n\ntorch.Size([2, 3, 224, 224])\n\n\n\n\n\n\nsource\n\n\nTSToMat\n\n TSToMat (size=224, dpi=100, cmap=None, **kwargs)\n\nTransforms a time series batch to a 4d TSImage (bs, n_vars, size, size) by creating a matplotlib matrix. Input data must be normalized with a range(-1, 1)\n\nout = TSToMat()(TSTensor(X[:2]), split_idx=0)\nprint(out.shape)\nout[0].show()\n\ntorch.Size([2, 3, 224, 224])\n\n\n\n\n\n\nout = TSToMat(cmap='spring')(TSTensor(X[:2]), split_idx=0)\nprint(out.shape)\nout[0].show()\n\ntorch.Size([2, 3, 224, 224])\n\n\n\n\n\n\nsource\n\n\nTSToJRP\n\n TSToJRP (size=224, cmap=None, dimension=1, time_delay=1, threshold=None,\n          percentage=10)\n\nTransforms a time series batch to a 4d TSImage (bs, n_vars, size, size) by applying Joint Recurrence Plot\n\nsource\n\n\nTSToRP\n\n TSToRP (size=224, cmap=None, dimension=1, time_delay=1, threshold=None,\n         percentage=10, flatten=False)\n\nTransforms a time series batch to a 4d TSImage (bs, n_vars, size, size) by applying Recurrence Plot. It requires input to be previously normalized between -1 and 1\n\nsource\n\n\nTSToMTF\n\n TSToMTF (size=224, cmap=None, n_bins=5, image_size=1.0,\n          strategy='quantile', overlapping=False, flatten=False)\n\nTransforms a time series batch to a 4d TSImage (bs, n_vars, size, size) by applying Markov Transition Field\n\nsource\n\n\nTSToGASF\n\n TSToGASF (size=224, cmap=None, range=None, image_size=1.0,\n           sample_range=(-1, 1), method='summation', overlapping=False,\n           flatten=False)\n\nTransforms a time series batch to a 4d TSImage (bs, n_vars, size, size) by applying Gramian Angular Summation Field. It requires either input to be previously normalized between -1 and 1 or set range to (-1, 1)\n\nsource\n\n\nTSToGADF\n\n TSToGADF (size=224, cmap=None, range=None, image_size=1.0,\n           sample_range=(-1, 1), method='summation', overlapping=False,\n           flatten=False)\n\nTransforms a time series batch to a 4d TSImage (bs, n_vars, size, size) by applying Gramian Angular Difference Field. It requires either input to be previously normalized between -1 and 1 or set range to (-1, 1)\n\nout = TSToRP()(TSTensor(X[:2]), split_idx=0)\nprint(out.shape)\nout[0].show()\n\ntorch.Size([2, 24, 224, 224])\n\n\n\n\n\n\no = TSTensor(X[0][1][None])\nencoder = RecurrencePlot()\na = encoder.fit_transform(o.cpu().numpy())[0]\no = TSTensor(X[0])\nencoder = RecurrencePlot()\nb = encoder.fit_transform(o.cpu().numpy())[1]\ntest_eq(a,b) # channels can all be processed in parallel\n\n\ntest_eq(TSToRP()(TSTensor(X[0]), split_idx=False)[0], TSToRP()(TSTensor(X[0][0][None]), split_idx=False)[0])\ntest_eq(TSToRP()(TSTensor(X[0]), split_idx=False)[1], TSToRP()(TSTensor(X[0][1][None]), split_idx=False)[0])\ntest_eq(TSToRP()(TSTensor(X[0]), split_idx=False)[2], TSToRP()(TSTensor(X[0][2][None]), split_idx=False)[0])\n\n\ndsid = 'NATOPS'\nX, y, splits = get_UCR_data(dsid, return_split=False)\ntfms = [None, Categorize()]\nbts = [[TSNormalize(), TSToPlot(100)], \n       [TSNormalize(), TSToMat(100)], \n       [TSNormalize(), TSToGADF(100)], \n       [TSNormalize(), TSToGASF(100)], \n       [TSNormalize(), TSToMTF(100)], \n       [TSNormalize(), TSToRP(100)]]\nbtns = ['Plot', 'Mat', 'GADF', 'GASF', 'MTF', 'RP']\ndsets = TSDatasets(X, y, tfms=tfms, splits=splits)\nfor i, (bt, btn) in enumerate(zip(bts, btns)): \n    dls = TSDataLoaders.from_dsets(dsets.train, dsets.valid, bs=8, batch_tfms=bt)\n    test_eq(dls.vars, 3 if i <2 else X.shape[1])\n    test_eq(dls.len, (100,100))\n    xb, yb = dls.train.one_batch()\n    print(i, btn, xb, xb.dtype, xb.min(), xb.max())\n    xb[0].show()\n    plt.show()\n\n0 Plot TSImage(shape:torch.Size([8, 3, 100, 100])) torch.float32 0.054901961237192154 1.0\n\n\n\n\n\n1 Mat TSImage(shape:torch.Size([8, 3, 100, 100])) torch.float32 0.04313725605607033 1.0\n\n\n\n\n\n2 GADF TSImage(shape:torch.Size([8, 24, 100, 100])) torch.float32 0.0 1.0\n\n\n\n\n\n3 GASF TSImage(shape:torch.Size([8, 24, 100, 100])) torch.float32 -5.960464477539063e-08 1.0\n\n\n\n\n\n4 MTF TSImage(shape:torch.Size([8, 24, 100, 100])) torch.float32 0.0 0.949999988079071\n\n\n\n\n\n5 RP TSImage(shape:torch.Size([8, 24, 100, 100])) torch.float32 0.0 0.8300262689590454\n\n\n\n\n\nThe simplest way to train a model using time series to image transforms is this:\ndsid = 'NATOPS'\nX, y, splits = get_UCR_data(dsid, return_split=False)\ntfms = [None, Categorize()]\nbatch_tfms = [TSNormalize(), TSToGADF(224)]\ndls = get_ts_dls(X, y, splits=splits, tfms=tfms, batch_tfms=batch_tfms)\nlearn = tsimage_learner(dls, xresnet34)\nlearn.fit_one_cycle(10)"
  },
  {
    "objectID": "models.tssequencerplus.html",
    "href": "models.tssequencerplus.html",
    "title": "TSSequencerPlus",
    "section": "",
    "text": "This is a PyTorch implementation created by Ignacio Oguiza (oguiza@timeseriesAI.co) based on Sequencer: Deep LSTM for Image Classification\nsource"
  },
  {
    "objectID": "models.tssequencerplus.html#feature-extractor",
    "href": "models.tssequencerplus.html#feature-extractor",
    "title": "TSSequencerPlus",
    "section": "Feature extractor",
    "text": "Feature extractor\nIt’s a known fact that transformers cannot be directly applied to long sequences. To avoid this, we have included a way to subsample the sequence to generate a more manageable input.\n\nfrom tsai.data.validation import get_splits\nfrom tsai.data.core import get_ts_dls\n\n\nX = np.zeros((10, 3, 5000)) \ny = np.random.randint(0,2,X.shape[0])\nsplits = get_splits(y)\ndls = get_ts_dls(X, y, splits=splits)\nxb, yb = dls.train.one_batch()\nxb\n\n\n\n\nTSTensor(samples:8, vars:3, len:5000, device=cpu, dtype=torch.float32)\n\n\nIf you try to use SequencerPlus, it’s likely you’ll get an ‘out-of-memory’ error.\nTo avoid this you can subsample the sequence reducing the input’s length. This can be done in multiple ways. Here are a few examples:\n\n# Separable convolution (to avoid mixing channels)\nfeature_extractor = Conv1d(xb.shape[1], xb.shape[1], ks=100, stride=50, padding=0, groups=xb.shape[1]).to(default_device())\nfeature_extractor.to(xb.device)(xb).shape\n\ntorch.Size([8, 3, 99])\n\n\n\n# Convolution (if you want to mix channels or change number of channels)\nfeature_extractor=MultiConv1d(xb.shape[1], 64, kss=[1,3,5,7,9], keep_original=True).to(default_device())\ntest_eq(feature_extractor.to(xb.device)(xb).shape, (xb.shape[0], 64, xb.shape[-1]))\n\n\n# MaxPool\nfeature_extractor = nn.Sequential(Pad1d((0, 50), 0), nn.MaxPool1d(kernel_size=100, stride=50)).to(default_device())\nfeature_extractor.to(xb.device)(xb).shape\n\ntorch.Size([8, 3, 100])\n\n\n\n# AvgPool\nfeature_extractor = nn.Sequential(Pad1d((0, 50), 0), nn.AvgPool1d(kernel_size=100, stride=50)).to(default_device())\nfeature_extractor.to(xb.device)(xb).shape\n\ntorch.Size([8, 3, 100])\n\n\nOnce you decide what type of transform you want to apply, you just need to pass the layer as the feature_extractor attribute:\n\nbs = 16\nnvars = 4\nseq_len = 1000\nc_out = 2\nd_model = 128\n\nxb = torch.rand(bs, nvars, seq_len)\nfeature_extractor = partial(Conv1d, ks=5, stride=3, padding=0, groups=xb.shape[1])\nmodel = TSSequencerPlus(nvars, c_out, seq_len, d_model=d_model, feature_extractor=feature_extractor)\ntest_eq(model.to(xb.device)(xb).shape, (bs, c_out))"
  },
  {
    "objectID": "models.tssequencerplus.html#categorical-variables",
    "href": "models.tssequencerplus.html#categorical-variables",
    "title": "TSSequencerPlus",
    "section": "Categorical variables",
    "text": "Categorical variables\n\nfrom tsai.utils import alphabet, ALPHABET\n\n\na = alphabet[np.random.randint(0,3,40)]\nb = ALPHABET[np.random.randint(6,10,40)]\nc = np.random.rand(40).reshape(4,1,10)\nmap_a = {k:v for v,k in enumerate(np.unique(a))}\nmap_b = {k:v for v,k in enumerate(np.unique(b))}\nn_cat_embeds = [len(m.keys()) for m in [map_a, map_b]]\nszs = [emb_sz_rule(n) for n in n_cat_embeds]\na = np.asarray(a.map(map_a)).reshape(4,1,10)\nb = np.asarray(b.map(map_b)).reshape(4,1,10)\ninp = torch.from_numpy(np.concatenate((c,a,b), 1)).float()\nfeature_extractor = partial(Conv1d, ks=3, padding='same')\nmodel = TSSequencerPlus(3, 2, 10, d_model=64, cat_pos=[1,2], feature_extractor=feature_extractor)\ntest_eq(model(inp).shape, (4,2))\n\n[W NNPACK.cpp:53] Could not initialize NNPACK! Reason: Unsupported hardware."
  },
  {
    "objectID": "models.tssequencerplus.html#sequence-embedding",
    "href": "models.tssequencerplus.html#sequence-embedding",
    "title": "TSSequencerPlus",
    "section": "Sequence Embedding",
    "text": "Sequence Embedding\nSometimes you have a samples with a very long sequence length. In those cases you may want to reduce it’s length before passing it to the transformer. To do that you may just pass a token_size like in this example:\n\nt = torch.rand(8, 2, 10080)\nSeqTokenizer(2, 128, 60)(t).shape\n\ntorch.Size([8, 128, 168])\n\n\n\nt = torch.rand(8, 2, 10080)\nmodel = TSSequencerPlus(2, 5, 10080, d_model=64, token_size=60)\nmodel(t).shape\n\ntorch.Size([8, 5])"
  },
  {
    "objectID": "callback.noisy_student.html",
    "href": "callback.noisy_student.html",
    "title": "Noisy student",
    "section": "",
    "text": "Callback to apply noisy student self-training (a semi-supervised learning approach) based on:\nXie, Q., Luong, M. T., Hovy, E., & Le, Q. V. (2020). Self-training with noisy student improves imagenet classification. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (pp. 10687-10698).\n\nsource\n\nNoisyStudent\n\n NoisyStudent (dl2:fastai.data.load.DataLoader, bs:Optional[int]=None,\n               l2pl_ratio:int=1, batch_tfms:Optional[list]=None,\n               do_setup:bool=True, pseudolabel_sample_weight:float=1.0,\n               verbose=False)\n\nA callback to implement the Noisy Student approach. In the original paper this was used in combination with noise: - stochastic depth: .8 - RandAugment: N=2, M=27 - dropout: .5\nSteps: 1. Build the dl you will use as a teacher 2. Create dl2 with the pseudolabels (either soft or hard preds) 3. Pass any required batch_tfms to the callback\n\nfrom tsai.data.all import *\nfrom tsai.models.all import *\nfrom tsai.tslearner import *\n\n\ndsid = 'NATOPS'\nX, y, splits = get_UCR_data(dsid, return_split=False)\n\n\npseudolabeled_data = X\nsoft_preds = True\n\npseudolabels = ToNumpyCategory()(y) if soft_preds else OneHot()(y)\ndsets2 = TSDatasets(pseudolabeled_data, pseudolabels)\ndl2 = TSDataLoader(dsets2, num_workers=0)\nnoisy_student_cb = NoisyStudent(dl2, bs=256, l2pl_ratio=2, verbose=True)\ntfms = [None, TSClassification]\nlearn = TSClassifier(X, y, splits=splits, tfms=tfms, batch_tfms=[TSStandardize(), TSRandomSize(.5)], cbs=noisy_student_cb)\nlearn.fit_one_cycle(1)\n\nlabels / pseudolabels per training batch              : 171 / 85\nrelative labeled/ pseudolabel sample weight in dataset: 4.0\n\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      accuracy\n      time\n    \n  \n  \n    \n      0\n      1.884984\n      1.809759\n      0.166667\n      00:06\n    \n  \n\n\n\n\nX: torch.Size([171, 24, 51])  X2: torch.Size([85, 24, 51])  X_comb: torch.Size([256, 24, 58])\ny: torch.Size([171])  y2: torch.Size([85])  y_comb: torch.Size([256])\n\n\n\npseudolabeled_data = X\nsoft_preds = False\n\npseudolabels = ToNumpyCategory()(y) if soft_preds else OneHot()(y)\ndsets2 = TSDatasets(pseudolabeled_data, pseudolabels)\ndl2 = TSDataLoader(dsets2, num_workers=0)\nnoisy_student_cb = NoisyStudent(dl2, bs=256, l2pl_ratio=2, verbose=True)\ntfms = [None, TSClassification]\nlearn = TSClassifier(X, y, splits=splits, tfms=tfms, batch_tfms=[TSStandardize(), TSRandomSize(.5)], cbs=noisy_student_cb)\nlearn.fit_one_cycle(1)\n\nlabels / pseudolabels per training batch              : 171 / 85\nrelative labeled/ pseudolabel sample weight in dataset: 4.0\n\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      accuracy\n      time\n    \n  \n  \n    \n      0\n      1.894964\n      1.814770\n      0.177778\n      00:03\n    \n  \n\n\n\n\nX: torch.Size([171, 24, 51])  X2: torch.Size([85, 24, 51])  X_comb: torch.Size([256, 24, 45])\ny: torch.Size([171, 6])  y2: torch.Size([85, 6])  y_comb: torch.Size([256, 6])"
  },
  {
    "objectID": "models.rnnattention.html",
    "href": "models.rnnattention.html",
    "title": "RNNAttention",
    "section": "",
    "text": "This is an custom PyTorch implementation by @yangtzech, based on TST implementation of Ignacio Oguiza."
  },
  {
    "objectID": "models.rnnattention.html#arguments",
    "href": "models.rnnattention.html#arguments",
    "title": "RNNAttention",
    "section": "Arguments",
    "text": "Arguments\nUsual values are the ones that appear in the “Attention is all you need” and “A Transformer-based Framework for Multivariate Time Series Representation Learning” papers. And some parameters are necessary for the RNN part.\nThe default values are the ones selected as a default configuration in the latter.\n\nc_in: the number of features (aka variables, dimensions, channels) in the time series dataset. dls.var\nc_out: the number of target classes. dls.c\nseq_len: number of time steps in the time series. dls.len\nhidden_size: the number of features in the hidden state in the RNN model. Default: 128.\nrnn_layers: the number of recurrent layers of the RNN model. Default: 1.\nbias: If False, then the layer does not use bias weights b_ih and b_hh. Default: True\nrnn_dropout: If non-zero, introduces a Dropout layer on the outputs of each RNN layer except the last layer, with dropout probability equal to :attr:rnn_dropout. Default: 0\nbidirectional: If True, becomes a bidirectional RNN. Default: False\nn_heads: parallel attention heads. Usual values: 8-16. Default: 16.\nd_k: size of the learned linear projection of queries and keys in the MHA. Usual values: 16-512. Default: None -> (d_model/n_heads) = 32.\nd_v: size of the learned linear projection of values in the MHA. Usual values: 16-512. Default: None -> (d_model/n_heads) = 32.\nd_ff: the dimension of the feedforward network model. Usual values: 256-4096. Default: 256.\nencoder_dropout: amount of residual dropout applied in the encoder. Usual values: 0.-0.3. Default: 0.1.\nact: the activation function of intermediate layer, relu or gelu. Default: ‘gelu’.\nencoder_layers: the number of sub-encoder-layers in the encoder. Usual values: 2-8. Default: 3.\nfc_dropout: dropout applied to the final fully connected layer. Usual values: 0.-0.8. Default: 0.\ny_range: range of possible y values (used in regression tasks). Default: None\nkwargs: nn.Conv1d kwargs. If not {}, a nn.Conv1d with those kwargs will be applied to original time series."
  },
  {
    "objectID": "models.rnnattention.html#imports",
    "href": "models.rnnattention.html#imports",
    "title": "RNNAttention",
    "section": "Imports",
    "text": "Imports"
  },
  {
    "objectID": "models.rnnattention.html#rnnattention",
    "href": "models.rnnattention.html#rnnattention",
    "title": "RNNAttention",
    "section": "RNNAttention",
    "text": "RNNAttention\n\nt = torch.rand(16, 50, 128)\noutput, attn = _MultiHeadAttention(d_model=128, n_heads=3, d_k=8, d_v=6)(t, t, t)\noutput.shape, attn.shape\n\n(torch.Size([16, 50, 128]), torch.Size([16, 3, 50, 50]))\n\n\n\nt = torch.rand(16, 50, 128)\noutput = _TSTEncoderLayer(q_len=50, d_model=128, n_heads=3, d_k=None, d_v=None, d_ff=512, dropout=0.1, activation='gelu')(t)\noutput.shape\n\ntorch.Size([16, 50, 128])\n\n\n\nsource\n\nGRUAttention\n\n GRUAttention (c_in:int, c_out:int, seq_len:int, hidden_size=128,\n               rnn_layers=1, bias=True, rnn_dropout=0,\n               bidirectional=False, encoder_layers:int=3, n_heads:int=16,\n               d_k:Optional[int]=None, d_v:Optional[int]=None,\n               d_ff:int=256, encoder_dropout:float=0.1, act:str='gelu',\n               fc_dropout:float=0.0, y_range:Optional[tuple]=None,\n               verbose:bool=False, **kwargs)\n\nSame as nn.Module, but no need for subclasses to call super().__init__\n\nsource\n\n\nLSTMAttention\n\n LSTMAttention (c_in:int, c_out:int, seq_len:int, hidden_size=128,\n                rnn_layers=1, bias=True, rnn_dropout=0,\n                bidirectional=False, encoder_layers:int=3, n_heads:int=16,\n                d_k:Optional[int]=None, d_v:Optional[int]=None,\n                d_ff:int=256, encoder_dropout:float=0.1, act:str='gelu',\n                fc_dropout:float=0.0, y_range:Optional[tuple]=None,\n                verbose:bool=False, **kwargs)\n\nSame as nn.Module, but no need for subclasses to call super().__init__\n\nsource\n\n\nRNNAttention\n\n RNNAttention (c_in:int, c_out:int, seq_len:int, hidden_size=128,\n               rnn_layers=1, bias=True, rnn_dropout=0,\n               bidirectional=False, encoder_layers:int=3, n_heads:int=16,\n               d_k:Optional[int]=None, d_v:Optional[int]=None,\n               d_ff:int=256, encoder_dropout:float=0.1, act:str='gelu',\n               fc_dropout:float=0.0, y_range:Optional[tuple]=None,\n               verbose:bool=False, **kwargs)\n\nSame as nn.Module, but no need for subclasses to call super().__init__\n\nbs = 32\nc_in = 9  # aka channels, features, variables, dimensions\nc_out = 2\nseq_len = 500\n\nxb = torch.randn(bs, c_in, seq_len)\n\n# standardize by channel by_var based on the training set\nxb = (xb - xb.mean((0, 2), keepdim=True)) / xb.std((0, 2), keepdim=True)\n\n# Settings\nhidden_size = 128\nrnn_layers=1\nbias=True\nrnn_dropout=0\nbidirectional=False\nencoder_layers=3\nn_heads = 16\nd_k = d_v = None # if None --> d_model // n_heads\nd_ff = 256\nencoder_dropout = 0.1\nactivation = \"gelu\"\nfc_dropout = 0.1\nkwargs = {}\n\nmodel = RNNAttention(c_in, c_out, seq_len, hidden_size=hidden_size, rnn_layers=rnn_layers, bias=bias, rnn_dropout=rnn_dropout, bidirectional=bidirectional,\n            encoder_layers=encoder_layers, n_heads=n_heads,\n            d_k=d_k, d_v=d_v, d_ff=d_ff, encoder_dropout=encoder_dropout, activation=activation, \n            fc_dropout=fc_dropout, **kwargs)\ntest_eq(model.to(xb.device)(xb).shape, [bs, c_out])\nprint(f'model parameters: {count_parameters(model)}')\n\nmodel parameters: 541698\n\n\n\nbs = 32\nc_in = 9  # aka channels, features, variables, dimensions\nc_out = 2\nseq_len = 60\n\nxb = torch.randn(bs, c_in, seq_len)\n\n# standardize by channel by_var based on the training set\nxb = (xb - xb.mean((0, 2), keepdim=True)) / xb.std((0, 2), keepdim=True)\n\n# Settings\nhidden_size = 128\nrnn_layers=1\nbias=True\nrnn_dropout=0\nbidirectional=False\nencoder_layers=3\nn_heads = 16\nd_k = d_v = None # if None --> d_model // n_heads\nd_ff = 256\nencoder_dropout = 0.1\nactivation = \"gelu\"\nfc_dropout = 0.1\nkwargs = {}\n# kwargs = dict(kernel_size=5, padding=2)\n\nmodel = RNNAttention(c_in, c_out, seq_len, hidden_size=hidden_size, rnn_layers=rnn_layers, bias=bias, rnn_dropout=rnn_dropout, bidirectional=bidirectional,\n            encoder_layers=encoder_layers, n_heads=n_heads,\n            d_k=d_k, d_v=d_v, d_ff=d_ff, encoder_dropout=encoder_dropout, activation=activation, \n            fc_dropout=fc_dropout, **kwargs)\ntest_eq(model.to(xb.device)(xb).shape, [bs, c_out])\nprint(f'model parameters: {count_parameters(model)}')\n\nmodel parameters: 429058"
  },
  {
    "objectID": "models.gmlp.html",
    "href": "models.gmlp.html",
    "title": "gMLP",
    "section": "",
    "text": "This is an unofficial PyTorch implementation based on:\n\nLiu, H., Dai, Z., So, D. R., & Le, Q. V. (2021). Pay Attention to MLPs. arXiv preprint arXiv:2105.08050.\nCholakov, R., & Kolev, T. (2022). The GatedTabTransformer. An enhanced deep learning architecture for tabular modeling. arXiv preprint arXiv:2201.00199.\n\n\nsource\n\ngMLP\n\n gMLP (c_in, c_out, seq_len, patch_size=1, d_model=256, d_ffn=512,\n       depth=6)\n\nBase class for all neural network modules.\nYour models should also subclass this class.\nModules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes::\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 20, 5)\n        self.conv2 = nn.Conv2d(20, 20, 5)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        return F.relu(self.conv2(x))\nSubmodules assigned in this way will be registered, and will have their parameters converted too when you call :meth:to, etc.\n.. note:: As per the example above, an __init__() call to the parent class must be made before assignment on the child.\n:ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool\n\nbs = 16\nc_in = 3\nc_out = 2\nseq_len = 64\npatch_size = 4\nxb = torch.rand(bs, c_in, seq_len)\nmodel = gMLP(c_in, c_out, seq_len, patch_size=patch_size)\ntest_eq(model(xb).shape, (bs, c_out))"
  },
  {
    "objectID": "data.unwindowed.html",
    "href": "data.unwindowed.html",
    "title": "Unwindowed datasets",
    "section": "",
    "text": "Functionality that will allow you to create a dataset that applies sliding windows to the input data on the fly. This heavily reduces the size of the input data files, as only the original unwindowed data needs to be stored.\n\nI’d like to thank both Thomas Capelle (https://github.com/tcapelle) and Xander Dunn (https://github.com/xanderdunn) for their contributions to make this code possible.\n\nsource\n\nTSUnwindowedDatasets\n\n TSUnwindowedDatasets (dataset, splits)\n\nBase class for lists with subsets\n\nsource\n\n\nTSUnwindowedDataset\n\n TSUnwindowedDataset (X=None, y=None, y_func=None, window_size=1,\n                      stride=1, drop_start=0, drop_end=0, seq_first=True,\n                      **kwargs)\n\nInitialize self. See help(type(self)) for accurate signature.\n\ndef y_func(y): return y.astype('float').mean(1)\n\nThis approach works with both univariate and multivariate data.\n\nUnivariate: we’ll use a simple array with 20 values, one with the seq_len first (X0), the other with seq_len second (X1).\nMultivariate: we’ll use 2 time series arrays, one with the seq_len first (X2), the other with seq_len second (X3). No sliding window has been applied to them yet.\n\n\n# Univariate\nX0 = np.arange(20).astype(float)\nX1 = np.arange(20).reshape(1, -1).astype(float)\nX0.shape, X0, X1.shape, X1\n\n((20,),\n array([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12.,\n        13., 14., 15., 16., 17., 18., 19.]),\n (1, 20),\n array([[ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12.,\n         13., 14., 15., 16., 17., 18., 19.]]))\n\n\n\n# Multivariate\nX2 = np.arange(20).reshape(-1,1)*np.array([1, 10, 100]).reshape(1,-1).astype(float)\nX3 = np.arange(20).reshape(1,-1)*np.array([1, 10, 100]).reshape(-1,1).astype(float)\nX2.shape, X3.shape, X2, X3\n\n((20, 3),\n (3, 20),\n array([[0.0e+00, 0.0e+00, 0.0e+00],\n        [1.0e+00, 1.0e+01, 1.0e+02],\n        [2.0e+00, 2.0e+01, 2.0e+02],\n        [3.0e+00, 3.0e+01, 3.0e+02],\n        [4.0e+00, 4.0e+01, 4.0e+02],\n        [5.0e+00, 5.0e+01, 5.0e+02],\n        [6.0e+00, 6.0e+01, 6.0e+02],\n        [7.0e+00, 7.0e+01, 7.0e+02],\n        [8.0e+00, 8.0e+01, 8.0e+02],\n        [9.0e+00, 9.0e+01, 9.0e+02],\n        [1.0e+01, 1.0e+02, 1.0e+03],\n        [1.1e+01, 1.1e+02, 1.1e+03],\n        [1.2e+01, 1.2e+02, 1.2e+03],\n        [1.3e+01, 1.3e+02, 1.3e+03],\n        [1.4e+01, 1.4e+02, 1.4e+03],\n        [1.5e+01, 1.5e+02, 1.5e+03],\n        [1.6e+01, 1.6e+02, 1.6e+03],\n        [1.7e+01, 1.7e+02, 1.7e+03],\n        [1.8e+01, 1.8e+02, 1.8e+03],\n        [1.9e+01, 1.9e+02, 1.9e+03]]),\n array([[0.0e+00, 1.0e+00, 2.0e+00, 3.0e+00, 4.0e+00, 5.0e+00, 6.0e+00,\n         7.0e+00, 8.0e+00, 9.0e+00, 1.0e+01, 1.1e+01, 1.2e+01, 1.3e+01,\n         1.4e+01, 1.5e+01, 1.6e+01, 1.7e+01, 1.8e+01, 1.9e+01],\n        [0.0e+00, 1.0e+01, 2.0e+01, 3.0e+01, 4.0e+01, 5.0e+01, 6.0e+01,\n         7.0e+01, 8.0e+01, 9.0e+01, 1.0e+02, 1.1e+02, 1.2e+02, 1.3e+02,\n         1.4e+02, 1.5e+02, 1.6e+02, 1.7e+02, 1.8e+02, 1.9e+02],\n        [0.0e+00, 1.0e+02, 2.0e+02, 3.0e+02, 4.0e+02, 5.0e+02, 6.0e+02,\n         7.0e+02, 8.0e+02, 9.0e+02, 1.0e+03, 1.1e+03, 1.2e+03, 1.3e+03,\n         1.4e+03, 1.5e+03, 1.6e+03, 1.7e+03, 1.8e+03, 1.9e+03]]))\n\n\nNow, instead of applying SlidingWindow to create and save the time series that can be consumed by a time series model, we can use a dataset that creates the data on the fly. In this way we avoid the need to create and save large files. This approach is also useful when you want to test different sliding window sizes, as otherwise you would need to create files for every size you want to test.The dataset will create the samples correctly formatted and ready to be passed on to a time series architecture.\n\nwds0 = TSUnwindowedDataset(X0, window_size=5, stride=2, seq_first=True)[:][0]\nwds1 = TSUnwindowedDataset(X1, window_size=5, stride=2, seq_first=False)[:][0]\ntest_eq(wds0, wds1)\nwds0, wds0.data, wds1, wds1.data\n\n(TSTensor(samples:8, vars:1, len:5, device=cpu),\n tensor([[[ 0.,  1.,  2.,  3.,  4.]],\n \n         [[ 2.,  3.,  4.,  5.,  6.]],\n \n         [[ 4.,  5.,  6.,  7.,  8.]],\n \n         [[ 6.,  7.,  8.,  9., 10.]],\n \n         [[ 8.,  9., 10., 11., 12.]],\n \n         [[10., 11., 12., 13., 14.]],\n \n         [[12., 13., 14., 15., 16.]],\n \n         [[14., 15., 16., 17., 18.]]]),\n TSTensor(samples:8, vars:1, len:5, device=cpu),\n tensor([[[ 0.,  1.,  2.,  3.,  4.]],\n \n         [[ 2.,  3.,  4.,  5.,  6.]],\n \n         [[ 4.,  5.,  6.,  7.,  8.]],\n \n         [[ 6.,  7.,  8.,  9., 10.]],\n \n         [[ 8.,  9., 10., 11., 12.]],\n \n         [[10., 11., 12., 13., 14.]],\n \n         [[12., 13., 14., 15., 16.]],\n \n         [[14., 15., 16., 17., 18.]]]))\n\n\n\nwds2 = TSUnwindowedDataset(X2, window_size=5, stride=2, seq_first=True)[:][0]\nwds3 = TSUnwindowedDataset(X3, window_size=5, stride=2, seq_first=False)[:][0]\ntest_eq(wds2, wds3)\nwds2, wds3, wds2.data, wds3.data\n\n(TSTensor(samples:8, vars:3, len:5, device=cpu),\n TSTensor(samples:8, vars:3, len:5, device=cpu),\n tensor([[[0.0000e+00, 1.0000e+00, 2.0000e+00, 3.0000e+00, 4.0000e+00],\n          [0.0000e+00, 1.0000e+01, 2.0000e+01, 3.0000e+01, 4.0000e+01],\n          [0.0000e+00, 1.0000e+02, 2.0000e+02, 3.0000e+02, 4.0000e+02]],\n \n         [[2.0000e+00, 3.0000e+00, 4.0000e+00, 5.0000e+00, 6.0000e+00],\n          [2.0000e+01, 3.0000e+01, 4.0000e+01, 5.0000e+01, 6.0000e+01],\n          [2.0000e+02, 3.0000e+02, 4.0000e+02, 5.0000e+02, 6.0000e+02]],\n \n         [[4.0000e+00, 5.0000e+00, 6.0000e+00, 7.0000e+00, 8.0000e+00],\n          [4.0000e+01, 5.0000e+01, 6.0000e+01, 7.0000e+01, 8.0000e+01],\n          [4.0000e+02, 5.0000e+02, 6.0000e+02, 7.0000e+02, 8.0000e+02]],\n \n         [[6.0000e+00, 7.0000e+00, 8.0000e+00, 9.0000e+00, 1.0000e+01],\n          [6.0000e+01, 7.0000e+01, 8.0000e+01, 9.0000e+01, 1.0000e+02],\n          [6.0000e+02, 7.0000e+02, 8.0000e+02, 9.0000e+02, 1.0000e+03]],\n \n         [[8.0000e+00, 9.0000e+00, 1.0000e+01, 1.1000e+01, 1.2000e+01],\n          [8.0000e+01, 9.0000e+01, 1.0000e+02, 1.1000e+02, 1.2000e+02],\n          [8.0000e+02, 9.0000e+02, 1.0000e+03, 1.1000e+03, 1.2000e+03]],\n \n         [[1.0000e+01, 1.1000e+01, 1.2000e+01, 1.3000e+01, 1.4000e+01],\n          [1.0000e+02, 1.1000e+02, 1.2000e+02, 1.3000e+02, 1.4000e+02],\n          [1.0000e+03, 1.1000e+03, 1.2000e+03, 1.3000e+03, 1.4000e+03]],\n \n         [[1.2000e+01, 1.3000e+01, 1.4000e+01, 1.5000e+01, 1.6000e+01],\n          [1.2000e+02, 1.3000e+02, 1.4000e+02, 1.5000e+02, 1.6000e+02],\n          [1.2000e+03, 1.3000e+03, 1.4000e+03, 1.5000e+03, 1.6000e+03]],\n \n         [[1.4000e+01, 1.5000e+01, 1.6000e+01, 1.7000e+01, 1.8000e+01],\n          [1.4000e+02, 1.5000e+02, 1.6000e+02, 1.7000e+02, 1.8000e+02],\n          [1.4000e+03, 1.5000e+03, 1.6000e+03, 1.7000e+03, 1.8000e+03]]]),\n tensor([[[0.0000e+00, 1.0000e+00, 2.0000e+00, 3.0000e+00, 4.0000e+00],\n          [0.0000e+00, 1.0000e+01, 2.0000e+01, 3.0000e+01, 4.0000e+01],\n          [0.0000e+00, 1.0000e+02, 2.0000e+02, 3.0000e+02, 4.0000e+02]],\n \n         [[2.0000e+00, 3.0000e+00, 4.0000e+00, 5.0000e+00, 6.0000e+00],\n          [2.0000e+01, 3.0000e+01, 4.0000e+01, 5.0000e+01, 6.0000e+01],\n          [2.0000e+02, 3.0000e+02, 4.0000e+02, 5.0000e+02, 6.0000e+02]],\n \n         [[4.0000e+00, 5.0000e+00, 6.0000e+00, 7.0000e+00, 8.0000e+00],\n          [4.0000e+01, 5.0000e+01, 6.0000e+01, 7.0000e+01, 8.0000e+01],\n          [4.0000e+02, 5.0000e+02, 6.0000e+02, 7.0000e+02, 8.0000e+02]],\n \n         [[6.0000e+00, 7.0000e+00, 8.0000e+00, 9.0000e+00, 1.0000e+01],\n          [6.0000e+01, 7.0000e+01, 8.0000e+01, 9.0000e+01, 1.0000e+02],\n          [6.0000e+02, 7.0000e+02, 8.0000e+02, 9.0000e+02, 1.0000e+03]],\n \n         [[8.0000e+00, 9.0000e+00, 1.0000e+01, 1.1000e+01, 1.2000e+01],\n          [8.0000e+01, 9.0000e+01, 1.0000e+02, 1.1000e+02, 1.2000e+02],\n          [8.0000e+02, 9.0000e+02, 1.0000e+03, 1.1000e+03, 1.2000e+03]],\n \n         [[1.0000e+01, 1.1000e+01, 1.2000e+01, 1.3000e+01, 1.4000e+01],\n          [1.0000e+02, 1.1000e+02, 1.2000e+02, 1.3000e+02, 1.4000e+02],\n          [1.0000e+03, 1.1000e+03, 1.2000e+03, 1.3000e+03, 1.4000e+03]],\n \n         [[1.2000e+01, 1.3000e+01, 1.4000e+01, 1.5000e+01, 1.6000e+01],\n          [1.2000e+02, 1.3000e+02, 1.4000e+02, 1.5000e+02, 1.6000e+02],\n          [1.2000e+03, 1.3000e+03, 1.4000e+03, 1.5000e+03, 1.6000e+03]],\n \n         [[1.4000e+01, 1.5000e+01, 1.6000e+01, 1.7000e+01, 1.8000e+01],\n          [1.4000e+02, 1.5000e+02, 1.6000e+02, 1.7000e+02, 1.8000e+02],\n          [1.4000e+03, 1.5000e+03, 1.6000e+03, 1.7000e+03, 1.8000e+03]]]))"
  },
  {
    "objectID": "models.mlp.html",
    "href": "models.mlp.html",
    "title": "MLP",
    "section": "",
    "text": "This is an unofficial PyTorch implementation created by Ignacio Oguiza (oguiza@timeseriesAI.co) based on:\nFawaz, H. I., Forestier, G., Weber, J., Idoumghar, L., & Muller, P. A. (2019). Deep learning for time series classification: a review. Data Mining and Knowledge Discovery, 33(4), 917-963.\nOfficial MLP TensorFlow implementation: https://github.com/hfawaz/dl-4-tsc/blob/master/classifiers/mlp.py\n\nsource\n\nMLP\n\n MLP (c_in, c_out, seq_len, layers=[500, 500, 500], ps=[0.1, 0.2, 0.2],\n      act=ReLU(inplace=True), use_bn=False, bn_final=False,\n      lin_first=False, fc_dropout=0.0, y_range=None)\n\nSame as nn.Module, but no need for subclasses to call super().__init__\n\nbs = 16\nnvars = 3\nseq_len = 128\nc_out = 2\nxb = torch.rand(bs, nvars, seq_len)\nmodel = MLP(nvars, c_out, seq_len)\ntest_eq(model(xb).shape, (bs, c_out))\nmodel\n\nMLP(\n  (flatten): Reshape(bs)\n  (mlp): ModuleList(\n    (0): LinBnDrop(\n      (0): Dropout(p=0.1, inplace=False)\n      (1): Linear(in_features=384, out_features=500, bias=True)\n      (2): ReLU(inplace=True)\n    )\n    (1): LinBnDrop(\n      (0): Dropout(p=0.2, inplace=False)\n      (1): Linear(in_features=500, out_features=500, bias=True)\n      (2): ReLU(inplace=True)\n    )\n    (2): LinBnDrop(\n      (0): Dropout(p=0.2, inplace=False)\n      (1): Linear(in_features=500, out_features=500, bias=True)\n      (2): ReLU(inplace=True)\n    )\n  )\n  (head): Sequential(\n    (0): LinBnDrop(\n      (0): Linear(in_features=500, out_features=2, bias=True)\n    )\n  )\n)"
  },
  {
    "objectID": "optimizer.html",
    "href": "optimizer.html",
    "title": "Optimizers",
    "section": "",
    "text": "This contains a set of optimizers.\n\n\nsource\n\nwrap_optimizer\n\n wrap_optimizer (opt, **kwargs)\n\nYou can natively use any of the optimizers included in the fastai library. You just need to pass it to the learner as the opt_func.\nIn addition, you will be able to use any of the optimizers from:\n\nPytorch\ntorch_optimizer (https://github.com/jettify/pytorch-optimizer). In this case, you will need to install torch-optimizer first)\n\nExamples of use:\nadamw = wrap_optimizer(torch.optim.AdamW)\nimport torch_optimizer as optim\nadabelief = wrap_optimizer(optim.AdaBelief)\nIf you want to use any these last 2, you can use the wrap_optimizer function. Here are a few examples:"
  },
  {
    "objectID": "metrics.html",
    "href": "metrics.html",
    "title": "Metrics",
    "section": "",
    "text": "Metrics not included in fastai.\n\n\nsource\n\nMatthewsCorrCoefBinary\n\n MatthewsCorrCoefBinary (sample_weight=None)\n\nMatthews correlation coefficient for single-label classification problems\n\nsource\n\n\nget_task_metrics\n\n get_task_metrics (dls, binary_metrics=None, multi_class_metrics=None,\n                   regression_metrics=None, verbose=True)\n\nAll metrics applicable to multi classification have been created by Doug Williams (https://github.com/williamsdoug). Thanks a lot Doug!!\n\nsource\n\n\nF1_multi\n\n F1_multi (*args, **kwargs)\n\n\nsource\n\n\nFbeta_multi\n\n Fbeta_multi (inp, targ, beta=1.0, thresh=0.5, sigmoid=True)\n\nComputes Fbeta when inp and targ are the same size.\n\nsource\n\n\nbalanced_accuracy_multi\n\n balanced_accuracy_multi (inp, targ, thresh=0.5, sigmoid=True)\n\nComputes balanced accuracy when inp and targ are the same size.\n\nsource\n\n\nspecificity_multi\n\n specificity_multi (inp, targ, thresh=0.5, sigmoid=True)\n\nComputes specificity (true negative rate) when inp and targ are the same size.\n\nsource\n\n\nrecall_multi\n\n recall_multi (inp, targ, thresh=0.5, sigmoid=True)\n\nComputes recall when inp and targ are the same size.\n\nsource\n\n\nprecision_multi\n\n precision_multi (inp, targ, thresh=0.5, sigmoid=True)\n\nComputes precision when inp and targ are the same size.\n\nsource\n\n\nmetrics_multi_common\n\n metrics_multi_common (inp, targ, thresh=0.5, sigmoid=True,\n                       by_sample=False)\n\nComputes TP, TN, FP, FN when inp and targ are the same size.\n\nsource\n\n\naccuracy_multi\n\n accuracy_multi (inp, targ, thresh=0.5, sigmoid=True, by_sample=False)\n\nComputes accuracy when inp and targ are the same size.\n\nsource\n\n\nmae\n\n mae (inp, targ)\n\nMean absolute error between inp and targ.\n\nsource\n\n\nmape\n\n mape (inp, targ)\n\nMean absolute percentage error between inp and targ.\n\nn_classes = 4\ninp = torch.normal(0, 1, (16, 20, n_classes))\ntarg = torch.randint(0, n_classes, (16, 20)).to(torch.int8)\n_mAP(inp, targ)\n\n0.27493315845795063"
  },
  {
    "objectID": "models.xcmplus.html",
    "href": "models.xcmplus.html",
    "title": "XCMPlus",
    "section": "",
    "text": "This is an unofficial PyTorch implementation of XCM created by Ignacio Oguiza (oguiza@timeseriesAI.co).\n\nsource\n\nXCMPlus\n\n XCMPlus (c_in:int, c_out:int, seq_len:Optional[int]=None, nf:int=128,\n          window_perc:float=1.0, flatten:bool=False, custom_head:<built-\n          infunctioncallable>=None, concat_pool:bool=False,\n          fc_dropout:float=0.0, bn:bool=False, y_range:tuple=None,\n          **kwargs)\n\nA sequential container. Modules will be added to it in the order they are passed in the constructor. Alternatively, an OrderedDict of modules can be passed in. The forward() method of [Sequential](https://timeseriesAI.github.io/models.layers.html#sequential) accepts any input and forwards it to the first module it contains. It then “chains” outputs to inputs sequentially for each subsequent module, finally returning the output of the last module.\nThe value a [Sequential](https://timeseriesAI.github.io/models.layers.html#sequential) provides over manually calling a sequence of modules is that it allows treating the whole container as a single module, such that performing a transformation on the [Sequential](https://timeseriesAI.github.io/models.layers.html#sequential) applies to each of the modules it stores (which are each a registered submodule of the [Sequential](https://timeseriesAI.github.io/models.layers.html#sequential)).\nWhat’s the difference between a [Sequential](https://timeseriesAI.github.io/models.layers.html#sequential) and a :class:torch.nn.ModuleList? A ModuleList is exactly what it sounds like–a list for storing Module s! On the other hand, the layers in a [Sequential](https://timeseriesAI.github.io/models.layers.html#sequential) are connected in a cascading way.\nExample::\n# Using Sequential to create a small model. When `model` is run,\n# input will first be passed to `Conv2d(1,20,5)`. The output of\n# `Conv2d(1,20,5)` will be used as the input to the first\n# `ReLU`; the output of the first `ReLU` will become the input\n# for `Conv2d(20,64,5)`. Finally, the output of\n# `Conv2d(20,64,5)` will be used as input to the second `ReLU`\nmodel = nn.Sequential(\n          nn.Conv2d(1,20,5),\n          nn.ReLU(),\n          nn.Conv2d(20,64,5),\n          nn.ReLU()\n        )\n\n# Using Sequential with OrderedDict. This is functionally the\n# same as the above code\nmodel = nn.Sequential(OrderedDict([\n          ('conv1', nn.Conv2d(1,20,5)),\n          ('relu1', nn.ReLU()),\n          ('conv2', nn.Conv2d(20,64,5)),\n          ('relu2', nn.ReLU())\n        ]))\n\nfrom tsai.data.basics import *\nfrom tsai.learner import *\n\n\ndsid = 'NATOPS'\nX, y, splits = get_UCR_data(dsid, split_data=False)\ntfms = [None, TSCategorize()]\ndls = get_ts_dls(X, y, splits=splits, tfms=tfms)\nmodel =  XCMPlus(dls.vars, dls.c, dls.len)\nlearn = ts_learner(dls, model, metrics=accuracy)\nxb, yb = dls.one_batch()\n\nbs, c_in, seq_len = xb.shape\nc_out = len(np.unique(yb.cpu().numpy()))\n\nmodel = XCMPlus(c_in, c_out, seq_len, fc_dropout=.5)\ntest_eq(model.to(xb.device)(xb).shape, (bs, c_out))\nmodel = XCMPlus(c_in, c_out, seq_len, concat_pool=True)\ntest_eq(model.to(xb.device)(xb).shape, (bs, c_out))\nmodel = XCMPlus(c_in, c_out, seq_len)\ntest_eq(model.to(xb.device)(xb).shape, (bs, c_out))\nmodel\n\nXCMPlus(\n  (backbone): _XCMPlus_Backbone(\n    (conv2dblock): Sequential(\n      (0): Unsqueeze(dim=1)\n      (1): Conv2dSame(\n        (conv2d_same): Conv2d(1, 128, kernel_size=(1, 51), stride=(1, 1))\n      )\n      (2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (3): ReLU()\n    )\n    (conv2d1x1block): Sequential(\n      (0): Conv2d(128, 1, kernel_size=(1, 1), stride=(1, 1))\n      (1): ReLU()\n      (2): Squeeze(dim=1)\n    )\n    (conv1dblock): Sequential(\n      (0): Conv1d(24, 128, kernel_size=(51,), stride=(1,), padding=(25,))\n      (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (2): ReLU()\n    )\n    (conv1d1x1block): Sequential(\n      (0): Conv1d(128, 1, kernel_size=(1,), stride=(1,))\n      (1): ReLU()\n    )\n    (concat): Concat(dim=1)\n    (conv1d): Sequential(\n      (0): Conv1d(25, 128, kernel_size=(51,), stride=(1,), padding=(25,))\n      (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (2): ReLU()\n    )\n  )\n  (head): Sequential(\n    (0): GAP1d(\n      (gap): AdaptiveAvgPool1d(output_size=1)\n      (flatten): Reshape(bs)\n    )\n    (1): LinBnDrop(\n      (0): Linear(in_features=128, out_features=6, bias=True)\n    )\n  )\n)\n\n\n\nmodel.show_gradcam(xb, yb)\n\n\n\n\n\n\n\n\nmodel.show_gradcam(xb[0], yb[0])\n\n[W NNPACK.cpp:53] Could not initialize NNPACK! Reason: Unsupported hardware.\n\n\n\n\n\n\n\n\n\nbs = 16\nn_vars = 3\nseq_len = 12\nc_out = 1\nxb = torch.rand(bs, n_vars, seq_len)\nnew_head = partial(conv_lin_nd_head, d=(5, 2))\nnet = XCMPlus(n_vars, c_out, seq_len, custom_head=new_head)\nprint(net.to(xb.device)(xb).shape)\nnet.head\n\ntorch.Size([16, 5, 2])\n\n\ncreate_conv_lin_nd_head(\n  (0): Conv1d(128, 1, kernel_size=(1,), stride=(1,))\n  (1): Linear(in_features=12, out_features=10, bias=True)\n  (2): Transpose(-1, -2)\n  (3): Reshape(bs, 5, 2)\n)\n\n\n\nbs = 16\nn_vars = 3\nseq_len = 12\nc_out = 2\nxb = torch.rand(bs, n_vars, seq_len)\nnet = XCMPlus(n_vars, c_out, seq_len)\nchange_model_head(net, create_pool_plus_head, concat_pool=False)\nprint(net.to(xb.device)(xb).shape)\nnet.head\n\ntorch.Size([16, 2])\n\n\nSequential(\n  (0): AdaptiveAvgPool1d(output_size=1)\n  (1): Reshape(bs)\n  (2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (3): Linear(in_features=128, out_features=512, bias=False)\n  (4): ReLU(inplace=True)\n  (5): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (6): Linear(in_features=512, out_features=2, bias=False)\n)"
  },
  {
    "objectID": "models.tabmodel.html",
    "href": "models.tabmodel.html",
    "title": "TabModel",
    "section": "",
    "text": "This is an implementation created by Ignacio Oguiza (oguiza@timeseriesAI.co) based on fastai’s TabularModel.\nWe built it so that it’s easy to change the head of the model, something that is particularly interesting when building hybrid models.\n\nsource\n\nTabHead\n\n TabHead (emb_szs, n_cont, c_out, layers=None, fc_dropout=None,\n          y_range=None, use_bn=True, bn_final=False, lin_first=False,\n          act=ReLU(inplace=True), skip=False)\n\nBasic head for tabular data.\n\nsource\n\n\nTabBackbone\n\n TabBackbone (emb_szs, n_cont, embed_p=0.0, bn_cont=True)\n\nSame as nn.Module, but no need for subclasses to call super().__init__\n\nsource\n\n\nTabModel\n\n TabModel (emb_szs, n_cont, c_out, layers=None, fc_dropout=None,\n           embed_p=0.0, y_range=None, use_bn=True, bn_final=False,\n           bn_cont=True, lin_first=False, act=ReLU(inplace=True),\n           skip=False)\n\nBasic model for tabular data.\n\nfrom fastai.tabular.core import *\nfrom tsai.data.tabular import *\n\n\npath = untar_data(URLs.ADULT_SAMPLE)\ndf = pd.read_csv(path/'adult.csv')\n# df['salary'] = np.random.rand(len(df)) # uncomment to simulate a cont dependent variable\nprocs = [Categorify, FillMissing, Normalize]\ncat_names = ['workclass', 'education', 'marital-status', 'occupation', 'relationship', 'race']\ncont_names = ['age', 'fnlwgt', 'education-num']\ny_names = ['salary']\ny_block = RegressionBlock() if isinstance(df['salary'].values[0], float) else CategoryBlock()\nsplits = RandomSplitter()(range_of(df))\npd.options.mode.chained_assignment=None\nto = TabularPandas(df, procs=procs, cat_names=cat_names, cont_names=cont_names, y_names=y_names, y_block=y_block, splits=splits, inplace=True, \n                   reduce_memory=False)\nto.show(5)\ntab_dls = to.dataloaders(bs=16, val_bs=32)\nb = first(tab_dls.train)\ntest_eq((b[0].shape, b[1].shape, b[2].shape), (torch.Size([16, 7]), torch.Size([16, 3]), torch.Size([16, 1])))\n\n\n\n  \n    \n      \n      workclass\n      education\n      marital-status\n      occupation\n      relationship\n      race\n      education-num_na\n      age\n      fnlwgt\n      education-num\n      salary\n    \n  \n  \n    \n      20505\n      Private\n      HS-grad\n      Married-civ-spouse\n      Sales\n      Husband\n      White\n      False\n      47.0\n      197836.0\n      9.0\n      <50k\n    \n    \n      28679\n      Private\n      HS-grad\n      Married-civ-spouse\n      Craft-repair\n      Husband\n      White\n      False\n      28.0\n      65078.0\n      9.0\n      >=50k\n    \n    \n      11669\n      Private\n      HS-grad\n      Never-married\n      Adm-clerical\n      Not-in-family\n      White\n      False\n      38.0\n      202683.0\n      9.0\n      <50k\n    \n    \n      29079\n      Self-emp-not-inc\n      Bachelors\n      Married-civ-spouse\n      Prof-specialty\n      Husband\n      White\n      False\n      41.0\n      168098.0\n      13.0\n      <50k\n    \n    \n      7061\n      Private\n      HS-grad\n      Married-civ-spouse\n      Adm-clerical\n      Husband\n      White\n      False\n      31.0\n      243442.0\n      9.0\n      <50k\n    \n  \n\n\n\n\ntab_model = build_tabular_model(TabModel, dls=tab_dls)\nb = first(tab_dls.train)\ntest_eq(tab_model.to(b[0].device)(*b[:-1]).shape, (tab_dls.bs, tab_dls.c))\nlearn = Learner(tab_dls, tab_model, splitter=ts_splitter)\np1 = count_parameters(learn.model)\nlearn.freeze()\np2 = count_parameters(learn.model)\nlearn.unfreeze()\np3 = count_parameters(learn.model)\nassert p1 == p3\nassert p1 > p2 > 0"
  },
  {
    "objectID": "callback.predictiondynamics.html",
    "href": "callback.predictiondynamics.html",
    "title": "PredictionDynamics",
    "section": "",
    "text": "Callback used to visualize model predictions during training.\n\nThis is an implementation created by Ignacio Oguiza (oguiza@timeseriesAI.co) based on a blog post by Andrej Karpathy I read some time ago that I really liked. One of the things he mentioned was this:\n\n“visualize prediction dynamics. I like to visualize model predictions on a fixed test batch during the course of training. The “dynamics” of how these predictions move will give you incredibly good intuition for how the training progresses. Many times it is possible to feel the network “struggle” to fit your data if it wiggles too much in some way, revealing instabilities. Very low or very high learning rates are also easily noticeable in the amount of jitter.” A. Karpathy\n\n\nsource\n\nPredictionDynamics\n\n PredictionDynamics (show_perc=1.0, figsize=(10, 6), alpha=0.3, size=30,\n                     color='lime', cmap='gist_rainbow', normalize=False,\n                     sensitivity=None, specificity=None)\n\nBasic class handling tweaks of the training loop by changing a Learner in various events\n\nfrom tsai.basics import *\nfrom tsai.models.InceptionTime import *\n\n\ndsid = 'NATOPS'\nX, y, splits = get_UCR_data(dsid, split_data=False)\ncheck_data(X, y, splits, False)\n\nX      - shape: [360 samples x 24 features x 51 timesteps]  type: memmap  dtype:float32  isnan: 0\ny      - shape: (360,)  type: memmap  dtype:<U3  n_classes: 6 (60 samples per class) ['1.0', '2.0', '3.0', '4.0', '5.0', '6.0']  isnan: False\nsplits - n_splits: 2 shape: [180, 180]  overlap: False\n\n\n\ntfms  = [None, [Categorize()]]\nbatch_tfms = [TSStandardize(by_var=True)]\ndls = get_ts_dls(X, y, splits=splits, tfms=tfms, batch_tfms=batch_tfms)\nlearn = ts_learner(dls, InceptionTime, metrics=accuracy, cbs=PredictionDynamics()) \nlearn.fit_one_cycle(2, 3e-3)\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      accuracy\n      time\n    \n  \n  \n    \n      0\n      1.885462\n      1.773872\n      0.238889\n      00:05\n    \n    \n      1\n      1.425667\n      1.640418\n      0.377778\n      00:05\n    \n  \n\n\n\n\n\n\n\n  \n    \n      \n      train_loss\n      valid_loss\n      accuracy\n    \n  \n  \n    \n      1\n      1.425667\n      1.640418\n      0.377778"
  },
  {
    "objectID": "utils.html",
    "href": "utils.html",
    "title": "Utilities",
    "section": "",
    "text": "General helper functions used throughout the library\n\n\nsource\n\nrandom_rand\n\n random_rand (*d, dtype=None, out=None, seed=None)\n\nSame as np.random.rand but with a faster random generator, dtype and seed\n\nsource\n\n\nrandom_randint\n\n random_randint (low, high=None, size=None, dtype=<class 'int'>,\n                 endpoint=False, seed=None)\n\nSame as np.random.randint but with a faster random generator and seed\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nlow\n\n\nint, lower endpoint of interval (inclusive)\n\n\nhigh\nNoneType\nNone\nint, upper endpoint of interval (exclusive), or None for a single-argument form of low.\n\n\nsize\nNoneType\nNone\nint or tuple of ints, optional. Output shape.\n\n\ndtype\ntype\nint\ndata type of the output.\n\n\nendpoint\nbool\nFalse\nbool, optional. If True, high is an inclusive endpoint. If False, the range is open on the right.\n\n\nseed\nNoneType\nNone\nint or None, optional. Seed for the random number generator.\n\n\n\n\nsource\n\n\nrandom_choice\n\n random_choice (a, size=None, replace=True, p=None, axis=0, shuffle=True,\n                dtype=None, seed=None)\n\nSame as np.random.choice but with a faster random generator, dtype and seed\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\na\n\n\n1-D array-like or int. The values from which to draw the samples.\n\n\nsize\nNoneType\nNone\nint or tuple of ints, optional. The shape of the output.\n\n\nreplace\nbool\nTrue\nbool, optional. Whether or not to allow the same value to be drawn multiple times.\n\n\np\nNoneType\nNone\n1-D array-like, optional. The probabilities associated with each entry in a.\n\n\naxis\nint\n0\nint, optional. The axis along which the samples are drawn.\n\n\nshuffle\nbool\nTrue\nbool, optional. Whether or not to shuffle the samples before returning them.\n\n\ndtype\nNoneType\nNone\ndata type of the output.\n\n\nseed\nNoneType\nNone\nint or None, optional. Seed for the random number generator.\n\n\n\n\na = random_choice(10, size=(2,3,4), replace=True, p=None, seed=1)\nb = random_choice(10, size=(2,3,4), replace=True, p=None, seed=1)\ntest_eq(a, b)\nc = random_choice(10, size=(2,3,4), replace=True, p=None, seed=2)\ntest_ne(a, c)\n\nassert random_choice(10, size=3, replace=True, p=None).shape == (3,)\nassert random_choice(10, size=(2,3,4), replace=True, p=None).shape == (2,3,4)\n\nprint(random_choice(10, size=3, replace=True, p=None))\nprint(random_choice(10, size=3, replace=False, p=None))\na = [2, 5, 4, 9, 13, 25, 56, 83, 99, 100]\nprint(random_choice(a, size=3, replace=False, p=None))\n\n[1 0 0]\n[1 9 8]\n[83  2  5]\n\n\n\na = random_randint(10, 20, 100, seed=1)\nb = random_randint(10, 20, 100, seed=1)\ntest_eq(a, b)\nc = random_randint(10, 20, 100, seed=2)\ntest_ne(a, c)\nassert (a >= 10).all() and (a < 20).all()\n\n\na = random_rand(2, 3, 4, seed=123)\nb = random_rand(2, 3, 4, seed=123)\ntest_eq(a, b)\nc = random_rand(2, 3, 4, seed=124)\ntest_ne(a, c)\nassert (a >= 0).all() and (a < 1).all()\n\na = random_rand(2, 3, 4)\na_copy = a.copy()\nrandom_rand(2, 3, 4, out=a)\ntest_ne(a, a_copy)\n\n\nsource\n\n\nis_slice\n\n is_slice (o)\n\n\nsource\n\n\nis_memmap\n\n is_memmap (o)\n\n\nsource\n\n\nis_dask\n\n is_dask (o)\n\n\nsource\n\n\nis_zarr\n\n is_zarr (o)\n\n\nsource\n\n\nis_tensor\n\n is_tensor (o)\n\n\nsource\n\n\nis_nparray\n\n is_nparray (o)\n\n\n# ensure these folders exist for testing purposes\nfns = ['data', 'export', 'models']\nfor fn in fns: \n    path = Path('.')/fn\n    if not os.path.exists(path): os.makedirs(path)\n\n\nsource\n\n\ntodtype\n\n todtype (dtype)\n\n\nsource\n\n\nto3dPlusArray\n\n to3dPlusArray (o)\n\n\nsource\n\n\nto3dPlusTensor\n\n to3dPlusTensor (o)\n\n\nsource\n\n\nto2dPlusArray\n\n to2dPlusArray (o)\n\n\nsource\n\n\nto2dPlusTensor\n\n to2dPlusTensor (o)\n\n\nsource\n\n\nto3dPlus\n\n to3dPlus (o)\n\n\nsource\n\n\nto2dPlus\n\n to2dPlus (o)\n\n\nsource\n\n\nto1d\n\n to1d (o)\n\n\nsource\n\n\nto2d\n\n to2d (o)\n\n\nsource\n\n\nto3d\n\n to3d (o)\n\n\nsource\n\n\nto1darray\n\n to1darray (o)\n\n\nsource\n\n\nto2darray\n\n to2darray (o)\n\n\nsource\n\n\nto3darray\n\n to3darray (o)\n\n\nsource\n\n\nto1dtensor\n\n to1dtensor (o)\n\n\nsource\n\n\nto2dtensor\n\n to2dtensor (o)\n\n\nsource\n\n\nto3dtensor\n\n to3dtensor (o)\n\n\nsource\n\n\ntoL\n\n toL (o)\n\n\nsource\n\n\ntoarray\n\n toarray (o)\n\n\nsource\n\n\ntotensor\n\n totensor (o)\n\n\na = np.random.rand(100).astype(np.float32)\nb = torch.from_numpy(a).float()\ntest_eq(totensor(a), b)\ntest_eq(a, toarray(b))\ntest_eq(to3dtensor(a).ndim, 3)\ntest_eq(to2dtensor(a).ndim, 2)\ntest_eq(to1dtensor(a).ndim, 1)\ntest_eq(to3darray(b).ndim, 3)\ntest_eq(to2darray(b).ndim, 2)\ntest_eq(to1darray(b).ndim, 1)\n\n\ndata = np.random.rand(10, 20)\ndf = pd.DataFrame(data)\ndf['target'] = np.random.randint(0, 3, len(df))\nX = df[df.columns[:-1]]\ny = df['target']\ntest_eq(to3darray(X).shape, (10, 1, 20))\ntest_eq(toarray(y).shape, (10,))\n\n\nsource\n\n\nget_file_size\n\n get_file_size (file_path:str, return_str:bool=True, decimals:int=2)\n\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nfile_path\nstr\n\npath to file\n\n\nreturn_str\nbool\nTrue\nTrue returns size in human-readable format (KB, MB, GB, …). False in bytes.\n\n\ndecimals\nint\n2\nNumber of decimals in the output\n\n\n\n\nsource\n\n\nget_dir_size\n\n get_dir_size (dir_path:str, return_str:bool=True, decimals:int=2,\n               verbose:bool=False)\n\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ndir_path\nstr\n\npath to directory\n\n\nreturn_str\nbool\nTrue\nTrue returns size in human-readable format (KB, MB, GB, …). False in bytes.\n\n\ndecimals\nint\n2\nNumber of decimals in the output\n\n\nverbose\nbool\nFalse\nControls verbosity\n\n\n\n\nsource\n\n\nget_size\n\n get_size (o, return_str=False, decimals=2)\n\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\no\n\n\nAny python object\n\n\nreturn_str\nbool\nFalse\nTrue returns size in human-readable format (KB, MB, GB, …). False in bytes.\n\n\ndecimals\nint\n2\nNumber of decimals in the output\n\n\n\n\nsource\n\n\nbytes2str\n\n bytes2str (size_bytes:int, decimals=2)\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nsize_bytes\nint\n\nNumber of bytes\n\n\ndecimals\nint\n2\nNumber of decimals in the output\n\n\nReturns\nstr\n\n\n\n\n\n\na = np.random.rand(10, 5, 3)\ntest_eq(get_size(a, True, 1), '1.2 KB')\n\n\nsource\n\n\nis_np_view\n\n is_np_view (o)\n\n\n\n\n\nDetails\n\n\n\n\no\na numpy array\n\n\n\n\na = np.array([1., 2., 3.])\ntest_eq(is_np_view(a), False)\ntest_eq(is_np_view(a[1:]), True)\n\n\nsource\n\n\nis_dir\n\n is_dir (path)\n\n\nsource\n\n\nis_file\n\n is_file (path)\n\n\ntest_eq(is_file(\"002_utils.ipynb\"), True)\ntest_eq(is_file(\"utils.ipynb\"), False)\n\n\nsource\n\n\ndelete_all_in_dir\n\n delete_all_in_dir (tgt_dir, exception=None)\n\n\nsource\n\n\nreverse_dict\n\n reverse_dict (dictionary)\n\n\nsource\n\n\nis_tuple\n\n is_tuple (o)\n\n\nsource\n\n\nitemify\n\n itemify (*o, tup_id=None)\n\n\na = [1, 2, 3]\nb = [4, 5, 6]\nprint(itemify(a, b))\ntest_eq(len(itemify(a, b)), len(a))\na = [1, 2, 3]\nb = None\nprint(itemify(a, b))\ntest_eq(len(itemify(a, b)), len(a))\na = [1, 2, 3]\nb = [4, 5, 6]\nc = None\nprint(itemify(a, b, c))\ntest_eq(len(itemify(a, b, c)), len(a))\n\n[(1, 4), (2, 5), (3, 6)]\n[(1,), (2,), (3,)]\n[(1, 4), (2, 5), (3, 6)]\n\n\n\nsource\n\n\nifelse\n\n ifelse (a, b, c)\n\nb if a is True else c\n\nsource\n\n\nexists\n\n exists (o)\n\n\nsource\n\n\nisnone\n\n isnone (o)\n\n\na = np.array(3)\ntest_eq(isnone(a), False)\ntest_eq(exists(a), True)\nb = None\ntest_eq(isnone(b), True)\ntest_eq(exists(b), False)\n\n\nsource\n\n\ntest_eq_nan\n\n test_eq_nan (a, b)\n\ntest that a==b excluding nan values (valid for torch.Tensor and np.ndarray)\n\nsource\n\n\ntest_error\n\n test_error (error, f, *args, **kwargs)\n\n\nsource\n\n\ntest_not_ok\n\n test_not_ok (f, *args, **kwargs)\n\n\nsource\n\n\ntest_ok\n\n test_ok (f, *args, **kwargs)\n\n\nsource\n\n\ntest_type\n\n test_type (a, b)\n\n\nsource\n\n\ntest_not_close\n\n test_not_close (a, b, eps=1e-05)\n\ntest that a is within eps of b\n\nsource\n\n\nis_not_close\n\n is_not_close (a, b, eps=1e-05)\n\nIs a within eps of b\n\nsource\n\n\nassert_fn\n\n assert_fn (*args, **kwargs)\n\n\nsource\n\n\ntest_le\n\n test_le (a, b)\n\ntest that a>b\n\nsource\n\n\ntest_lt\n\n test_lt (a, b)\n\ntest that a>b\n\nsource\n\n\ntest_ge\n\n test_ge (a, b)\n\ntest that a>=b\n\nsource\n\n\ntest_gt\n\n test_gt (a, b)\n\ntest that a>b\n\ntest_ok(test_gt, 5, 4)\ntest_not_ok(test_gt, 4, 4)\ntest_ok(test_ge, 4, 4)\ntest_not_ok(test_ge, 3, 4)\n\ntest_ok(test_lt, 3, 4)\ntest_not_ok(test_lt, 4, 4)\ntest_ok(test_le, 4, 4)\ntest_not_ok(test_le, 5, 4)\n\n\nt = torch.rand(100)\nt[t<.5] = np.nan\ntest_ne(t, t)\ntest_eq_nan(t, t)\n\n\nsource\n\n\nstack_pad\n\n stack_pad (o, padding_value=nan)\n\nConverts a an iterable into a numpy array using padding if necessary\n\nsource\n\n\nstack\n\n stack (o, axis=0, retain=True)\n\n\no = [[0,1,2], [4,5,6,7]]\ntest_eq(stack_pad(o).shape, (1, 2, 4))\ntest_eq(type(stack_pad(o)), np.ndarray)\ntest_eq(np.isnan(stack_pad(o)).sum(), 1)\n\n/Users/nacho/opt/anaconda3/envs/py37torch113/lib/python3.7/site-packages/ipykernel_launcher.py:13: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n  del sys.path[0]\n\n\n\no = 3\nprint(stack_pad(o))\ntest_eq(stack_pad(o), np.array([[3.]]))\no = [4,5]\nprint(stack_pad(o))\ntest_eq(stack_pad(o), np.array([[4., 5.]]))\no = [[0,1,2], [4,5,6,7]]\nprint(stack_pad(o))\no = np.array([0, [1,2]], dtype=object)\nprint(stack_pad(o))\no = np.array([[[0], [10, 20], [100, 200, 300]], [[0, 1, 2, 3], [10, 20], [100]]], dtype=object)\nprint(stack_pad(o))\no = np.array([0, [10, 20]], dtype=object)\nprint(stack_pad(o))\n\n[[3.]]\n[[4. 5.]]\n[[[ 0.  1.  2. nan]\n  [ 4.  5.  6.  7.]]]\n[[ 0. nan]\n [ 1.  2.]]\n[[[  0.  nan  nan  nan]\n  [ 10.  20.  nan  nan]\n  [100. 200. 300.  nan]]\n\n [[  0.   1.   2.   3.]\n  [ 10.  20.  nan  nan]\n  [100.  nan  nan  nan]]]\n[[ 0. nan]\n [10. 20.]]\n\n\n/Users/nacho/opt/anaconda3/envs/py37torch113/lib/python3.7/site-packages/ipykernel_launcher.py:13: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n  del sys.path[0]\n\n\n\na = np.random.rand(2, 3, 4)\nt = torch.from_numpy(a)\ntest_eq_type(stack(itemify(a, tup_id=0)), a)\ntest_eq_type(stack(itemify(t, tup_id=0)), t)\n\n\nsource\n\n\npad_sequences\n\n pad_sequences (o, maxlen:int=None,\n                dtype:(<class'str'>,<class'type'>)=<class\n                'numpy.float64'>, padding:str='pre', truncating:str='pre',\n                padding_value:float=nan)\n\nTransforms an iterable with sequences into a 3d numpy array using padding or truncating sequences if necessary\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\no\n\n\nIterable object\n\n\nmaxlen\nint\nNone\nOptional max length of the output. If None, max length of the longest individual sequence.\n\n\ndtype\n(<class ‘str’>, <class ‘type’>)\nfloat64\nType of the output sequences. To pad sequences with variable length strings, you can use object.\n\n\npadding\nstr\npre\n‘pre’ or ‘post’ pad either before or after each sequence.\n\n\ntruncating\nstr\npre\n‘pre’ or ‘post’ remove values from sequences larger than maxlen, either at the beginning or at the end of the sequences.\n\n\npadding_value\nfloat\nnan\nValue used for padding.\n\n\n\nThis function transforms a list (of length n_samples) of sequences into a 3d numpy array of shape:\n                          [n_samples x n_vars x seq_len]\nseq_len is either the maxlen argument if provided, or the length of the longest sequence in the list.\nSequences that are shorter than seq_len are padded with value until they are seq_len long.\nSequences longer than seq_len are truncated so that they fit the desired length.\nThe position where padding or truncation happens is determined by the arguments padding and truncating, respectively. Pre-padding or removing values from the beginning of the sequence is the default.\nInput sequences to pad_sequences may be have 1, 2 or 3 dimensions:\n\n# 1 dim\na1 = np.arange(6)\na2 = np.arange(3) * 10\na3 = np.arange(2) * 100\no  = [a1, a2, a3]\npadded_o = pad_sequences(o, maxlen=4, dtype=np.float64, padding='post', truncating='pre', padding_value=np.nan)\ntest_eq(padded_o.shape, (3, 1, 4))\npadded_o\n\narray([[[  2.,   3.,   4.,   5.]],\n\n       [[  0.,  10.,  20.,  nan]],\n\n       [[  0., 100.,  nan,  nan]]])\n\n\n\n# 2 dim\na1 = np.arange(12).reshape(2, 6)\na2 = np.arange(6).reshape(2, 3) * 10\na3 = np.arange(4).reshape(2, 2) * 100\no  = [a1, a2, a3]\npadded_o = pad_sequences(o, maxlen=4, dtype=np.float64, padding='post', truncating='pre', padding_value=np.nan)\ntest_eq(padded_o.shape, (3, 2, 4))\npadded_o\n\narray([[[  2.,   3.,   4.,   5.],\n        [  8.,   9.,  10.,  11.]],\n\n       [[  0.,  10.,  20.,  nan],\n        [ 30.,  40.,  50.,  nan]],\n\n       [[  0., 100.,  nan,  nan],\n        [200., 300.,  nan,  nan]]])\n\n\n\n# 3 dim\na1 = np.arange(10).reshape(1, 2, 5)\na2 = np.arange(6).reshape(1, 2, 3) * 10\na3 = np.arange(4).reshape(1, 2, 2) * 100\no  = [a1, a2, a3]\npadded_o = pad_sequences(o, maxlen=None, dtype=np.float64, padding='pre', truncating='pre', padding_value=np.nan)\ntest_eq(padded_o.shape, (3, 2, 5))\npadded_o\n\narray([[[  0.,   1.,   2.,   3.,   4.],\n        [  5.,   6.,   7.,   8.,   9.]],\n\n       [[ nan,  nan,   0.,  10.,  20.],\n        [ nan,  nan,  30.,  40.,  50.]],\n\n       [[ nan,  nan,  nan,   0., 100.],\n        [ nan,  nan,  nan, 200., 300.]]])\n\n\n\n# 3 dim\na1 = np.arange(10).reshape(1, 2, 5)\na2 = np.arange(6).reshape(1, 2, 3) * 10\na3 = np.arange(4).reshape(1, 2, 2) * 100\no  = [a1, a2, a3]\npadded_o = pad_sequences(o, maxlen=4, dtype=np.float64, padding='pre', truncating='pre', padding_value=np.nan)\ntest_eq(padded_o.shape, (3, 2, 4))\npadded_o\n\narray([[[  1.,   2.,   3.,   4.],\n        [  6.,   7.,   8.,   9.]],\n\n       [[ nan,   0.,  10.,  20.],\n        [ nan,  30.,  40.,  50.]],\n\n       [[ nan,  nan,   0., 100.],\n        [ nan,  nan, 200., 300.]]])\n\n\n\n# 3 dim\na1 = np.arange(10).reshape(1, 2, 5)\na2 = np.arange(6).reshape(1, 2, 3) * 10\na3 = np.arange(4).reshape(1, 2, 2) * 100\no  = [a1, a2, a3]\npadded_o = pad_sequences(o, maxlen=4, dtype=np.float64, padding='post', truncating='pre', padding_value=np.nan)\ntest_eq(padded_o.shape, (3, 2, 4))\npadded_o\n\narray([[[  1.,   2.,   3.,   4.],\n        [  6.,   7.,   8.,   9.]],\n\n       [[  0.,  10.,  20.,  nan],\n        [ 30.,  40.,  50.,  nan]],\n\n       [[  0., 100.,  nan,  nan],\n        [200., 300.,  nan,  nan]]])\n\n\n\n# 3 dim\na1 = np.arange(10).reshape(1, 2, 5)\na2 = np.arange(6).reshape(1, 2, 3) * 10\na3 = np.arange(4).reshape(1, 2, 2) * 100\no  = [a1, a2, a3]\npadded_o = pad_sequences(o, maxlen=4, dtype=np.float64, padding='post', truncating='post', padding_value=np.nan)\ntest_eq(padded_o.shape, (3, 2, 4))\npadded_o\n\narray([[[  0.,   1.,   2.,   3.],\n        [  5.,   6.,   7.,   8.]],\n\n       [[  0.,  10.,  20.,  nan],\n        [ 30.,  40.,  50.,  nan]],\n\n       [[  0., 100.,  nan,  nan],\n        [200., 300.,  nan,  nan]]])\n\n\n\n# iterable is a list of lists\na1 = np.arange(12).reshape(1, 2, 6).tolist()\na2 = (np.arange(6).reshape(1, 2, 3) * 10).tolist()\na3 = (np.arange(4).reshape(1, 2, 2) * 100).tolist()\no  = [a1, a2, a3]\npadded_o = pad_sequences(o, maxlen=None, dtype=np.float64, padding='post', truncating='pre', padding_value=np.nan)\ntest_eq(padded_o.shape, (3, 2, 6))\npadded_o\n\narray([[[  0.,   1.,   2.,   3.,   4.,   5.],\n        [  6.,   7.,   8.,   9.,  10.,  11.]],\n\n       [[  0.,  10.,  20.,  nan,  nan,  nan],\n        [ 30.,  40.,  50.,  nan,  nan,  nan]],\n\n       [[  0., 100.,  nan,  nan,  nan,  nan],\n        [200., 300.,  nan,  nan,  nan,  nan]]])\n\n\n\nsource\n\n\nmatch_seq_len\n\n match_seq_len (*arrays)\n\n\na = np.random.rand(10, 5, 8)\nb = np.random.rand(3, 5, 10)\nc, d = match_seq_len(a, b)\ntest_eq(c.shape[-1], d.shape[-1])\n\n\nsource\n\n\nrandom_shuffle\n\n random_shuffle (o, random_state=None)\n\n\na = np.arange(10)\ntest_eq_type(random_shuffle(a, 1), np.array([2, 9, 6, 4, 0, 3, 1, 7, 8, 5]))\nt = torch.arange(10)\ntest_eq_type(random_shuffle(t, 1), tensor([2, 9, 6, 4, 0, 3, 1, 7, 8, 5]))\nl = list(a)\ntest_eq(random_shuffle(l, 1), [2, 9, 6, 4, 0, 3, 1, 7, 8, 5])\nl2 = L(l)\ntest_eq_type(random_shuffle(l2, 1), L([2, 9, 6, 4, 0, 3, 1, 7, 8, 5]))\n\n\nsource\n\n\ncat2int\n\n cat2int (o)\n\n\na = np.array(['b', 'a', 'a', 'b', 'a', 'b', 'a'])\ntest_eq_type(cat2int(a), TensorCategory([1, 0, 0, 1, 0, 1, 0]))\n\n\nTensorBase([1,2,3])\n\nTensorBase([1, 2, 3])\n\n\n\nsource\n\n\ncycle_dl_estimate\n\n cycle_dl_estimate (dl, iters=10)\n\n\nsource\n\n\ncycle_dl_to_device\n\n cycle_dl_to_device (dl, show_progress_bar=True)\n\n\nsource\n\n\ncycle_dl\n\n cycle_dl (dl, show_progress_bar=True)\n\n\nsource\n\n\ncache_data\n\n cache_data (o, slice_len=10000, verbose=False)\n\n\nsource\n\n\nget_func_defaults\n\n get_func_defaults (f)\n\n\nsource\n\n\nget_idx_from_df_col_vals\n\n get_idx_from_df_col_vals (df, col, val_list)\n\n\nsource\n\n\nget_sublist_idxs\n\n get_sublist_idxs (aList, bList)\n\nGet idxs that when applied to aList will return bList. aList must contain all values in bList\n\nx = np.array([3, 5, 7, 1, 9, 8, 6, 2])\ny = np.array([6, 1, 5, 7])\nidx = get_sublist_idxs(x, y)\ntest_eq(x[idx], y)\nx = np.array([3, 5, 7, 1, 9, 8, 6, 6, 2])\ny = np.array([6, 1, 5, 7, 5])\nidx = get_sublist_idxs(x, y)\ntest_eq(x[idx], y)\n\n\nsource\n\n\nflatten_list\n\n flatten_list (l)\n\n\nsource\n\n\ndisplay_pd_df\n\n display_pd_df (df, max_rows:Union[bool,int]=False,\n                max_columns:Union[bool,int]=False)\n\n\nold_max_rows, old_max_columns = pd.get_option('display.max_rows'), pd.get_option('display.max_columns')\ndf = pd.DataFrame(np.random.rand(70, 25))\ndisplay_pd_df(df, max_rows=2, max_columns=3)\ntest_eq(old_max_rows, pd.get_option('display.max_rows'))\ntest_eq(old_max_columns, pd.get_option('display.max_columns'))\n\n\n\n\n\n  \n    \n      \n      0\n      ...\n      24\n    \n  \n  \n    \n      0\n      0.137592\n      ...\n      0.466512\n    \n    \n      ...\n      ...\n      ...\n      ...\n    \n    \n      69\n      0.068388\n      ...\n      0.702192\n    \n  \n\n70 rows × 25 columns\n\n\n\n\nsource\n\n\ntscore\n\n tscore (o)\n\n\nsource\n\n\nkstest\n\n kstest (data1, data2, alternative='two-sided', mode='auto', by_axis=None)\n\nPerforms the two-sample Kolmogorov-Smirnov test for goodness of fit.\nParameters data1, data2: Two arrays of sample observations assumed to be drawn from a continuous distributions. Sample sizes can be different. alternative: {‘two-sided’, ‘less’, ‘greater’}, optional. Defines the null and alternative hypotheses. Default is ‘two-sided’. mode: {‘auto’, ‘exact’, ‘asymp’}, optional. Defines the method used for calculating the p-value. by_axis (optional, int): for arrays with more than 1 dimension, the test will be run for each variable in that axis if by_axis is not None.\n\nsource\n\n\nttest\n\n ttest (data1, data2, equal_var=False)\n\nCalculates t-statistic and p-value based on 2 sample distributions\n\na = np.random.normal(0.5, 1, 100)\nb = np.random.normal(0.15, .5, 50)\nplt.hist(a, 50)\nplt.hist(b, 50)\nplt.show()\nttest(a,b)\n\n\n\n\n(2.4597239909532593, 0.015059901570983684)\n\n\n\na = np.random.normal(0.5, 1, (100,3))\nb = np.random.normal(0.5, 1, (50,))\nkstest(a,b)\n\n(0.2, 0.0584616647639814)\n\n\n\na = np.random.normal(0.5, 1, (100,3))\nb = np.random.normal(0.15, .5, (50,))\nkstest(a,b)\n\n(0.31, 0.0004061333917841914)\n\n\n\ndata1 = np.random.normal(0,1,(100, 5, 3))\ndata2 = np.random.normal(0,2,(100, 5, 3))\nkstest(data1, data2, by_axis=1)\n\n([0.21666666666666667,\n  0.25333333333333335,\n  0.18333333333333332,\n  0.20666666666666667,\n  0.17666666666666667],\n [1.4007759411179028e-06,\n  7.277526651183985e-09,\n  8.024952455325493e-05,\n  5.0749555303276145e-06,\n  0.00016597474985360523])\n\n\n\na = np.random.normal(0.5, 1, 100)\nt = torch.normal(0.5, 1, (100, ))\ntscore(a), tscore(t)\n\n(4.659080669220386, tensor(4.8233))\n\n\n\nsource\n\n\nscc\n\n scc (a, b)\n\n\nsource\n\n\npcc\n\n pcc (a, b)\n\n\nsource\n\n\nremove_fn\n\n remove_fn (fn, verbose=False)\n\nRemoves a file (fn) if exists\n\nsource\n\n\nnpsave\n\n npsave (array_fn, array, verbose=True)\n\n\nfn = 'data/remove_fn_test.npy'\na = np.zeros(1)\nnpsave(fn, a)\ndel a\nnp.load(fn, mmap_mode='r+')\nremove_fn(fn, True)\nremove_fn(fn, True)\n\ndata/remove_fn_test.npy does not exist\nsaving data/remove_fn_test.npy...\n...data/remove_fn_test.npy saved\ndata/remove_fn_test.npy file removed\ndata/remove_fn_test.npy does not exist\n\n\n\nsource\n\n\npermute_2D\n\n permute_2D (array, axis=None)\n\nPermute rows or columns in an array. This can be used, for example, in feature permutation\n\ns = np.arange(100 * 50).reshape(100, 50) \ntest_eq(permute_2D(s, axis=0).mean(0), s.mean(0))\ntest_ne(permute_2D(s, axis=0), s)\ntest_eq(permute_2D(s, axis=1).mean(1), s.mean(1))\ntest_ne(permute_2D(s, axis=1), s)\ntest_ne(permute_2D(s), s)\n\n\nsource\n\n\nrandom_half_normal_tensor\n\n random_half_normal_tensor (shape=1, device=None)\n\nReturns a tensor of a predefined shape between 0 and 1 with a half-normal distribution\n\nsource\n\n\nrandom_normal_tensor\n\n random_normal_tensor (shape=1, device=None)\n\nReturns a tensor of a predefined shape between -1 and 1 with a normal distribution\n\nsource\n\n\nrandom_half_normal\n\n random_half_normal ()\n\nReturns a number between 0 and 1 with a half-normal distribution\n\nsource\n\n\nrandom_normal\n\n random_normal ()\n\nReturns a number between -1 and 1 with a normal distribution\n\nsource\n\n\nfig2buf\n\n fig2buf (fig)\n\n\nsource\n\n\nget_plot_fig\n\n get_plot_fig (size=None, dpi=100)\n\n\nsource\n\n\ndefault_dpi\n\n default_dpi ()\n\n\ndefault_dpi()\n\n100\n\n\n\nsource\n\n\nplot_scatter\n\n plot_scatter (x, y, deg=1)\n\n\na = np.random.rand(100)\nb = np.random.rand(100)**2\nplot_scatter(a, b)\n\n\n\n\n\nsource\n\n\nget_idxs\n\n get_idxs (o, aList)\n\n\na = random_shuffle(np.arange(100, 200))\nb = np.random.choice(a, 10, False)\nidxs = get_idxs(a, b)\ntest_eq(a[idxs], b)\n\n\nsource\n\n\napply_cmap\n\n apply_cmap (o, cmap)\n\n\na = np.random.rand(16, 1, 40, 50)\ns = L(a.shape)\ns[1] = 3\ntest_eq(L(apply_cmap(a, 'viridis').shape), s)\n\ns[0] = 1\na = np.random.rand(1, 40, 50)\ntest_eq(L(apply_cmap(a, 'viridis').shape), s)\n\n\nsource\n\n\ntorch_tile\n\n torch_tile (a, n_tile, dim=0)\n\n\ntest_eq(torch_tile(torch.arange(2), 3), tensor([0, 1, 0, 1, 0, 1]))\n\n\nsource\n\n\nto_tsfresh_df\n\n to_tsfresh_df (ts)\n\nPrepares a time series (Tensor/ np.ndarray) to be used as a tsfresh dataset to allow feature extraction\n\nts = torch.rand(16, 3, 20)\na = to_tsfresh_df(ts)\nts = ts.numpy()\nb = to_tsfresh_df(ts)\n\n\nsource\n\n\nscorr\n\n scorr (a, b)\n\n\nsource\n\n\npcorr\n\n pcorr (a, b)\n\n\nsource\n\n\ntorch_diff\n\n torch_diff (t, lag=1, pad=True, append=0)\n\n\nt = torch.arange(24).reshape(2,3,4)\ntest_eq(torch_diff(t, 1)[..., 1:].float().mean(), 1.)\ntest_eq(torch_diff(t, 2)[..., 2:].float().mean(), 2.)\n\n\nsource\n\n\ntorch_clamp\n\n torch_clamp (o, min=None, max=None)\n\nClamp torch.Tensor using 1 or multiple dimensions\n\nsource\n\n\nget_percentile\n\n get_percentile (o, percentile, axis=None)\n\n\nsource\n\n\nclip_outliers\n\n clip_outliers (o, axis=None)\n\n\nsource\n\n\nget_outliers_IQR\n\n get_outliers_IQR (o, axis=None, quantile_range=(25.0, 75.0))\n\n\nt = torch.randn(2,3,100)\ntest_eq(type(get_outliers_IQR(t, -1)[0]), torch.Tensor)\na = t.numpy()\ntest_eq(type(get_outliers_IQR(a, -1)[0]), np.ndarray)\ntest_close(get_percentile(t, 25).numpy(), get_percentile(a, 25))\n\n\nsource\n\n\nget_robustscale_params\n\n get_robustscale_params (o, sel_vars=None, not_sel_vars=None, by_var=True,\n                         percentiles=(25, 75), eps=1e-06)\n\nCalculates median and inter-quartile range required to robust scaler inputs\n\na = np.random.rand(16, 3, 100)\na[a>.8] = np.nan\nmedian, IQR = get_robustscale_params(a, by_var=True, percentiles=(25, 75))\na_scaled = (a - median) / IQR\ntest_eq(a.shape, a_scaled.shape)\ntest_eq(np.isnan(median).sum(),0)\ntest_eq(np.isnan(IQR).sum(),0)\ntest_eq(np.isnan(a), np.isnan(a_scaled))\n\n\nsource\n\n\ntorch_slice_by_dim\n\n torch_slice_by_dim (t, index, dim=-1, **kwargs)\n\n\nt = torch.rand(5, 3)\nindex = torch.randint(0, 3, (5, 1))\n# index = [[0, 2], [0, 1], [1, 2], [0, 2], [0, 1]]\ntorch_slice_by_dim(t, index)\n\ntensor([[0.0703],\n        [0.5010],\n        [0.3556],\n        [0.7908],\n        [0.2815]])\n\n\n\nsource\n\n\ntorch_nanstd\n\n torch_nanstd (o, dim=None, keepdim=False)\n\nThere’s currently no torch.nanstd function\n\nsource\n\n\ntorch_nanmean\n\n torch_nanmean (o, dim=None, keepdim=False)\n\nThere’s currently no torch.nanmean function\n\nt = torch.rand(1000)\nt[:100] = float('nan')\nassert torch_nanmean(t).item() > 0\n\n\nsource\n\n\nconcat\n\n concat (*ls, dim=0)\n\nConcatenate tensors, arrays, lists, or tuples by a dimension\n\nsource\n\n\nreduce_memory_usage\n\n reduce_memory_usage (df)\n\n\nsource\n\n\ncls_name\n\n cls_name (o)\n\n\ntest_eq(cls_name(timer), 'Timer')\n\n\nsource\n\n\nrotate_axis2\n\n rotate_axis2 (o, steps=1)\n\n\nsource\n\n\nrotate_axis1\n\n rotate_axis1 (o, steps=1)\n\n\nsource\n\n\nrotate_axis0\n\n rotate_axis0 (o, steps=1)\n\n\nsource\n\n\nrandom_roll3d\n\n random_roll3d (o, axis=(), replace=False)\n\nRandomly rolls a 3D object along the indicated axes This solution is based on https://stackoverflow.com/questions/20360675/roll-rows-of-a-matrix-independently\n\nsource\n\n\nrandom_roll2d\n\n random_roll2d (o, axis=(), replace=False)\n\nRolls a 2D object on the indicated axis This solution is based on https://stackoverflow.com/questions/20360675/roll-rows-of-a-matrix-independently\n\nsource\n\n\nroll3d\n\n roll3d (o, roll1:Union[NoneType,list,int]=None,\n         roll2:Union[NoneType,list,int]=None,\n         roll3:Union[NoneType,list,int]=None)\n\nRolls a 3D object on the indicated axis This solution is based on https://stackoverflow.com/questions/20360675/roll-rows-of-a-matrix-independently\n\nsource\n\n\nroll2d\n\n roll2d (o, roll1:Union[NoneType,list,int]=None,\n         roll2:Union[NoneType,list,int]=None)\n\nRolls a 2D object on the indicated axis This solution is based on https://stackoverflow.com/questions/20360675/roll-rows-of-a-matrix-independently\n\na = np.tile(np.arange(10), 3).reshape(3, 10) * np.array([1, 10, 100]).reshape(-1, 1)\na\n\narray([[  0,   1,   2,   3,   4,   5,   6,   7,   8,   9],\n       [  0,  10,  20,  30,  40,  50,  60,  70,  80,  90],\n       [  0, 100, 200, 300, 400, 500, 600, 700, 800, 900]])\n\n\n\nroll2d(a, roll1=[2, 1, 0])\n\narray([[  0, 100, 200, 300, 400, 500, 600, 700, 800, 900],\n       [  0,  10,  20,  30,  40,  50,  60,  70,  80,  90],\n       [  0,   1,   2,   3,   4,   5,   6,   7,   8,   9]])\n\n\n\nroll2d(a, roll2=3)\n\narray([[  7,   8,   9,   0,   1,   2,   3,   4,   5,   6],\n       [ 70,  80,  90,   0,  10,  20,  30,  40,  50,  60],\n       [700, 800, 900,   0, 100, 200, 300, 400, 500, 600]])\n\n\n\no = torch.arange(24).reshape(2,3,4)\ntest_eq(rotate_axis0(o)[1], o[0])\ntest_eq(rotate_axis1(o)[:,1], o[:,0])\ntest_eq(rotate_axis2(o)[...,1], o[...,0])\n\n\nsource\n\n\nchunks_calculator\n\n chunks_calculator (shape, dtype='float32', n_bytes=1073741824)\n\nFunction to calculate chunks for a given size of n_bytes (default = 1024**3 == 1GB). It guarantees > 50% of the chunk will be filled\n\nshape = (1_000, 10, 1000)\ndtype = 'float32'\ntest_eq(chunks_calculator(shape, dtype), False)\n\nshape = (54684, 10, 1000)\ndtype = 'float32'\ntest_eq(chunks_calculator(shape, dtype), (27342, -1, -1))\n\n\nsource\n\n\nis_memory_shared\n\n is_memory_shared (a, b)\n\nCheck if 2 array-like objects share memory\n\na = np.random.rand(2,3,4)\nt1 = torch.from_numpy(a)\ntest_eq(is_memory_shared(a, t1), True)\na = np.random.rand(2,3,4)\nt2 = torch.as_tensor(a)\ntest_eq(is_memory_shared(a, t2), True)\na = np.random.rand(2,3,4)\nt3 = torch.tensor(a)\ntest_eq(is_memory_shared(a, t3), False)\n\n\nsource\n\n\nassign_in_chunks\n\n assign_in_chunks (a, b, chunksize='auto', inplace=True, verbose=True)\n\nAssigns values in b to an array-like object a using chunks to avoid memory overload. The resulting a retains it’s dtype and share it’s memory. a: array-like object b: may be an integer, float, str, ‘rand’ (for random data), or another array like object. chunksize: is the size of chunks. If ‘auto’ chunks will have around 1GB each.\n\na = np.random.rand(10,3,4).astype('float32')\na_dtype = a.dtype\na_id = id(a)\nb = np.random.rand(10,3,4).astype('float64')\nassign_in_chunks(a, b, chunksize=2, inplace=True, verbose=True)\ntest_close(a, b)\ntest_eq(a.dtype, a_dtype)\ntest_eq(id(a), a_id)\n\na = np.random.rand(10,3,4).astype('float32')\na_dtype = a.dtype\na_id = id(a)\nb = 1\nassign_in_chunks(a, b, chunksize=2, inplace=True, verbose=True)\ntest_eq(a, np.ones_like(a).astype(a.dtype))\ntest_eq(a.dtype, a_dtype)\ntest_eq(id(a), a_id)\n\na = np.random.rand(10,3,4).astype('float32')\na_dtype = a.dtype\na_id = id(a)\nb = 0.5\nassign_in_chunks(a, b, chunksize=2, inplace=True, verbose=True)\ntest_eq(a.dtype, a_dtype)\ntest_eq(id(a), a_id)\n\na = np.random.rand(10,3,4).astype('float32')\na_dtype = a.dtype\na_id = id(a)\nb = 'rand'\nassign_in_chunks(a, b, chunksize=2, inplace=True, verbose=True)\ntest_eq(a.dtype, a_dtype)\ntest_eq(id(a), a_id)\n\n\n\n\n\n\n\n\n\n\n\n\n\na = np.random.rand(10,3,4).astype('float32')\nb = np.random.rand(10,3,4).astype('float64')\nc = assign_in_chunks(a, b, chunksize=2, inplace=False, verbose=True)\ntest_close(c, b)\ntest_eq(a.dtype, c.dtype)\ntest_eq(is_memory_shared(a, c), True)\n\na = np.random.rand(10,3,4).astype('float32')\nb = 1\nc = assign_in_chunks(a, b, chunksize=2, inplace=False, verbose=True)\ntest_eq(a, np.ones_like(a).astype(a.dtype))\ntest_eq(a.dtype, c.dtype)\ntest_eq(is_memory_shared(a, c), True)\n\na = np.random.rand(10,3,4).astype('float32')\nb = 0.5\nc = assign_in_chunks(a, b, chunksize=2, inplace=False, verbose=True)\ntest_eq(a.dtype, c.dtype)\ntest_eq(is_memory_shared(a, c), True)\n\na = np.random.rand(10,3,4).astype('float32')\nb = 'rand'\nc = assign_in_chunks(a, b, chunksize=2, inplace=False, verbose=True)\ntest_eq(a.dtype, c.dtype)\ntest_eq(is_memory_shared(a, c), True)\n\n\nsource\n\n\ncreate_array\n\n create_array (shape, fname=None, path='./data', on_disk=True,\n               dtype='float32', mode='r+', fill_value='rand',\n               chunksize='auto', verbose=True, **kwargs)\n\nmode: ‘r’: Open existing file for reading only. ‘r+’: Open existing file for reading and writing. ‘w+’: Create or overwrite existing file for reading and writing. ‘c’: Copy-on-write: assignments affect data in memory, but changes are not saved to disk. The file on disk is read-only. fill_value: ‘rand’ (for random numbers), int or float chunksize = ‘auto’ to calculate chunks of 1GB, or any integer (for a given number of samples)\n\nfname = 'X_on_disk'\nshape = (100, 10, 10)\nX = create_array(shape, fname, on_disk=True, mode='r+')\ntest_ne(abs(X).sum(), 0)\nos.remove(X.filename)\ndel X\n\n\nfname = 'X_on_disk'\nshape = (100, 10, 10)\nX = create_empty_array(shape, fname, on_disk=True, mode='r+')\ntest_eq(abs(X).sum(), 0)\n\nchunksize = 10\npbar = progress_bar(range(math.ceil(len(X) / chunksize)), leave=False)\nstart = 0\nfor i in pbar: \n    end = min(start + chunksize, len(X))\n    partial_data = np.random.rand(end - start, X.shape[1] , X.shape[2])\n    X[start:end] = partial_data\n    start = end\n    del partial_data\n    gc.collect()\nfilename = X.filename\ndel X\nX = np.load(filename, mmap_mode='r+')\ntest_eq((X == 0).sum(), 0)\ntest_eq(X.shape, shape)\nos.remove(X.filename)\ndel X\n\n\n\n\n\n\n    \n      \n      10.00% [1/10 00:00<00:05]\n    \n    \n\n\n\nsource\n\n\nnp_load_compressed\n\n np_load_compressed (fname=None, path='./data', **kwargs)\n\n\nsource\n\n\nnp_save_compressed\n\n np_save_compressed (arr, fname=None, path='./data', verbose=False,\n                     **kwargs)\n\n\nX1 = np.random.rand(10)\nnp_save_compressed(X1, 'X_comp', path='./data')\nX2 = np_load_compressed('X_comp')\ntest_eq(X1, X2)\n\n\nsource\n\n\nnp2memmap\n\n np2memmap (arr, fname=None, path='./data', dtype='float32', mode='c',\n            **kwargs)\n\nFunction that turns an ndarray into a memmap ndarray mode: ‘r’: Open existing file for reading only. ‘r+’: Open existing file for reading and writing. ‘w+’: Create or overwrite existing file for reading and writing. ‘c’: Copy-on-write: assignments affect data in memory, but changes are not saved to disk. The file on disk is read-only.\n\nX1 = np.random.rand(10)\nX2 = np2memmap(X1, 'X1_test')\ntest_eq(X1, X2)\ntest_ne(type(X1), type(X2))\n\n\nsource\n\n\ntorch_mean_groupby\n\n torch_mean_groupby (o, idxs)\n\nComputes torch mean along axis 0 grouped by the idxs. Need to ensure that idxs have the same order as o\n\no = torch.arange(6*2*3).reshape(6, 2, 3).float()\nidxs = np.array([[0,1,2,3], [2,3]], dtype=object)\noutput = torch_mean_groupby(o, idxs)\ntest_eq(o[:2], output[:2])\ntest_eq(o[2:4].mean(0), output[2])\ntest_eq(o[4:6].mean(0), output[3])\n\n\nsource\n\n\ntorch_flip\n\n torch_flip (t, dims=-1)\n\n\nt = torch.randn(2, 3, 4)\ntest_eq(torch.flip(t, (2,)), torch_flip(t, dims=-1))\n\n\nsource\n\n\ntorch_masked_to_num\n\n torch_masked_to_num (o, mask, num=0, inplace=False)\n\n\nsource\n\n\ntorch_nan_to_num\n\n torch_nan_to_num (o, num=0, inplace=False)\n\n\nx = torch.rand(2, 4, 6)\nx[:, :3][x[:, :3] < .5] = np.nan\nnan_values = torch.isnan(x).sum()\ny = torch_nan_to_num(x[:, :3], inplace=False)\ntest_eq(torch.isnan(y).sum(), 0)\ntest_eq(torch.isnan(x).sum(), nan_values)\ntorch_nan_to_num(x[:, :3], inplace=True)\ntest_eq(torch.isnan(x).sum(), 0)\n\n\nx = torch.rand(2, 4, 6)\nmask = x[:, :3] > .5\nx[:, :3] = torch_masked_to_num(x[:, :3], mask, num=0, inplace=False)\ntest_eq(x[:, :3][mask].sum(), 0)\n\n\nx = torch.rand(2, 4, 6)\nmask = x[:, :3] > .5\ntorch_masked_to_num(x[:, :3], mask, num=0, inplace=True)\ntest_eq(x[:, :3][mask].sum(), 0)\n\n\nsource\n\n\nmpl_trend\n\n mpl_trend (x, y, deg=1)\n\n\nx = np.sort(np.random.randint(0, 100, 100)/10)\ny = np.random.rand(100) + np.linspace(0, 10, 100)\ntrend = mpl_trend(x, y)\nplt.scatter(x, y)\nplt.plot(x, trend, 'r')\nplt.show()\n\n\n\n\n\nsource\n\n\narray2digits\n\n array2digits (o, n_digits=None, normalize=True)\n\n\nsource\n\n\nint2digits\n\n int2digits (o, n_digits=None, normalize=True)\n\n\no = -9645\ntest_eq(int2digits(o, 6), np.array([ 0,  0, -.9, -.6, -.4, -.5]))\n\na = np.random.randint(-1000, 1000, 10)\ntest_eq(array2digits(a,5).shape, (10,5))\n\n\nsource\n\n\nsincos_encoding\n\n sincos_encoding (seq_len, device=None, to_np=False)\n\n\nsin, cos = sincos_encoding(100)\nplt.plot(sin.cpu().numpy())\nplt.plot(cos.cpu().numpy())\nplt.show()\n\n\n\n\n\nsource\n\n\nlinear_encoding\n\n linear_encoding (seq_len, device=None, to_np=False, lin_range=(-1, 1))\n\n\nlin = linear_encoding(100)\nplt.plot(lin.cpu().numpy())\nplt.show()\n\n\n\n\n\nsource\n\n\nencode_positions\n\n encode_positions (pos_arr, min_val=None, max_val=None, linear=False,\n                   lin_range=(-1, 1))\n\nEncodes an array with positions using a linear or sincos methods\n\nn_samples = 10\nlength = 500\n_a = []\nfor i in range(n_samples):\n    a = np.arange(-4000, 4000, 10)\n    mask = np.random.rand(len(a)) > .5\n    a = a[mask]\n    a = np.concatenate([a, np.array([np.nan] * (length - len(a)))])\n    _a.append(a.reshape(-1,1))\na = np.concatenate(_a, -1).transpose(1,0)\nsin, cos = encode_positions(a, linear=False)\ntest_eq(a.shape, (n_samples, length))\ntest_eq(sin.shape, (n_samples, length))\ntest_eq(cos.shape, (n_samples, length))\nplt.plot(sin.T)\nplt.plot(cos.T)\nplt.xlim(0, 500)\nplt.show()\n\n\n\n\n\nn_samples = 10\nlength = 500\n_a = []\nfor i in range(n_samples):\n    a = np.arange(-4000, 4000, 10)\n    mask = np.random.rand(len(a)) > .5\n    a = a[mask]\n    a = np.concatenate([a, np.array([np.nan] * (length - len(a)))])\n    _a.append(a.reshape(-1,1))\na = np.concatenate(_a, -1).transpose(1,0)\nlin = encode_positions(a, linear=True)\ntest_eq(a.shape, (n_samples, length))\ntest_eq(lin.shape, (n_samples, length))\nplt.plot(lin.T)\nplt.xlim(0, 500)\nplt.show()\n\n\n\n\n\nsource\n\n\nsort_generator\n\n sort_generator (generator, bs)\n\n\ngenerator = (i for i in np.random.permutation(np.arange(1000000)).tolist())\nl = list(sort_generator(generator, 512))\ntest_eq(l[:512], sorted(l[:512]))\n\n\nsource\n\n\nget_subset_dict\n\n get_subset_dict (d, keys)\n\n\nkeys = string.ascii_lowercase\nvalues = np.arange(len(keys))\nd = {k:v for k,v in zip(keys,values)}\ntest_eq(get_subset_dict(d, ['a', 'k', 'j', 'e']), {'a': 0, 'k': 10, 'j': 9, 'e': 4})\n\n\nsource\n\n\nremove_dir\n\n remove_dir (directory, verbose=True)\n\n\nsource\n\n\ncreate_dir\n\n create_dir (directory, verbose=True)\n\n\npath = \"wandb3/wandb2/wandb\"\ncreate_dir(path)\nassert Path(path).exists()\n\npaths = [\"wandb3/wandb2/wandb\", \"wandb3/wandb2\", \"wandb\"]\nremove_dir(paths)\nfor p in paths: \n    assert not Path(p).exists()\n\npath = \"wandb3\"\nassert Path(path).exists()\nremove_dir(path)\nassert not Path(path).exists()\n\nwandb3/wandb2/wandb directory created.\nwandb3/wandb2/wandb directory removed.\nwandb3/wandb2 directory removed.\nwandb directory doesn't exist.\nwandb3 directory removed.\n\n\n\ncreate_dir('./test')\n\ntest directory created.\n\n\n\na = 5\ndef fn(b): return a + b\n\nWriting ./test/mod_dev.py\n\n\n\nfname = \"./test/mod_dev.py\"\nwhile True: \n    if fname[0] in \"/ .\": fname = fname.split(fname[0], 1)[1]\n    else: break\nif '/' in fname and fname.rsplit('/', 1)[0] not in sys.path: sys.path.append(fname.rsplit('/', 1)[0])\nmod = import_file_as_module(fname)\ntest_eq(mod.fn(3), 8)\nsys.path = sys.path[:-1]\nremove_dir('./test/')\n\ntest directory removed.\n\n\n\nsource\n\n\nnamed_partial\n\n named_partial (name, func, *args, **kwargs)\n\nCreate a partial function with a name\n\ndef add_1(x, add=1): return x+add\ntest_eq(add_1(1), 2)\nadd_2 = partial(add_1, add=2)\ntest_eq(add_2(2), 4)\ntest_ne(str(add_2), \"add_2\")\nadd_2 = named_partial('add_2', add_1, add=2)\ntest_eq(add_2(2), 4)\ntest_eq(str(add_2), \"add_2\")\n\nclass _A():\n    def __init__(self, add=1): self.add = add\n    def __call__(self, x): return x + self.add\n    \ntest_eq(_A()(1), 2)\n_A2 = partial(_A, add=2)\ntest_eq(_A2()(1), 3)\ntest_ne(str(_A2), '_A2')\n_A2 = named_partial('_A2', _A, add=2)\ntest_eq(_A2()(1), 3)\ntest_eq(str(_A2), '_A2')\n\n\nsource\n\n\ndict2attrdict\n\n dict2attrdict (d:dict)\n\nConverts a (nested) dict to an AttrDict.\n\n\n\n\nType\nDetails\n\n\n\n\nd\ndict\na dict\n\n\n\n\nsource\n\n\nattrdict2dict\n\n attrdict2dict (d:dict)\n\nConverts a (nested) AttrDict dict to a dict.\n\n\n\n\nType\nDetails\n\n\n\n\nd\ndict\na dict\n\n\n\n\n# Test attrdict2dict\nd = AttrDict({'a': 1, 'b': AttrDict({'c': 2, 'd': 3})})\ntest_eq(attrdict2dict(d), {'a': 1, 'b': {'c': 2, 'd': 3}})\n# Test dict2attrdict\nd = {'a': 1, 'b': {'c': 2, 'd': 3}}\ntest_eq(dict2attrdict(d), AttrDict({'a': 1, 'b': AttrDict({'c': 2, 'd': 3})}))\n\n\nsource\n\n\nget_config\n\n get_config (file_path)\n\nGets a config from a yaml file.\n\nsource\n\n\nyaml2dict\n\n yaml2dict (file_path, attrdict=True)\n\nConverts a yaml file to a dict (optionally AttrDict).\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nfile_path\n\n\na path to a yaml file\n\n\nattrdict\nbool\nTrue\nif True, convert output to AttrDict\n\n\n\n\nsource\n\n\ndict2yaml\n\n dict2yaml (d, file_path, sort_keys=False)\n\nConverts a dict to a yaml file.\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nd\n\n\na dict\n\n\nfile_path\n\n\na path to a yaml file\n\n\nsort_keys\nbool\nFalse\nif True, sort the keys\n\n\n\n\nprogram: wandb_scripts/train_script.py          # (required) Path to training script.\nmethod: bayes                                   # (required) Specify the search strategy: grid, random or bayes\nparameters:                                     # (required) Specify parameters bounds to search.\n   bs:\n      values: [32, 64, 128]\n   depth:\n      values: [3, 6, 9, 12]\n   fc_dropout:\n      distribution: uniform\n      min: 0.\n      max: 0.5\n   lr_max:\n      values: [0.001, 0.003, 0.01, 0.03, 0.1]\n   n_epoch:\n      values: [10, 15, 20]\n   nb_filters:\n      values: [32, 64, 128]\nname: LSST_sweep_01\nmetric: \n   name: accuracy                              # This must match one of the metrics in the training script\n   goal: maximize\nearly_terminate: \n   type: hyperband\n   min_iter: 3\nproject: LSST_wandb_hpo\n\nWriting sweep_config.yaml\n\n\n\nfname = \"sweep_config.yaml\"\nsweep_config = yaml2dict(fname)\nprint(sweep_config)\ntest_eq(sweep_config.method, 'bayes')\ntest_eq(sweep_config['metric'], {'name': 'accuracy', 'goal': 'maximize'})\nos.remove(fname)\n\n{'program': 'wandb_scripts/train_script.py', 'method': 'bayes', 'parameters': {'bs': {'values': [32, 64, 128]}, 'depth': {'values': [3, 6, 9, 12]}, 'fc_dropout': {'distribution': 'uniform', 'min': 0.0, 'max': 0.5}, 'lr_max': {'values': [0.001, 0.003, 0.01, 0.03, 0.1]}, 'n_epoch': {'values': [10, 15, 20]}, 'nb_filters': {'values': [32, 64, 128]}}, 'name': 'LSST_sweep_01', 'metric': {'name': 'accuracy', 'goal': 'maximize'}, 'early_terminate': {'type': 'hyperband', 'min_iter': 3}, 'project': 'LSST_wandb_hpo'}\n\n\n\nsource\n\n\nget_cat_cols\n\n get_cat_cols (df)\n\n\nsource\n\n\nget_cont_cols\n\n get_cont_cols (df)\n\n\nsource\n\n\nstr2index\n\n str2index (o)\n\n\nsource\n\n\nstr2list\n\n str2list (o)\n\n\nsource\n\n\nmap_array\n\n map_array (arr, dim=1)\n\n\nsource\n\n\nget_mapping\n\n get_mapping (arr, dim=1, return_counts=False)\n\n\na = np.asarray(alphabet[np.random.randint(0,15,30)]).reshape(10,3)\nb = np.asarray(ALPHABET[np.random.randint(6,10,30)]).reshape(10,3)\nx = concat(a,b,dim=1)\nmaps, counts = get_mapping(x, dim=1, return_counts=True)\nx, maps, counts\n\n(array([['o', 'd', 'k', 'H', 'G', 'J'],\n        ['i', 'a', 'i', 'H', 'J', 'G'],\n        ['j', 'k', 'm', 'J', 'G', 'H'],\n        ['b', 'h', 'm', 'J', 'G', 'J'],\n        ['d', 'k', 'a', 'G', 'I', 'J'],\n        ['e', 'e', 'b', 'J', 'I', 'I'],\n        ['e', 'd', 'o', 'G', 'J', 'I'],\n        ['m', 'f', 'c', 'J', 'I', 'J'],\n        ['i', 'l', 'l', 'H', 'J', 'G'],\n        ['h', 'a', 'm', 'I', 'H', 'G']], dtype='<U1'),\n [(#8) ['b','d','e','h','i','j','m','o'],\n  (#7) ['a','d','e','f','h','k','l'],\n  (#8) ['a','b','c','i','k','l','m','o'],\n  (#4) ['G','H','I','J'],\n  (#4) ['G','H','I','J'],\n  (#4) ['G','H','I','J']],\n [8, 7, 8, 4, 4, 4])\n\n\n\nx = np.asarray(alphabet[np.random.randint(0,15,30)]).reshape(10,3)\nx, map_array(x), map_array(x, 1)\n\n(array([['g', 'b', 'c'],\n        ['o', 'j', 'f'],\n        ['k', 'k', 'n'],\n        ['g', 'a', 'o'],\n        ['f', 'a', 'd'],\n        ['g', 'b', 'c'],\n        ['b', 'j', 'c'],\n        ['d', 'l', 'g'],\n        ['a', 'k', 'h'],\n        ['l', 'j', 'n']], dtype='<U1'),\n array([[4, 1, 0],\n        [7, 2, 2],\n        [5, 3, 5],\n        [4, 0, 6],\n        [3, 0, 1],\n        [4, 1, 0],\n        [1, 2, 0],\n        [2, 4, 3],\n        [0, 3, 4],\n        [6, 2, 5]]),\n array([[4, 1, 0],\n        [7, 2, 2],\n        [5, 3, 5],\n        [4, 0, 6],\n        [3, 0, 1],\n        [4, 1, 0],\n        [1, 2, 0],\n        [2, 4, 3],\n        [0, 3, 4],\n        [6, 2, 5]]))\n\n\n\nsource\n\n\nlog_tfm\n\n log_tfm (o, inplace=False)\n\nLog transforms an array-like object with positive and/or negative values\n\narr = np.asarray([-1000, -100, -10, -1, 0, 1, 10, 100, 1000]).astype(float)\nplt.plot(arr, log_tfm(arr, False))\nplt.show()\n\n\n\n\n\nt = tensor([-1000, -100, -10, -1, 0, 1, 10, 100, 1000]).float()\nplt.plot(t, log_tfm(t, False))\nplt.show()\n\n\n\n\n\nsource\n\n\nto_sincos_time\n\n to_sincos_time (arr, max_value)\n\n\narr = np.sort(np.random.rand(100) * 5)\narr_sin, arr_cos = to_sincos_time(arr, 5)\nplt.scatter(arr, arr_sin)\nplt.scatter(arr, arr_cos)\nplt.show()\n\n\n\n\n\nsource\n\n\nplot_feature_dist\n\n plot_feature_dist (X, percentiles=[0, 0.1, 0.5, 1, 5, 10, 25, 50, 75, 90,\n                    95, 99, 99.5, 99.9, 100])\n\n\narr = np.random.rand(10, 3, 100)\nplot_feature_dist(arr, percentiles=[0,0.1,0.5,1,5,10,25,50,75,90,95,99,99.5,99.9,100])\n\n\n\n\n\n\n\n\n\n\n\nsource\n\n\nrolling_moving_average\n\n rolling_moving_average (o, window=2)\n\n\na = np.arange(60).reshape(2,3,10).astype(float)\nt = torch.arange(60).reshape(2,3,10).float()\ntest_close(rolling_moving_average(a, window=3), rolling_moving_average(t, window=3).numpy())\nprint(t)\nprint(rolling_moving_average(t, window=3))\n\ntensor([[[ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9.],\n         [10., 11., 12., 13., 14., 15., 16., 17., 18., 19.],\n         [20., 21., 22., 23., 24., 25., 26., 27., 28., 29.]],\n\n        [[30., 31., 32., 33., 34., 35., 36., 37., 38., 39.],\n         [40., 41., 42., 43., 44., 45., 46., 47., 48., 49.],\n         [50., 51., 52., 53., 54., 55., 56., 57., 58., 59.]]])\ntensor([[[ 0.0000,  0.5000,  1.0000,  2.0000,  3.0000,  4.0000,  5.0000,\n           6.0000,  7.0000,  8.0000],\n         [10.0000, 10.5000, 11.0000, 12.0000, 13.0000, 14.0000, 15.0000,\n          16.0000, 17.0000, 18.0000],\n         [20.0000, 20.5000, 21.0000, 22.0000, 23.0000, 24.0000, 25.0000,\n          26.0000, 27.0000, 28.0000]],\n\n        [[30.0000, 30.5000, 31.0000, 32.0000, 33.0000, 34.0000, 35.0000,\n          36.0000, 37.0000, 38.0000],\n         [40.0000, 40.5000, 41.0000, 42.0000, 43.0000, 44.0000, 45.0000,\n          46.0000, 47.0000, 48.0000],\n         [50.0000, 50.5000, 51.0000, 52.0000, 53.0000, 54.0000, 55.0000,\n          56.0000, 57.0000, 58.0000]]])\n\n\n\nsource\n\n\nfbfill_sequence\n\n fbfill_sequence (o)\n\nForward and backward fills an array-like object alongside sequence dimension\n\nsource\n\n\nbfill_sequence\n\n bfill_sequence (o)\n\nBackward fills an array-like object alongside sequence dimension\n\nsource\n\n\nffill_sequence\n\n ffill_sequence (o)\n\nForward fills an array-like object alongside sequence dimension\n\na = np.arange(80).reshape(2, 4, 10).astype(float)\nmask = np.random.rand(*a.shape)\na[mask > .8] = np.nan\nt = torch.from_numpy(a)\nt\n\ntensor([[[ 0.,  1.,  2.,  3., nan,  5.,  6.,  7.,  8.,  9.],\n         [10., 11., 12., 13., 14., 15., 16., 17., 18., nan],\n         [20., 21., 22., 23., 24., 25., 26., 27., 28., 29.],\n         [30., nan, nan, nan, 34., 35., 36., 37., 38., 39.]],\n\n        [[40., nan, 42., 43., 44., nan, 46., 47., 48., 49.],\n         [50., 51., 52., 53., 54., 55., nan, 57., 58., 59.],\n         [60., 61., nan, 63., 64., 65., nan, 67., 68., 69.],\n         [70., 71., 72., nan, 74., 75., 76., 77., 78., nan]]],\n       dtype=torch.float64)\n\n\n\n# forward fill\nfilled_a = ffill_sequence(a)\nprint(filled_a)\nm = np.isnan(filled_a)\ntest_eq(filled_a[~m], ffill_sequence(t).numpy()[~m])\n\n[[[ 0.  1.  2.  3.  3.  5.  6.  7.  8.  9.]\n  [10. 11. 12. 13. 14. 15. 16. 17. 18. 18.]\n  [20. 21. 22. 23. 24. 25. 26. 27. 28. 29.]\n  [30. 30. 30. 30. 34. 35. 36. 37. 38. 39.]]\n\n [[40. 40. 42. 43. 44. 44. 46. 47. 48. 49.]\n  [50. 51. 52. 53. 54. 55. 55. 57. 58. 59.]\n  [60. 61. 61. 63. 64. 65. 65. 67. 68. 69.]\n  [70. 71. 72. 72. 74. 75. 76. 77. 78. 78.]]]\n\n\n\n# backward fill\nfilled_a = bfill_sequence(a)\nprint(filled_a)\nm = np.isnan(filled_a)\ntest_eq(filled_a[~m], bfill_sequence(t).numpy()[~m])\n\n[[[ 0.  1.  2.  3.  5.  5.  6.  7.  8.  9.]\n  [10. 11. 12. 13. 14. 15. 16. 17. 18. nan]\n  [20. 21. 22. 23. 24. 25. 26. 27. 28. 29.]\n  [30. 34. 34. 34. 34. 35. 36. 37. 38. 39.]]\n\n [[40. 42. 42. 43. 44. 46. 46. 47. 48. 49.]\n  [50. 51. 52. 53. 54. 55. 57. 57. 58. 59.]\n  [60. 61. 63. 63. 64. 65. 67. 67. 68. 69.]\n  [70. 71. 72. 74. 74. 75. 76. 77. 78. nan]]]\n\n\n\n# forward & backward fill\nfilled_a = fbfill_sequence(a)\nprint(filled_a)\nm = np.isnan(filled_a)\ntest_eq(filled_a[~m], fbfill_sequence(t).numpy()[~m])\n\n[[[ 0.  1.  2.  3.  3.  5.  6.  7.  8.  9.]\n  [10. 11. 12. 13. 14. 15. 16. 17. 18. 18.]\n  [20. 21. 22. 23. 24. 25. 26. 27. 28. 29.]\n  [30. 30. 30. 30. 34. 35. 36. 37. 38. 39.]]\n\n [[40. 40. 42. 43. 44. 44. 46. 47. 48. 49.]\n  [50. 51. 52. 53. 54. 55. 55. 57. 58. 59.]\n  [60. 61. 61. 63. 64. 65. 65. 67. 68. 69.]\n  [70. 71. 72. 72. 74. 75. 76. 77. 78. 78.]]]\n\n\n\nsource\n\n\ndummify\n\n dummify (o:Union[numpy.ndarray,torch.Tensor], by_var:bool=True,\n          inplace:bool=False, skip:Optional[list]=None, random_state=None)\n\nShuffles an array-like object along all dimensions or dimension 1 (variables) if by_var is True.\n\narr = np.random.rand(2,3,10)\narr_original = arr.copy()\ndummy_arr = dummify(arr)\ntest_ne(arr_original, dummy_arr)\ntest_eq(arr_original, arr)\ndummify(arr, inplace=True)\ntest_ne(arr_original, arr)\n\n\nt = torch.rand(2,3,10)\nt_original = t.clone()\ndummy_t = dummify(t)\ntest_ne(t_original, dummy_t)\ntest_eq(t_original, t)\ndummify(t, inplace=True)\ntest_ne(t_original, t)\n\n\n\n\n\n\n    \n      \n      66.67% [2/3 00:00<00:00]\n    \n    \n\n\n\nsource\n\n\nshuffle_along_axis\n\n shuffle_along_axis (o, axis=-1, random_state=None)\n\n\nX = np.arange(60).reshape(2,3,10) + 10\nX_shuffled = shuffle_along_axis(X,(0, -1), random_state=23)\ntest_eq(X_shuffled, np.array([[[13, 15, 41, 14, 40, 49, 18, 42, 47, 46],\n                               [28, 56, 53, 50, 52, 25, 24, 57, 51, 59],\n                               [34, 30, 38, 35, 69, 66, 63, 67, 61, 62]],\n\n                              [[19, 10, 11, 16, 43, 12, 17, 48, 45, 44],\n                               [23, 20, 26, 22, 21, 27, 58, 29, 54, 55],\n                               [36, 31, 39, 60, 33, 68, 37, 32, 65, 64]]]))\n\n\nsource\n\n\nanalyze_array\n\n analyze_array (o, bins=100, density=False, feature_names=None,\n                clip_outliers_plot=False, quantile_range=(25.0, 75.0),\n                percentiles=[1, 25, 50, 75, 99], text_len=12, figsize=(10,\n                6))\n\n\nsource\n\n\nanalyze_feature\n\n analyze_feature (feature, bins=100, density=False, feature_name=None,\n                  clip_outliers_plot=False, quantile_range=(25.0, 75.0),\n                  percentiles=[1, 25, 50, 75, 99], text_len=12,\n                  figsize=(10, 6))\n\n\nx = np.random.normal(size=(1000))\nanalyze_array(x)\n\n array shape: (1000,)\n       dtype: float64\n  nan values: 0.0%\n         max: 3.1970730641602483\n           1: -2.356745488349626\n          25: -0.6892966055666662\n          50: -0.006465819439255997\n          75: 0.6636981673686502\n          99: 2.476501149059868\n         min: -3.3737675123084783\n outlier min: -2.718788764969641\n outlier max: 2.693190326771625\n    outliers: 1.0%\n        mean: -0.012211944913539965\n         std: 1.0002671473421683\n normal dist: True\n\n\n\n\n\n\nx1 = np.random.normal(size=(1000,2))\nx2 = np.random.normal(3, 5, size=(1000,2))\nx = x1 + x2\nanalyze_array(x)\n\n array shape: (1000, 2)\n\n  0  feature: 0\n\n       dtype: float64\n  nan values: 0.0%\n         max: 19.34161521120589\n           1: -9.521267646388687\n          25: -0.0470111656023188\n          50: 3.566494543750938\n          75: 6.820831202240424\n          99: 13.683667802761658\n         min: -13.143794308827092\n outlier min: -10.348774717366434\n outlier max: 17.12259475400454\n    outliers: 0.8%\n        mean: 3.303848320692596\n         std: 5.149018878780698\n normal dist: True\n\n\n\n\n\n\n  1  feature: 1\n\n       dtype: float64\n  nan values: 0.0%\n         max: 20.672094144356908\n           1: -8.56533399982716\n          25: -0.38884250023654826\n          50: 2.732623188793667\n          75: 6.128916801264278\n          99: 13.75431022041546\n         min: -10.561435870109753\n outlier min: -10.165481452487787\n outlier max: 15.905555753515518\n    outliers: 0.5%\n        mean: 2.8251747667882583\n         std: 4.84283364242821\n normal dist: True\n\n\n\n\n\n\nsource\n\n\nget_relpath\n\n get_relpath (path)\n\n\nsource\n\n\nto_root_path\n\n to_root_path (path)\n\nConverts a path to an absolute path from the root directory of the repository.\n\nsource\n\n\nget_root\n\n get_root ()\n\nReturns the root directory of the git repository.\n\nsource\n\n\nsplit_in_chunks\n\n split_in_chunks (o, chunksize, start=0, shuffle=False, drop_last=False)\n\n\na = np.arange(5, 15)\ntest_eq(split_in_chunks(a, 3, drop_last=False), [array([5, 6, 7]), array([ 8,  9, 10]), array([11, 12, 13]), array([14])])\ntest_eq(split_in_chunks(a, 3, drop_last=True), [array([5, 6, 7]), array([ 8,  9, 10]), array([11, 12, 13])])\ntest_eq(split_in_chunks(a, 3, start=2, drop_last=True), [array([7, 8, 9]), array([10, 11, 12])])\n\n\nsource\n\n\nload_object\n\n load_object (file_path)\n\n\nsource\n\n\nsave_object\n\n save_object (o, file_path, verbose=True)\n\n\nsplit = np.arange(100)\nsave_object(split, file_path='data/test')\nsplit2 = load_object('data/test.pkl')\ntest_eq(split, split2)\n\ndata directory already exists.\nndarray saved as data/test.pkl\n\n\n\nsplits = L([[[0,1,2,3,4], [5,6,7,8,9]],[[10,11,12,13,14], [15,16,17,18,19]]])\nsave_object(splits, file_path=Path('data/test'))\nsplits2 = load_object('data/test')\ntest_eq(splits, splits2)\n\ndata directory already exists.\nL saved as data/test.pkl\n\n\n\nsource\n\n\nget_idxs_to_keep\n\n get_idxs_to_keep (o, cond, crit='all', invert=False, axis=(1, 2),\n                   keepdims=False)\n\n\na = np.random.rand(100, 2, 10)\na[a > .95] = np.nan\nidxs_to_keep = get_idxs_to_keep(a, np.isfinite)\nif idxs_to_keep.size>0: \n    test_eq(np.isnan(a[idxs_to_keep]).sum(), 0)\n\n\nsource\n\n\nzerofy\n\n zerofy (a, stride, keep=False)\n\nCreate copies of an array setting individual/ group values to zero\n\nstride = 3\na = np.arange(2*5).reshape(2,5) + 1\n\nzerofy(a, stride, keep=False)\n\narray([[[ 0.,  0.,  3.,  4.,  5.],\n        [ 6.,  7.,  8.,  9., 10.]],\n\n       [[ 1.,  2.,  0.,  0.,  0.],\n        [ 6.,  7.,  8.,  9., 10.]],\n\n       [[ 1.,  2.,  3.,  4.,  5.],\n        [ 0.,  0.,  8.,  9., 10.]],\n\n       [[ 1.,  2.,  3.,  4.,  5.],\n        [ 6.,  7.,  0.,  0.,  0.]]])\n\n\n\nsource\n\n\nfeat2list\n\n feat2list (o)\n\n\na = 'a'\ntest_eq(feat2list(a), ['a'])\na = ['a', 'b']\ntest_eq(feat2list(a), ['a', 'b'])\na = None\ntest_eq(feat2list(a), [])\n\n\nsource\n\n\nsmallest_dtype\n\n smallest_dtype (num, use_unsigned=False)\n\nFind the smallest dtype that can safely hold num\n\ntest_eq(smallest_dtype(3654), 'int16')\ntest_eq(smallest_dtype(2048.), 'float16')\ntest_eq(smallest_dtype(365454), 'int32')\ntest_eq(smallest_dtype(365454.), 'float32')\ntest_eq(smallest_dtype(3654545134897), 'int64')\n\n\nsource\n\n\nplot_forecast\n\n plot_forecast (X_true, y_true, y_pred, sel_vars=None, idx=None,\n                figsize=(8, 4), n_samples=1)"
  },
  {
    "objectID": "callback.core.html",
    "href": "callback.core.html",
    "title": "Callback",
    "section": "",
    "text": "Miscellaneous callbacks for timeseriesAI."
  },
  {
    "objectID": "callback.core.html#events",
    "href": "callback.core.html#events",
    "title": "Callback",
    "section": "Events",
    "text": "Events\nA callback can implement actions on the following events: * before_fit: called before doing anything, ideal for initial setup. * before_epoch: called at the beginning of each epoch, useful for any behavior you need to reset at each epoch. * before_train: called at the beginning of the training part of an epoch. * before_batch: called at the beginning of each batch, just after drawing said batch. It can be used to do any setup necessary for the batch (like hyper-parameter scheduling) or to change the input/target before it goes in the model (change of the input with techniques like mixup for instance). * after_pred: called after computing the output of the model on the batch. It can be used to change that output before it’s fed to the loss. * after_loss: called after the loss has been computed, but before the backward pass. It can be used to add any penalty to the loss (AR or TAR in RNN training for instance). * before_backward: called after the loss has been computed, but only in training mode (i.e. when the backward pass will be used) * after_backward: called after the backward pass, but before the update of the parameters. It can be used to do any change to the gradients before said update (gradient clipping for instance). * after_step: called after the step and before the gradients are zeroed. * after_batch: called at the end of a batch, for any clean-up before the next one. * after_train: called at the end of the training phase of an epoch. * before_validate: called at the beginning of the validation phase of an epoch, useful for any setup needed specifically for validation. * after_validate: called at the end of the validation part of an epoch. * after_epoch: called at the end of an epoch, for any clean-up before the next one. * after_fit: called at the end of training, for final clean-up."
  },
  {
    "objectID": "callback.core.html#learner-attributes",
    "href": "callback.core.html#learner-attributes",
    "title": "Callback",
    "section": "Learner attributes",
    "text": "Learner attributes\nWhen writing a callback, the following attributes of Learner are available:\n\nmodel: the model used for training/validation\ndata: the underlying DataLoaders\nloss_func: the loss function used\nopt: the optimizer used to udpate the model parameters\nopt_func: the function used to create the optimizer\ncbs: the list containing all Callbacks\ndl: current DataLoader used for iteration\nx/xb: last input drawn from self.dl (potentially modified by callbacks). xb is always a tuple (potentially with one element) and x is detuplified. You can only assign to xb.\ny/yb: last target drawn from self.dl (potentially modified by callbacks). yb is always a tuple (potentially with one element) and y is detuplified. You can only assign to yb.\npred: last predictions from self.model (potentially modified by callbacks)\nloss: last computed loss (potentially modified by callbacks)\nn_epoch: the number of epochs in this training\nn_iter: the number of iterations in the current self.dl\nepoch: the current epoch index (from 0 to n_epoch-1)\niter: the current iteration index in self.dl (from 0 to n_iter-1)\n\nThe following attributes are added by TrainEvalCallback and should be available unless you went out of your way to remove that callback: * train_iter: the number of training iterations done since the beginning of this training * pct_train: from 0. to 1., the percentage of training iterations completed * training: flag to indicate if we’re in training mode or not\nThe following attribute is added by Recorder and should be available unless you went out of your way to remove that callback: * smooth_loss: an exponentially-averaged version of the training loss"
  },
  {
    "objectID": "callback.core.html#transform-scheduler",
    "href": "callback.core.html#transform-scheduler",
    "title": "Callback",
    "section": "Transform scheduler",
    "text": "Transform scheduler\n\nsource\n\nTransformScheduler\n\n TransformScheduler (schedule_func:<built-infunctioncallable>,\n                     show_plot:bool=False)\n\nA callback to schedule batch transforms during training based on a function (sched_lin, sched_exp, sched_cos (default), etc)\n\nTransformScheduler(SchedCos(1, 0))\n\nTransformScheduler(<fastai.callback.schedule._Annealer object>)\n\n\n\np = torch.linspace(0.,1,100)\nf = combine_scheds([0.3, 0.4, 0.3], [SchedLin(1.,1.), SchedCos(1.,0.), SchedLin(0.,.0), ])\nplt.plot(p, [f(o) for o in p]);\n\n\n\n\n\np = torch.linspace(0.,1,100)\nf = combine_scheds([0.3, 0.7], [SchedCos(0.,1.), SchedCos(1.,0.)])\nplt.plot(p, [f(o) for o in p]);"
  },
  {
    "objectID": "callback.core.html#showgraph",
    "href": "callback.core.html#showgraph",
    "title": "Callback",
    "section": "ShowGraph",
    "text": "ShowGraph\n\nsource\n\nShowGraph\n\n ShowGraph (plot_metrics:bool=True, final_losses:bool=True,\n            perc:float=0.5)\n\n(Modified) Update a graph of training and validation loss"
  },
  {
    "objectID": "callback.core.html#savemodel",
    "href": "callback.core.html#savemodel",
    "title": "Callback",
    "section": "SaveModel",
    "text": "SaveModel\n\nsource\n\nSaveModel\n\n SaveModel (monitor='valid_loss', comp=None, min_delta=0.0, fname='model',\n            every_epoch=False, at_end=False, with_opt=False,\n            reset_on_fit=True, verbose=False)\n\nA TrackerCallback that saves the model’s best during training and loads it at the end with a verbose option."
  },
  {
    "objectID": "models.tsitplus.html",
    "href": "models.tsitplus.html",
    "title": "TSiT",
    "section": "",
    "text": "This is a PyTorch implementation created by Ignacio Oguiza (oguiza@timeseriesAI.co) based on ViT (Vision Transformer):\nDosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner, T., … & Houlsby, N. (2020).\nAn image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929.\nsource"
  },
  {
    "objectID": "models.tsitplus.html#feature-extractor",
    "href": "models.tsitplus.html#feature-extractor",
    "title": "TSiT",
    "section": "Feature extractor",
    "text": "Feature extractor\nIt’s a known fact that transformers cannot be directly applied to long sequences. To avoid this, we have included a way to subsample the sequence to generate a more manageable input.\n\nfrom tsai.data.validation import get_splits\nfrom tsai.data.core import get_ts_dls\n\n\nX = np.zeros((10, 3, 5000)) \ny = np.random.randint(0,2,X.shape[0])\nsplits = get_splits(y)\ndls = get_ts_dls(X, y, splits=splits)\nxb, yb = dls.train.one_batch()\nxb\n\n\n\n\nTSTensor(samples:8, vars:3, len:5000, device=cpu, dtype=torch.float32)\n\n\nIf you try to use TSiTPlus, it’s likely you’ll get an ‘out-of-memory’ error.\nTo avoid this you can subsample the sequence reducing the input’s length. This can be done in multiple ways. Here are a few examples:\n\n# Separable convolution (to avoid mixing channels)\nfeature_extractor = Conv1d(xb.shape[1], xb.shape[1], ks=100, stride=50, padding=0, groups=xb.shape[1]).to(default_device())\nfeature_extractor.to(xb.device)(xb).shape\n\ntorch.Size([8, 3, 99])\n\n\n\n# Convolution (if you want to mix channels or change number of channels)\nfeature_extractor=MultiConv1d(xb.shape[1], 64, kss=[1,3,5,7,9], keep_original=True).to(default_device())\ntest_eq(feature_extractor.to(xb.device)(xb).shape, (xb.shape[0], 64, xb.shape[-1]))\n\n\n# MaxPool\nfeature_extractor = nn.Sequential(Pad1d((0, 50), 0), nn.MaxPool1d(kernel_size=100, stride=50)).to(default_device())\nfeature_extractor.to(xb.device)(xb).shape\n\ntorch.Size([8, 3, 100])\n\n\n\n# AvgPool\nfeature_extractor = nn.Sequential(Pad1d((0, 50), 0), nn.AvgPool1d(kernel_size=100, stride=50)).to(default_device())\nfeature_extractor.to(xb.device)(xb).shape\n\ntorch.Size([8, 3, 100])\n\n\nOnce you decide what type of transform you want to apply, you just need to pass the layer as the feature_extractor attribute:\n\nbs = 16\nnvars = 4\nseq_len = 1000\nc_out = 2\nd_model = 128\n\nxb = torch.rand(bs, nvars, seq_len)\nfeature_extractor = partial(Conv1d, ks=5, stride=3, padding=0, groups=xb.shape[1])\nmodel = TSiTPlus(nvars, c_out, seq_len, d_model=d_model, feature_extractor=feature_extractor)\ntest_eq(model.to(xb.device)(xb).shape, (bs, c_out))"
  },
  {
    "objectID": "models.tsitplus.html#categorical-variables",
    "href": "models.tsitplus.html#categorical-variables",
    "title": "TSiT",
    "section": "Categorical variables",
    "text": "Categorical variables\n\nfrom tsai.utils import alphabet, ALPHABET\n\n\na = alphabet[np.random.randint(0,3,40)]\nb = ALPHABET[np.random.randint(6,10,40)]\nc = np.random.rand(40).reshape(4,1,10)\nmap_a = {k:v for v,k in enumerate(np.unique(a))}\nmap_b = {k:v for v,k in enumerate(np.unique(b))}\nn_cat_embeds = [len(m.keys()) for m in [map_a, map_b]]\nszs = [emb_sz_rule(n) for n in n_cat_embeds]\na = np.asarray(a.map(map_a)).reshape(4,1,10)\nb = np.asarray(b.map(map_b)).reshape(4,1,10)\ninp = torch.from_numpy(np.concatenate((c,a,b), 1)).float()\nfeature_extractor = partial(Conv1d, ks=3, padding='same')\nmodel = TSiTPlus(3, 2, 10, d_model=64, cat_pos=[1,2], feature_extractor=feature_extractor)\ntest_eq(model(inp).shape, (4,2))\n\n[W NNPACK.cpp:53] Could not initialize NNPACK! Reason: Unsupported hardware."
  },
  {
    "objectID": "models.tsitplus.html#sequence-embedding",
    "href": "models.tsitplus.html#sequence-embedding",
    "title": "TSiT",
    "section": "Sequence Embedding",
    "text": "Sequence Embedding\nSometimes you have a samples with a very long sequence length. In those cases you may want to reduce it’s length before passing it to the transformer. To do that you may just pass a token_size like in this example:\n\nt = torch.rand(8, 2, 10080)\nSeqTokenizer(2, 128, 60)(t).shape\n\ntorch.Size([8, 128, 168])\n\n\n\nt = torch.rand(8, 2, 10080)\nmodel = TSiTPlus(2, 5, 10080, d_model=64, token_size=60)\nmodel(t).shape\n\ntorch.Size([8, 5])"
  },
  {
    "objectID": "data.mixed_augmentation.html",
    "href": "data.mixed_augmentation.html",
    "title": "Label-mixing transforms",
    "section": "",
    "text": "Callbacks that perform data augmentation by mixing samples in different ways.\n\n\nsource\n\nMixHandler1d\n\n MixHandler1d (alpha=0.5)\n\nA handler class for implementing mixed sample data augmentation\n\nsource\n\n\nMixUp1d\n\n MixUp1d (alpha=0.4)\n\nImplementation of https://arxiv.org/abs/1710.09412\n\nfrom fastai.learner import *\nfrom tsai.models.InceptionTime import *\nfrom tsai.data.external import get_UCR_data\nfrom tsai.data.core import get_ts_dls, TSCategorize\nfrom tsai.data.preprocessing import TSStandardize\nfrom tsai.learner import ts_learner\n\n\nX, y, splits = get_UCR_data('NATOPS', return_split=False)\ntfms = [None, TSCategorize()]\nbatch_tfms = TSStandardize()\ndls = get_ts_dls(X, y, tfms=tfms, splits=splits, batch_tfms=batch_tfms)\nlearn = ts_learner(dls, InceptionTime, cbs=MixUp1d(0.4))\nlearn.fit_one_cycle(1)\n\n[0, 1.9250454902648926, 1.826296329498291, '00:06']\n\n\n\nsource\n\n\nCutMix1d\n\n CutMix1d (alpha=1.0)\n\nImplementation of https://arxiv.org/abs/1905.04899\n\nsource\n\n\nIntraClassCutMix1d\n\n IntraClassCutMix1d (alpha=1.0)\n\nImplementation of CutMix applied to examples of the same class\n\nX, y, splits = get_UCR_data('NATOPS', split_data=False)\ntfms = [None, TSCategorize()]\nbatch_tfms = TSStandardize()\ndls = get_ts_dls(X, y, tfms=tfms, splits=splits, batch_tfms=batch_tfms)\nlearn = ts_learner(dls, InceptionTime, cbs=IntraClassCutMix1d())\nlearn.fit_one_cycle(1)\n\n[0, 1.781386375427246, 1.7941926717758179, '00:04']\n\n\n\nX, y, splits = get_UCR_data('NATOPS', split_data=False)\ntfms = [None, TSCategorize()]\nbatch_tfms = TSStandardize()\ndls = get_ts_dls(X, y, tfms=tfms, splits=splits, batch_tfms=batch_tfms)\nlearn = ts_learner(dls, cbs=CutMix1d(1.))\nlearn.fit_one_cycle(1)\n\n[0, 1.7089701890945435, 1.777895450592041, '00:05']"
  },
  {
    "objectID": "losses.html",
    "href": "losses.html",
    "title": "Losses",
    "section": "",
    "text": "Losses not available in fastai or Pytorch.\n\n\nsource\n\nHuberLoss\n\n HuberLoss (reduction='mean', delta=1.0)\n\nHuber loss\nCreates a criterion that uses a squared term if the absolute element-wise error falls below delta and a delta-scaled L1 term otherwise. This loss combines advantages of both :class:L1Loss and :class:MSELoss; the delta-scaled L1 region makes the loss less sensitive to outliers than :class:MSELoss, while the L2 region provides smoothness over :class:L1Loss near 0. See Huber loss <https://en.wikipedia.org/wiki/Huber_loss>_ for more information. This loss is equivalent to nn.SmoothL1Loss when delta == 1.\n\nsource\n\n\nLogCoshLoss\n\n LogCoshLoss (reduction='mean', delta=1.0)\n\nBase class for all neural network modules.\nYour models should also subclass this class.\nModules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes::\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 20, 5)\n        self.conv2 = nn.Conv2d(20, 20, 5)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        return F.relu(self.conv2(x))\nSubmodules assigned in this way will be registered, and will have their parameters converted too when you call :meth:to, etc.\n.. note:: As per the example above, an __init__() call to the parent class must be made before assignment on the child.\n:ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool\n\ninp = torch.rand(8, 3, 10)\ntarg = torch.randn(8, 3, 10)\ntest_close(HuberLoss(delta=1)(inp, targ), nn.SmoothL1Loss()(inp, targ))\nLogCoshLoss()(inp, targ)\n\ntensor(0.4588)\n\n\n\nsource\n\n\nMaskedLossWrapper\n\n MaskedLossWrapper (crit)\n\nSame as nn.Module, but no need for subclasses to call super().__init__\n\ninp = torch.rand(8, 3, 10)\ntarg = torch.randn(8, 3, 10)\ntarg[targ >.8] = np.nan\nnn.L1Loss()(inp, targ), MaskedLossWrapper(nn.L1Loss())(inp, targ)\n\n(tensor(nan), tensor(1.0520))\n\n\n\nsource\n\n\nCenterPlusLoss\n\n CenterPlusLoss (loss, c_out, λ=0.01, logits_dim=None)\n\nSame as nn.Module, but no need for subclasses to call super().__init__\n\nsource\n\n\nCenterLoss\n\n CenterLoss (c_out, logits_dim=None)\n\nCode in Pytorch has been slightly modified from: https://github.com/KaiyangZhou/pytorch-center-loss/blob/master/center_loss.py Based on paper: Wen et al. A Discriminative Feature Learning Approach for Deep Face Recognition. ECCV 2016.\nArgs: c_out (int): number of classes. logits_dim (int): dim 1 of the logits. By default same as c_out (for one hot encoded logits)\n\nc_in = 10\nx = torch.rand(64, c_in).to(device=default_device())\nx = F.softmax(x, dim=1)\nlabel = x.max(dim=1).indices\nCenterLoss(c_in).to(x.device)(x, label), CenterPlusLoss(LabelSmoothingCrossEntropyFlat(), c_in).to(x.device)(x, label)\n\n(tensor(9.2481, grad_fn=<DivBackward0>),\n TensorBase(2.3559, grad_fn=<AliasBackward0>))\n\n\n\nCenterPlusLoss(LabelSmoothingCrossEntropyFlat(), c_in)\n\nCenterPlusLoss(loss=FlattenedLoss of LabelSmoothingCrossEntropy(), c_out=10, λ=0.01)\n\n\n\nsource\n\n\nFocalLoss\n\n FocalLoss (alpha:Optional[torch.Tensor]=None, gamma:float=2.0,\n            reduction:str='mean')\n\nWeighted, multiclass focal loss\n\ninputs = torch.normal(0, 2, (16, 2)).to(device=default_device())\ntargets = torch.randint(0, 2, (16,)).to(device=default_device())\nFocalLoss()(inputs, targets)\n\ntensor(0.9829)\n\n\n\nsource\n\n\nTweedieLoss\n\n TweedieLoss (p=1.5, eps=1e-08)\n\nSame as nn.Module, but no need for subclasses to call super().__init__\n\nc_in = 10\noutput = torch.rand(64).to(device=default_device())\ntarget = torch.rand(64).to(device=default_device())\nTweedieLoss().to(output.device)(output, target)\n\ntensor(3.0539)"
  },
  {
    "objectID": "tslearner.html",
    "href": "tslearner.html",
    "title": "TSLearner",
    "section": "",
    "text": "New set of time series learners with a new sklearn-like API that simplifies the learner creation. The following classes are included:"
  },
  {
    "objectID": "tslearner.html#tsclassifier-api",
    "href": "tslearner.html#tsclassifier-api",
    "title": "TSLearner",
    "section": "TSClassifier API",
    "text": "TSClassifier API\n\nCommonly used arguments:\n\nX: array-like of shape (n_samples, n_steps) or (n_samples, n_features, n_steps) with the input time series samples. Internally, they will be converted to torch tensors.\ny: array-like of shape (n_samples), (n_samples, n_outputs) or (n_samples, n_features, n_outputs) with the target. Internally, they will be converted to torch tensors. Default=None. None is used for unlabeled datasets.\nsplits: lists of indices used to split data between train and validation. Default=None. If no splits are passed, data will be split 100:0 between train and test without shuffling.\ntfms: item transforms that will be applied to each sample individually. Default:None.\nbatch_tfms: transforms applied to each batch. Default=None.\npipelines: store sklearn-type pipelines that can then be applied to pandas dataframes with transform or inverse_transform methods. Default=None.\nbs: batch size (if batch_size is provided then batch_size will override bs). An int or a list of ints can be passed. Default=[64, 128]. If a list of ints, the first one will be used for training, and the second for the valid (batch size can be larger as it doesn’t require backpropagation which consumes more memory).\narch: indicates which architecture will be used. Alternatively, you can pass an instantiated model. Default: InceptionTimePlus.\narch_config: keyword arguments passed to the selected architecture. Default={}.\npretrained: indicates if pretrained model weights will be used. Default=False.\nweights_path: indicates the path to the pretrained weights in case they are used.\nloss_func: allows you to pass any loss function. Default=None (in which case CrossEntropyLossFlat() is applied).\nopt_func: allows you to pass an optimizer. Default=Adam.\nlr: learning rate. Default=0.001.\nmetrics: list of metrics passed to the Learner. Default=accuracy.\ncbs: list of callbacks passed to the Learner. Default=None.\nwd: is the default weight decay used when training the model. Default=None.\n\nLess frequently used arguments:\n\nsel_vars: used to select which of the features in multivariate datasets are used. Default=None means all features are used. If necessary a list-like of indices can be used (eg.[0,3,5]).\nsel_steps: used to select the steps used. Default=None means all steps are used. If necessary a list-like of indices can be used (eg. slice(-50, None) will select the last 50 steps from each time series).\nweights: indicates a sample weight per instance. Used to pass pass a probability to the train dataloader sampler. Samples with more weight will be selected more often during training.\npartial_n: select randomly partial quantity of data at each epoch. Used to reduce the training size (for example for testing purposes). int or float can be used.\nvocab: vocabulary used to transform the target. Only required when transformed is not perform by a dataloader’s tfm (external transforms).\ntrain_metrics: flag used to display metrics in the training set. Defaults to False.\nvalid_metrics: flag used to display metrics in the validtion set. Defaults to True.\ninplace: indicates whether tfms are applied during instantiation or on-the-fly. Default=True, which means that tfms will be applied during instantiation. This results in a faster training, but it can only be used when data fits in memory. Otherwise set it to False.\nshuffle_train: indicates whether to shuffle the training set every time the dataloader is fully read/iterated or not. This doesn’t have an impact on the validation set which is never shuffled. Default=True.\ndrop_last: if True the last incomplete training batch is dropped (thus ensuring training batches of equal size). This doesn’t have an impact on the validation set where samples are never dropped. Default=True.\nnum_workers: num_workers (int): how many subprocesses to use for data loading. 0 means that the data will be loaded in the main process. Default=0.\ndo_setup: ndicates if the Pipeline.setup method should be called during initialization. Default=True.\ndevice: Defaults to default_device() which is CUDA by default. You can specify device as `torch.device(‘cpu’).\nseed: Set to an int to ensure reprodubibility. Default=None.\nverbose: controls the verbosity when fitting and predicting.\nexclude_head: indicates whether the head of the pretrained model needs to be removed or not. Default=True.\ncut: indicates the position where the pretrained model head needs to be cut. Defaults=-1.\ninit: allows you to set to None (no initialization applied), set to True (in which case nn.init.kaiming_normal_ will be applied) or pass an initialization. Default=None.\nsplitter: To do transfer learning, you need to pass a splitter to Learner. This should be a function taking the model and returning a collection of parameter groups, e.g. a list of list of parameters. Default=trainable_params. If the model has a backbone and a head, it will then be split in those 2 groups.\npath and model_dir: are used to save and/or load models. Often path will be inferred from dls, but you can override it or pass a Path object to model_dir.\nwd_bn_bias: controls if weight decay is applied to BatchNorm layers and bias. Default=False. train_bn=True\nmoms: the default momentums used in Learner.fit_one_cycle. Default=(0.95, 0.85, 0.95).\n\n\nsource\n\nTSClassifier\n\n TSClassifier (X, y=None, splits=None, tfms=None, inplace=True,\n               sel_vars=None, sel_steps=None, weights=None,\n               partial_n=None, vocab=None, train_metrics=False,\n               valid_metrics=True, bs=[64, 128], batch_size=None,\n               batch_tfms=None, pipelines=None, shuffle_train=True,\n               drop_last=True, num_workers=0, do_setup=True, device=None,\n               seed=None, arch=None, arch_config={}, pretrained=False,\n               weights_path=None, exclude_head=True, cut=-1, init=None,\n               loss_func=None, opt_func=<function Adam>, lr=0.001,\n               metrics=<function accuracy>, cbs=None, wd=None,\n               wd_bn_bias=False, train_bn=True, moms=(0.95, 0.85, 0.95),\n               path='.', model_dir='models', splitter=<function\n               trainable_params>, verbose=False)\n\nGroup together a model, some dls and a loss_func to handle training\n\nfrom tsai.data.external import *\nfrom tsai.data.preprocessing import *\nfrom tsai.models.InceptionTimePlus import *\n\n\n# With validation split\nX, y, splits = get_classification_data('OliveOil', split_data=False)\ntfms = [None, TSClassification()]\nbatch_tfms = [TSStandardize(by_sample=True)]\nlearn = TSClassifier(X, y, splits=splits, tfms=tfms, batch_tfms=batch_tfms, metrics=accuracy, arch=InceptionTimePlus, arch_config=dict(fc_dropout=.5), \n                     train_metrics=True)\nlearn.fit_one_cycle(1)\n\n\n\n\n\n\n# Without validation split\nX, y, splits = get_classification_data('OliveOil', split_data=False)\nsplits = (splits[0], None)\ntfms = [None, TSClassification()]\nbatch_tfms = [TSStandardize(by_sample=True)]\nlearn = TSClassifier(X, y, splits=splits, tfms=tfms, batch_tfms=batch_tfms, metrics=accuracy, arch=InceptionTimePlus, arch_config=dict(fc_dropout=.5), \n                     train_metrics=True)\nlearn.fit_one_cycle(1)"
  },
  {
    "objectID": "tslearner.html#tsregressor-api",
    "href": "tslearner.html#tsregressor-api",
    "title": "TSLearner",
    "section": "TSRegressor API",
    "text": "TSRegressor API\n\nCommonly used arguments:\n\nX: array-like of shape (n_samples, n_steps) or (n_samples, n_features, n_steps) with the input time series samples. Internally, they will be converted to torch tensors.\ny: array-like of shape (n_samples), (n_samples, n_outputs) or (n_samples, n_features, n_outputs) with the target. Internally, they will be converted to torch tensors. Default=None. None is used for unlabeled datasets.\nsplits: lists of indices used to split data between train and validation. Default=None. If no splits are passed, data will be split 100:0 between train and test without shuffling.\ntfms: item transforms that will be applied to each sample individually. Default=None.\nbatch_tfms: transforms applied to each batch. Default=None.\npipelines: store sklearn-type pipelines that can then be applied to pandas dataframes with transform or inverse_transform methods. Default=None.\nbs: batch size (if batch_size is provided then batch_size will override bs). An int or a list of ints can be passed. Default=[64, 128]. If a list of ints, the first one will be used for training, and the second for the valid (batch size can be larger as it doesn’t require backpropagation which consumes more memory).\narch: indicates which architecture will be used. Alternatively, you can pass an instantiated model. Default: InceptionTimePlus.\narch_config: keyword arguments passed to the selected architecture. Default={}.\npretrained: indicates if pretrained model weights will be used. Default=False.\nweights_path: indicates the path to the pretrained weights in case they are used.\nloss_func: allows you to pass any loss function. Default=None (in which case CrossEntropyLossFlat() is applied).\nopt_func: allows you to pass an optimizer. Default=Adam.\nlr: learning rate. Default=0.001.\nmetrics: list of metrics passed to the Learner. Default=None.\ncbs: list of callbacks passed to the Learner. Default=None.\nwd: is the default weight decay used when training the model. Default=None.\n\nLess frequently used arguments:\n\nsel_vars: used to select which of the features in multivariate datasets are used. Default=None means all features are used. If necessary a list-like of indices can be used (eg.[0,3,5]).\nsel_steps: used to select the steps used. Default=None means all steps are used. If necessary a list-like of indices can be used (eg. slice(-50, None) will select the last 50 steps from each time series).\nweights: indicates a sample weight per instance. Used to pass pass a probability to the train dataloader sampler. Samples with more weight will be selected more often during training.\npartial_n: select randomly partial quantity of data at each epoch. Used to reduce the training size (for example for testing purposes). int or float can be used.\ntrain_metrics: flag used to display metrics in the training set. Defaults to False.\nvalid_metrics: flag used to display metrics in the validtion set. Defaults to True.\ninplace: indicates whether tfms are applied during instantiation or on-the-fly. Default=True, which means that tfms will be applied during instantiation. This results in a faster training, but it can only be used when data fits in memory. Otherwise set it to False.\nshuffle_train: indicates whether to shuffle the training set every time the dataloader is fully read/iterated or not. This doesn’t have an impact on the validation set which is never shuffled. Default=True.\ndrop_last: if True the last incomplete training batch is dropped (thus ensuring training batches of equal size). This doesn’t have an impact on the validation set where samples are never dropped. Default=True.\nnum_workers: num_workers (int): how many subprocesses to use for data loading. 0 means that the data will be loaded in the main process. Default=0.\ndo_setup: ndicates if the Pipeline.setup method should be called during initialization. Default=True.\ndevice: Defaults to default_device() which is CUDA by default. You can specify device as `torch.device(‘cpu’).\nseed: Set to an int to ensure reprodubibility. Default=None.\nverbose: controls the verbosity when fitting and predicting.\nexclude_head: indicates whether the head of the pretrained model needs to be removed or not. Default=True.\ncut: indicates the position where the pretrained model head needs to be cut. Defaults=-1.\ninit: allows you to set to None (no initialization applied), set to True (in which case nn.init.kaiming_normal_ will be applied) or pass an initialization. Default=None.\nsplitter: To do transfer learning, you need to pass a splitter to Learner. This should be a function taking the model and returning a collection of parameter groups, e.g. a list of list of parameters. Default=trainable_params. If the model has a backbone and a head, it will then be split in those 2 groups.\npath and model_dir: are used to save and/or load models. Often path will be inferred from dls, but you can override it or pass a Path object to model_dir.\nwd_bn_bias: controls if weight decay is applied to BatchNorm layers and bias. Default=False. train_bn=True\nmoms: the default momentums used in Learner.fit_one_cycle. Default=(0.95, 0.85, 0.95).\n\n\nsource\n\nTSRegressor\n\n TSRegressor (X, y=None, splits=None, tfms=None, inplace=True,\n              sel_vars=None, sel_steps=None, weights=None, partial_n=None,\n              train_metrics=False, valid_metrics=True, bs=[64, 128],\n              batch_size=None, batch_tfms=None, pipelines=None,\n              shuffle_train=True, drop_last=True, num_workers=0,\n              do_setup=True, device=None, seed=None, arch=None,\n              arch_config={}, pretrained=False, weights_path=None,\n              exclude_head=True, cut=-1, init=None, loss_func=None,\n              opt_func=<function Adam>, lr=0.001, metrics=None, cbs=None,\n              wd=None, wd_bn_bias=False, train_bn=True, moms=(0.95, 0.85,\n              0.95), path='.', model_dir='models', splitter=<function\n              trainable_params>, verbose=False)\n\nGroup together a model, some dls and a loss_func to handle training\n\nX, y, splits = get_regression_data('AppliancesEnergy', split_data=False)\nif X is not None: # This is to prevent a test fail when the data server is not available\n    batch_tfms = [TSStandardize()]\n    learn = TSRegressor(X, y, splits=splits, batch_tfms=batch_tfms, arch=None, metrics=mae, bs=512, train_metrics=True)\n    learn.fit_one_cycle(1, 1e-4)"
  },
  {
    "objectID": "tslearner.html#tsforecaster-api",
    "href": "tslearner.html#tsforecaster-api",
    "title": "TSLearner",
    "section": "TSForecaster API",
    "text": "TSForecaster API\n\nCommonly used arguments:\n\nX: array-like of shape (n_samples, n_steps) or (n_samples, n_features, n_steps) with the input time series samples. Internally, they will be converted to torch tensors.\ny: array-like of shape (n_samples), (n_samples, n_outputs) or (n_samples, n_features, n_outputs) with the target. Internally, they will be converted to torch tensors. Default=None. None is used for unlabeled datasets.\nsplits: lists of indices used to split data between train and validation. Default=None. If no splits are passed, data will be split 100:0 between train and test without shuffling.\ntfms: item transforms that will be applied to each sample individually. Default=None.\nbatch_tfms: transforms applied to each batch. Default=None.\npipelines: store sklearn-type pipelines that can then be applied to pandas dataframes with transform or inverse_transform methods. Default=None.\nbs: batch size (if batch_size is provided then batch_size will override bs). An int or a list of ints can be passed. Default=[64, 128]. If a list of ints, the first one will be used for training, and the second for the valid (batch size can be larger as it doesn’t require backpropagation which consumes more memory).\narch: indicates which architecture will be used. Alternatively, you can pass an instantiated model. Default: InceptionTimePlus.\narch_config: keyword arguments passed to the selected architecture. Default={}.\npretrained: indicates if pretrained model weights will be used. Default=False.\nweights_path: indicates the path to the pretrained weights in case they are used.\nloss_func: allows you to pass any loss function. Default=None (in which case CrossEntropyLossFlat() is applied).\nopt_func: allows you to pass an optimizer. Default=Adam.\nlr: learning rate. Default=0.001.\nmetrics: list of metrics passed to the Learner. Default=None.\ncbs: list of callbacks passed to the Learner. Default=None.\nwd: is the default weight decay used when training the model. Default=None.\n\nLess frequently used arguments:\n\nsel_vars: used to select which of the features in multivariate datasets are used. Default=None means all features are used. If necessary a list-like of indices can be used (eg.[0,3,5]).\nsel_steps: used to select the steps used. Default=None means all steps are used. If necessary a list-like of indices can be used (eg. slice(-50, None) will select the last 50 steps from each time series).\nweights: indicates a sample weight per instance. Used to pass pass a probability to the train dataloader sampler. Samples with more weight will be selected more often during training.\npartial_n: select randomly partial quantity of data at each epoch. Used to reduce the training size (for example for testing purposes). int or float can be used.\ntrain_metrics: flag used to display metrics in the training set. Defaults to False.\nvalid_metrics: flag used to display metrics in the validtion set. Defaults to True.\ninplace: indicates whether tfms are applied during instantiation or on-the-fly. Default=True, which means that tfms will be applied during instantiation. This results in a faster training, but it can only be used when data fits in memory. Otherwise set it to False.\nshuffle_train: indicates whether to shuffle the training set every time the dataloader is fully read/iterated or not. This doesn’t have an impact on the validation set which is never shuffled. Default=True.\ndrop_last: if True the last incomplete training batch is dropped (thus ensuring training batches of equal size). This doesn’t have an impact on the validation set where samples are never dropped. Default=True.\nnum_workers: num_workers (int): how many subprocesses to use for data loading. 0 means that the data will be loaded in the main process. Default=None.\ndo_setup: ndicates if the Pipeline.setup method should be called during initialization. Default=True.\ndevice: Defaults to default_device() which is CUDA by default. You can specify device as `torch.device(‘cpu’).\nseed: Set to an int to ensure reprodubibility. Default=None.\nverbose: controls the verbosity when fitting and predicting.\nexclude_head: indicates whether the head of the pretrained model needs to be removed or not. Default=True.\ncut: indicates the position where the pretrained model head needs to be cut. Defaults=-1.\ninit: allows you to set to None (no initialization applied), set to True (in which case nn.init.kaiming_normal_ will be applied) or pass an initialization. Default=None.\nsplitter: To do transfer learning, you need to pass a splitter to Learner. This should be a function taking the model and returning a collection of parameter groups, e.g. a list of list of parameters. Default=trainable_params. If the model has a backbone and a head, it will then be split in those 2 groups.\npath and model_dir: are used to save and/or load models. Often path will be inferred from dls, but you can override it or pass a Path object to model_dir.\nwd_bn_bias: controls if weight decay is applied to BatchNorm layers and bias. Default=False. train_bn=True\nmoms: the default momentums used in Learner.fit_one_cycle. Default=(0.95, 0.85, 0.95).\n\n\nsource\n\nTSForecaster\n\n TSForecaster (X, y=None, splits=None, tfms=None, inplace=True,\n               sel_vars=None, sel_steps=None, weights=None,\n               partial_n=None, train_metrics=False, valid_metrics=True,\n               bs=[64, 128], batch_size=None, batch_tfms=None,\n               pipelines=None, shuffle_train=True, drop_last=True,\n               num_workers=0, do_setup=True, device=None, seed=None,\n               arch=None, arch_config={}, pretrained=False,\n               weights_path=None, exclude_head=True, cut=-1, init=None,\n               loss_func=None, opt_func=<function Adam>, lr=0.001,\n               metrics=None, cbs=None, wd=None, wd_bn_bias=False,\n               train_bn=True, moms=(0.95, 0.85, 0.95), path='.',\n               model_dir='models', splitter=<function trainable_params>,\n               verbose=False)\n\nGroup together a model, some dls and a loss_func to handle training\n\nfrom tsai.data.preparation import *\n\n\nts = get_forecasting_time_series('Sunspots')\nif ts is not None: # This is to prevent a test fail when the data server is not available\n    X, y = SlidingWindowSplitter(60, horizon=1)(ts)\n    splits = TSSplitter(235)(y)\n    batch_tfms = [TSStandardize(by_var=True)]\n    learn = TSForecaster(X, y, splits=splits, batch_tfms=batch_tfms, arch=None, arch_config=dict(fc_dropout=.5), metrics=mae, bs=512, \n                         partial_n=.1, train_metrics=True)\n    learn.fit_one_cycle(1)\n\nDataset: Sunspots\ndownloading data...\n...done. Path = data/forecasting/Sunspots.csv"
  },
  {
    "objectID": "models.multiinputnet.html",
    "href": "models.multiinputnet.html",
    "title": "MultiInputNet",
    "section": "",
    "text": "This is an implementation created by Ignacio Oguiza (oguiza@timeseriesAI.co).\nIt can be used to combine different types of deep learning models into a single one that will accept multiple inputs from a MixedDataLoaders.\n\nsource\n\nMultiInputNet\n\n MultiInputNet (*models, c_out=None, reshape_fn=None, multi_output=False,\n                custom_head=None, device=None, **kwargs)\n\nSame as nn.Module, but no need for subclasses to call super().__init__\n\nfrom tsai.basics import *\nfrom tsai.data.all import *\nfrom tsai.models.utils import *\nfrom tsai.models.InceptionTimePlus import *\nfrom tsai.models.TabModel import *\n\n\ndsid = 'NATOPS'\nX, y, splits = get_UCR_data(dsid, split_data=False)\nts_features_df = get_ts_features(X, y)\n\nFeature Extraction: 100%|███████████████████████████████████████████| 40/40 [00:07<00:00,  5.23it/s]\n\n\n\n# raw ts\ntfms  = [None, [TSCategorize()]]\nbatch_tfms = TSStandardize()\nts_dls = get_ts_dls(X, y, splits=splits, tfms=tfms, batch_tfms=batch_tfms)\nts_model = build_ts_model(InceptionTimePlus, dls=ts_dls)\n\n# ts features\ncat_names = None\ncont_names = ts_features_df.columns[:-2]\ny_names = 'target'\ntab_dls = get_tabular_dls(ts_features_df, cat_names=cat_names, cont_names=cont_names, y_names=y_names, splits=splits)\ntab_model = build_tabular_model(TabModel, dls=tab_dls)\n\n# mixed\nmixed_dls = get_mixed_dls(ts_dls, tab_dls)\nMultiModalNet = MultiInputNet(ts_model, tab_model)\nlearn = Learner(mixed_dls, MultiModalNet, metrics=[accuracy, RocAuc()])\nlearn.fit_one_cycle(1, 1e-3)\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      accuracy\n      roc_auc_score\n      time\n    \n  \n  \n    \n      0\n      1.780674\n      1.571718\n      0.477778\n      0.857444\n      00:05\n    \n  \n\n\n\n\n(ts, (cat, cont)),yb = mixed_dls.one_batch()\nlearn.model((ts, (cat, cont))).shape\n\ntorch.Size([64, 6])\n\n\n\ntab_dls.c, ts_dls.c, ts_dls.cat\n\n(6, 6, True)\n\n\n\nlearn.loss_func\n\nFlattenedLoss of CrossEntropyLoss()"
  },
  {
    "objectID": "models.layers.html",
    "href": "models.layers.html",
    "title": "Layers",
    "section": "",
    "text": "Helper functions used to build PyTorch timeseries models.\n\n\nsource\n\ntest_module_to_torchscript\n\n test_module_to_torchscript (m:torch.nn.modules.module.Module,\n                             inputs:torch.Tensor, trace:bool=True,\n                             script:bool=True, serialize:bool=True,\n                             verbose:bool=True)\n\nTests if a PyTorch module can be correctly traced or scripted and serialized\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nm\nModule\n\nThe PyTorch module to be tested.\n\n\ninputs\nTensor\n\nA tensor or tuple of tensors representing the inputs to the model.\n\n\ntrace\nbool\nTrue\nIf True, attempts to trace the model. Defaults to True.\n\n\nscript\nbool\nTrue\nIf True, attempts to script the model. Defaults to True.\n\n\nserialize\nbool\nTrue\nIf True, saves and loads the traced/scripted module to ensure it can be serialized. Defaults to True.\n\n\nverbose\nbool\nTrue\nIf True, prints detailed information about the tracing and scripting process. Defaults to True.\n\n\n\n\nm = nn.Linear(10, 2)\ninp = torch.randn(3, 10)\ntest_module_to_torchscript(m, inp, trace=True, script=True, serialize=True, verbose=True)\n\noutput.shape: torch.Size([3, 2])\nTracing...\n...Linear has been successfully traced 😃\n\n\n\nTrue\n\n\n\nsource\n\n\ninit_lin_zero\n\n init_lin_zero (m)\n\n\nsource\n\n\nSwishBeta\n\n SwishBeta ()\n\nSame as nn.Module, but no need for subclasses to call super().__init__\n\nsource\n\n\nSmeLU\n\n SmeLU (beta:float=2.0)\n\nSmooth ReLU activation function based on https://arxiv.org/pdf/2202.06499.pdf\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nbeta\nfloat\n2.0\nBeta value\n\n\nReturns\nNone\n\n\n\n\n\n\nsource\n\n\nChomp1d\n\n Chomp1d (chomp_size)\n\nBase class for all neural network modules.\nYour models should also subclass this class.\nModules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes::\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 20, 5)\n        self.conv2 = nn.Conv2d(20, 20, 5)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        return F.relu(self.conv2(x))\nSubmodules assigned in this way will be registered, and will have their parameters converted too when you call :meth:to, etc.\n.. note:: As per the example above, an __init__() call to the parent class must be made before assignment on the child.\n:ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool\n\nsource\n\n\nSameConv1d\n\n SameConv1d (ni, nf, ks=3, stride=1, dilation=1, **kwargs)\n\nConv1d with padding=‘same’\n\nsource\n\n\nPad1d\n\n Pad1d (padding, value=0.0)\n\nPads the input tensor boundaries with a constant value.\nFor N-dimensional padding, use :func:torch.nn.functional.pad().\nArgs: padding (int, tuple): the size of the padding. If is int, uses the same padding in both boundaries. If a 2-tuple, uses (:math:\\text{padding\\_left}, :math:\\text{padding\\_right})\nShape: - Input: :math:(C, W_{in}) or :math:(N, C, W_{in}). - Output: :math:(C, W_{out}) or :math:(N, C, W_{out}), where\n  :math:`W_{out} = W_{in} + \\text{padding\\_left} + \\text{padding\\_right}`\nExamples::\n>>> # xdoctest: +IGNORE_WANT(\"non-deterministic\")\n>>> m = nn.ConstantPad1d(2, 3.5)\n>>> input = torch.randn(1, 2, 4)\n>>> input\ntensor([[[-1.0491, -0.7152, -0.0749,  0.8530],\n         [-1.3287,  1.8966,  0.1466, -0.2771]]])\n>>> m(input)\ntensor([[[ 3.5000,  3.5000, -1.0491, -0.7152, -0.0749,  0.8530,  3.5000,\n           3.5000],\n         [ 3.5000,  3.5000, -1.3287,  1.8966,  0.1466, -0.2771,  3.5000,\n           3.5000]]])\n>>> m = nn.ConstantPad1d(2, 3.5)\n>>> input = torch.randn(1, 2, 3)\n>>> input\ntensor([[[ 1.6616,  1.4523, -1.1255],\n         [-3.6372,  0.1182, -1.8652]]])\n>>> m(input)\ntensor([[[ 3.5000,  3.5000,  1.6616,  1.4523, -1.1255,  3.5000,  3.5000],\n         [ 3.5000,  3.5000, -3.6372,  0.1182, -1.8652,  3.5000,  3.5000]]])\n>>> # using different paddings for different sides\n>>> m = nn.ConstantPad1d((3, 1), 3.5)\n>>> m(input)\ntensor([[[ 3.5000,  3.5000,  3.5000,  1.6616,  1.4523, -1.1255,  3.5000],\n         [ 3.5000,  3.5000,  3.5000, -3.6372,  0.1182, -1.8652,  3.5000]]])\n\nsource\n\n\nsame_padding1d\n\n same_padding1d (seq_len, ks, stride=1, dilation=1)\n\nSame padding formula as used in Tensorflow\n\nsource\n\n\nConv2d\n\n Conv2d (ni, nf, kernel_size=None, ks=None, stride=1, padding='same',\n         dilation=1, init='auto', bias_std=0.01, **kwargs)\n\nconv1d layer with padding=‘same’, ‘valid’, or any integer (defaults to ‘same’)\n\nsource\n\n\nConv2dSame\n\n Conv2dSame (ni, nf, ks=(3, 3), stride=(1, 1), dilation=(1, 1), **kwargs)\n\nConv2d with padding=‘same’\n\nsource\n\n\nPad2d\n\n Pad2d (padding, value=0.0)\n\nPads the input tensor boundaries with a constant value.\nFor N-dimensional padding, use :func:torch.nn.functional.pad().\nArgs: padding (int, tuple): the size of the padding. If is int, uses the same padding in all boundaries. If a 4-tuple, uses (:math:\\text{padding\\_left}, :math:\\text{padding\\_right}, :math:\\text{padding\\_top}, :math:\\text{padding\\_bottom})\nShape: - Input: :math:(N, C, H_{in}, W_{in}) or :math:(C, H_{in}, W_{in}). - Output: :math:(N, C, H_{out}, W_{out}) or :math:(C, H_{out}, W_{out}), where\n  :math:`H_{out} = H_{in} + \\text{padding\\_top} + \\text{padding\\_bottom}`\n\n  :math:`W_{out} = W_{in} + \\text{padding\\_left} + \\text{padding\\_right}`\nExamples::\n>>> # xdoctest: +IGNORE_WANT(\"non-deterministic\")\n>>> m = nn.ConstantPad2d(2, 3.5)\n>>> input = torch.randn(1, 2, 2)\n>>> input\ntensor([[[ 1.6585,  0.4320],\n         [-0.8701, -0.4649]]])\n>>> m(input)\ntensor([[[ 3.5000,  3.5000,  3.5000,  3.5000,  3.5000,  3.5000],\n         [ 3.5000,  3.5000,  3.5000,  3.5000,  3.5000,  3.5000],\n         [ 3.5000,  3.5000,  1.6585,  0.4320,  3.5000,  3.5000],\n         [ 3.5000,  3.5000, -0.8701, -0.4649,  3.5000,  3.5000],\n         [ 3.5000,  3.5000,  3.5000,  3.5000,  3.5000,  3.5000],\n         [ 3.5000,  3.5000,  3.5000,  3.5000,  3.5000,  3.5000]]])\n>>> # using different paddings for different sides\n>>> m = nn.ConstantPad2d((3, 0, 2, 1), 3.5)\n>>> m(input)\ntensor([[[ 3.5000,  3.5000,  3.5000,  3.5000,  3.5000],\n         [ 3.5000,  3.5000,  3.5000,  3.5000,  3.5000],\n         [ 3.5000,  3.5000,  3.5000,  1.6585,  0.4320],\n         [ 3.5000,  3.5000,  3.5000, -0.8701, -0.4649],\n         [ 3.5000,  3.5000,  3.5000,  3.5000,  3.5000]]])\n\nsource\n\n\nsame_padding2d\n\n same_padding2d (H, W, ks, stride=(1, 1), dilation=(1, 1))\n\nSame padding formula as used in Tensorflow\n\nbs = 2\nc_in = 3\nc_out = 5\nh = 16\nw = 20\nt = torch.rand(bs, c_in, h, w)\ntest_eq(Conv2dSame(c_in, c_out, ks=3, stride=1, dilation=1, bias=False)(t).shape, (bs, c_out, h, w))\ntest_eq(Conv2dSame(c_in, c_out, ks=(3, 1), stride=1, dilation=1, bias=False)(t).shape, (bs, c_out, h, w))\ntest_eq(Conv2dSame(c_in, c_out, ks=3, stride=(1, 1), dilation=(2, 2), bias=False)(t).shape, (bs, c_out, h, w))\ntest_eq(Conv2dSame(c_in, c_out, ks=3, stride=(2, 2), dilation=(1, 1), bias=False)(t).shape, (bs, c_out, h//2, w//2))\ntest_eq(Conv2dSame(c_in, c_out, ks=3, stride=(2, 2), dilation=(2, 2), bias=False)(t).shape, (bs, c_out, h//2, w//2))\ntest_eq(Conv2d(c_in, c_out, ks=3, padding='same', stride=1, dilation=1, bias=False)(t).shape, (bs, c_out, h, w))\n\n\nsource\n\n\nCausalConv1d\n\n CausalConv1d (ni, nf, ks, stride=1, dilation=1, groups=1, bias=True)\n\nApplies a 1D convolution over an input signal composed of several input planes.\nIn the simplest case, the output value of the layer with input size :math:(N, C_{\\text{in}}, L) and output :math:(N, C_{\\text{out}}, L_{\\text{out}}) can be precisely described as:\n.. math:: (N_i, C_{j}) = (C{j}) + {k = 0}^{C_{in} - 1} (C_{_j}, k) (N_i, k)\nwhere :math:\\star is the valid cross-correlation_ operator, :math:N is a batch size, :math:C denotes a number of channels, :math:L is a length of signal sequence.\nThis module supports :ref:TensorFloat32<tf32_on_ampere>.\nOn certain ROCm devices, when using float16 inputs this module will use :ref:different precision<fp16_on_mi200> for backward.\n\n:attr:stride controls the stride for the cross-correlation, a single number or a one-element tuple.\n:attr:padding controls the amount of padding applied to the input. It can be either a string {‘valid’, ‘same’} or a tuple of ints giving the amount of implicit padding applied on both sides.\n:attr:dilation controls the spacing between the kernel points; also known as the à trous algorithm. It is harder to describe, but this link_ has a nice visualization of what :attr:dilation does.\n:attr:groups controls the connections between inputs and outputs. :attr:in_channels and :attr:out_channels must both be divisible by :attr:groups. For example,\n\nAt groups=1, all inputs are convolved to all outputs.\nAt groups=2, the operation becomes equivalent to having two conv layers side by side, each seeing half the input channels and producing half the output channels, and both subsequently concatenated.\nAt groups= :attr:in_channels, each input channel is convolved with its own set of filters (of size :math:\\frac{\\text{out\\_channels}}{\\text{in\\_channels}}).\n\n\nNote: When groups == in_channels and out_channels == K * in_channels, where K is a positive integer, this operation is also known as a “depthwise convolution”.\nIn other words, for an input of size :math:`(N, C_{in}, L_{in})`,\na depthwise convolution with a depthwise multiplier `K` can be performed with the arguments\n:math:`(C_\\text{in}=C_\\text{in}, C_\\text{out}=C_\\text{in} \\times \\text{K}, ..., \\text{groups}=C_\\text{in})`.\nNote: In some circumstances when given tensors on a CUDA device and using CuDNN, this operator may select a nondeterministic algorithm to increase performance. If this is undesirable, you can try to make the operation deterministic (potentially at a performance cost) by setting torch.backends.cudnn.deterministic = True. See :doc:/notes/randomness for more information.\nNote: padding='valid' is the same as no padding. padding='same' pads the input so the output has the shape as the input. However, this mode doesn’t support any stride values other than 1.\nNote: This module supports complex data types i.e. complex32, complex64, complex128.\nArgs: in_channels (int): Number of channels in the input image out_channels (int): Number of channels produced by the convolution kernel_size (int or tuple): Size of the convolving kernel stride (int or tuple, optional): Stride of the convolution. Default: 1 padding (int, tuple or str, optional): Padding added to both sides of the input. Default: 0 padding_mode (str, optional): 'zeros', 'reflect', 'replicate' or 'circular'. Default: 'zeros' dilation (int or tuple, optional): Spacing between kernel elements. Default: 1 groups (int, optional): Number of blocked connections from input channels to output channels. Default: 1 bias (bool, optional): If True, adds a learnable bias to the output. Default: True\nShape: - Input: :math:(N, C_{in}, L_{in}) or :math:(C_{in}, L_{in}) - Output: :math:(N, C_{out}, L_{out}) or :math:(C_{out}, L_{out}), where\n  .. math::\n      L_{out} = \\left\\lfloor\\frac{L_{in} + 2 \\times \\text{padding} - \\text{dilation}\n                \\times (\\text{kernel\\_size} - 1) - 1}{\\text{stride}} + 1\\right\\rfloor\nAttributes: weight (Tensor): the learnable weights of the module of shape :math:(\\text{out\\_channels},         \\frac{\\text{in\\_channels}}{\\text{groups}}, \\text{kernel\\_size}). The values of these weights are sampled from :math:\\mathcal{U}(-\\sqrt{k}, \\sqrt{k}) where :math:k = \\frac{groups}{C_\\text{in} * \\text{kernel\\_size}} bias (Tensor): the learnable bias of the module of shape (out_channels). If :attr:bias is True, then the values of these weights are sampled from :math:\\mathcal{U}(-\\sqrt{k}, \\sqrt{k}) where :math:k = \\frac{groups}{C_\\text{in} * \\text{kernel\\_size}}\nExamples::\n>>> m = nn.Conv1d(16, 33, 3, stride=2)\n>>> input = torch.randn(20, 16, 50)\n>>> output = m(input)\n.. _cross-correlation: https://en.wikipedia.org/wiki/Cross-correlation\n.. _link: https://github.com/vdumoulin/conv_arithmetic/blob/master/README.md\n\nsource\n\n\nConv1d\n\n Conv1d (ni, nf, kernel_size=None, ks=None, stride=1, padding='same',\n         dilation=1, init='auto', bias_std=0.01, **kwargs)\n\nconv1d layer with padding=‘same’, ‘causal’, ‘valid’, or any integer (defaults to ‘same’)\n\nbs = 2\nc_in = 3\nc_out = 5\nseq_len = 512\nt = torch.rand(bs, c_in, seq_len)\ndilation = 1\ntest_eq(CausalConv1d(c_in, c_out, ks=3, dilation=dilation)(t).shape, Conv1d(c_in, c_out, ks=3, padding=\"same\", dilation=dilation)(t).shape)\ndilation = 2\ntest_eq(CausalConv1d(c_in, c_out, ks=3, dilation=dilation)(t).shape, Conv1d(c_in, c_out, ks=3, padding=\"same\", dilation=dilation)(t).shape)\n\n\nbs = 2\nni = 3\nnf = 5\nseq_len = 6\nks = 3\nt = torch.rand(bs, c_in, seq_len)\ntest_eq(Conv1d(ni, nf, ks, padding=0)(t).shape, (bs, c_out, seq_len - (2 * (ks//2))))\ntest_eq(Conv1d(ni, nf, ks, padding='valid')(t).shape, (bs, c_out, seq_len - (2 * (ks//2))))\ntest_eq(Conv1d(ni, nf, ks, padding='same')(t).shape, (bs, c_out, seq_len))\ntest_eq(Conv1d(ni, nf, ks, padding='causal')(t).shape, (bs, c_out, seq_len))\ntest_error('use kernel_size or ks but not both simultaneously', Conv1d, ni, nf, kernel_size=3, ks=3)\ntest_error('you need to pass a ks', Conv1d, ni, nf)\n\n\nconv = Conv1d(ni, nf, ks, padding='same')\ninit_linear(conv, None, init='auto', bias_std=.01)\nconv\n\nConv1d(3, 5, kernel_size=(3,), stride=(1,), padding=(1,))\n\n\n\nconv = Conv1d(ni, nf, ks, padding='causal')\ninit_linear(conv, None, init='auto', bias_std=.01)\nconv\n\nCausalConv1d(3, 5, kernel_size=(3,), stride=(1,))\n\n\n\nconv = Conv1d(ni, nf, ks, padding='valid')\ninit_linear(conv, None, init='auto', bias_std=.01)\nweight_norm(conv)\nconv\n\nConv1d(3, 5, kernel_size=(3,), stride=(1,))\n\n\n\nconv = Conv1d(ni, nf, ks, padding=0)\ninit_linear(conv, None, init='auto', bias_std=.01)\nweight_norm(conv)\nconv\n\nConv1d(3, 5, kernel_size=(3,), stride=(1,))\n\n\n\nsource\n\n\nSeparableConv1d\n\n SeparableConv1d (ni, nf, ks, stride=1, padding='same', dilation=1,\n                  bias=True, bias_std=0.01)\n\nSame as nn.Module, but no need for subclasses to call super().__init__\n\nbs = 64\nc_in = 6\nc_out = 5\nseq_len = 512\nt = torch.rand(bs, c_in, seq_len)\ntest_eq(SeparableConv1d(c_in, c_out, 3)(t).shape, (bs, c_out, seq_len))\n\n\nsource\n\n\nAddCoords1d\n\n AddCoords1d ()\n\nAdd coordinates to ease position identification without modifying mean and std\n\nbs = 2\nc_in = 3\nc_out = 5\nseq_len = 50\n\nt = torch.rand(bs, c_in, seq_len)\nt = (t - t.mean()) / t.std()\ntest_eq(AddCoords1d()(t).shape, (bs, c_in + 1, seq_len))\nnew_t = AddCoords1d()(t)\ntest_close(new_t.mean(),0, 1e-2)\ntest_close(new_t.std(), 1, 1e-2)\n\n\nsource\n\n\nConvBlock\n\n ConvBlock (ni, nf, kernel_size=None, ks=3, stride=1, padding='same',\n            bias=None, bias_std=0.01, norm='Batch', zero_norm=False,\n            bn_1st=True, act=<class 'torch.nn.modules.activation.ReLU'>,\n            act_kwargs={}, init='auto', dropout=0.0, xtra=None,\n            coord=False, separable=False, **kwargs)\n\nCreate a sequence of conv1d (ni to nf), activation (if act_cls) and norm_type layers.\n\nsource\n\n\nResBlock1dPlus\n\n ResBlock1dPlus (expansion, ni, nf, coord=False, stride=1, groups=1,\n                 reduction=None, nh1=None, nh2=None, dw=False, g2=1,\n                 sa=False, sym=False, norm='Batch', zero_norm=True,\n                 act_cls=<class 'torch.nn.modules.activation.ReLU'>, ks=3,\n                 pool=<function AvgPool>, pool_first=True, **kwargs)\n\nResnet block from ni to nh with stride\n\nsource\n\n\nSEModule1d\n\n SEModule1d (ni, reduction=16, act=<class\n             'torch.nn.modules.activation.ReLU'>, act_kwargs={})\n\nSqueeze and excitation module for 1d\n\nt = torch.rand(8, 32, 12)\ntest_eq(SEModule1d(t.shape[1], 16, act=nn.ReLU, act_kwargs={})(t).shape, t.shape)\n\n\nsource\n\n\nNorm\n\n Norm (nf, ndim=1, norm='Batch', zero_norm=False, init=True, **kwargs)\n\nNorm layer with nf features and ndim with auto init.\n\nbs = 2\nni = 3\nnf = 5\nsl = 4\nks = 5\n\nt = torch.rand(bs, ni, sl)\ntest_eq(ConvBlock(ni, nf, ks)(t).shape, (bs, nf, sl))\ntest_eq(ConvBlock(ni, nf, ks, padding='causal')(t).shape, (bs, nf, sl))\ntest_eq(ConvBlock(ni, nf, ks, coord=True)(t).shape, (bs, nf, sl))\n\n\ntest_eq(BN1d(ni)(t).shape, (bs, ni, sl))\ntest_eq(BN1d(ni).weight.data.mean().item(), 1.)\ntest_eq(BN1d(ni, zero_norm=True).weight.data.mean().item(), 0.)\n\n\ntest_eq(ConvBlock(ni, nf, ks, norm='batch', zero_norm=True)[1].weight.data.unique().item(), 0)\ntest_ne(ConvBlock(ni, nf, ks, norm='batch', zero_norm=False)[1].weight.data.unique().item(), 0)\ntest_eq(ConvBlock(ni, nf, ks, bias=False)[0].bias, None)\nConvBlock(ni, nf, ks, act=Swish, coord=True)\n\nConvBlock(\n  (0): AddCoords1d()\n  (1): Conv1d(4, 5, kernel_size=(5,), stride=(1,), padding=(2,), bias=False)\n  (2): BatchNorm1d(5, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (3): Swish()\n)\n\n\n\nsource\n\n\nLinLnDrop\n\n LinLnDrop (n_in, n_out, ln=True, p=0.0, act=None, lin_first=False)\n\nModule grouping LayerNorm1d, Dropout and Linear layers\n\nLinLnDrop(2, 3, p=.5)\n\nLinLnDrop(\n  (0): LayerNorm((2,), eps=1e-05, elementwise_affine=True)\n  (1): Dropout(p=0.5, inplace=False)\n  (2): Linear(in_features=2, out_features=3, bias=False)\n)\n\n\n\nsource\n\n\nLambdaPlus\n\n LambdaPlus (func, *args, **kwargs)\n\nSame as nn.Module, but no need for subclasses to call super().__init__\n\nsource\n\n\nReZero\n\n ReZero (module)\n\nSame as nn.Module, but no need for subclasses to call super().__init__\n\nsource\n\n\nClip\n\n Clip (min=None, max=None)\n\nSame as nn.Module, but no need for subclasses to call super().__init__\n\nsource\n\n\nClamp\n\n Clamp (min=None, max=None)\n\nSame as nn.Module, but no need for subclasses to call super().__init__\n\nsource\n\n\nSoftMax\n\n SoftMax (dim=-1)\n\nSoftMax layer\n\nsource\n\n\nLastStep\n\n LastStep ()\n\nSame as nn.Module, but no need for subclasses to call super().__init__\n\nsource\n\n\nMax\n\n Max (dim=None, keepdim=False)\n\nSame as nn.Module, but no need for subclasses to call super().__init__\n\nsource\n\n\nReshape\n\n Reshape (*shape)\n\nSame as nn.Module, but no need for subclasses to call super().__init__\n\nsource\n\n\nView\n\n View (*shape)\n\nSame as nn.Module, but no need for subclasses to call super().__init__\n\nsource\n\n\nTranspose\n\n Transpose (*dims, contiguous=False)\n\nSame as nn.Module, but no need for subclasses to call super().__init__\n\nsource\n\n\nPermute\n\n Permute (*dims)\n\nSame as nn.Module, but no need for subclasses to call super().__init__\n\nsource\n\n\nUnfold\n\n Unfold (dim, size, step=1)\n\nSame as nn.Module, but no need for subclasses to call super().__init__\n\nsource\n\n\nConcat\n\n Concat (dim=1)\n\nSame as nn.Module, but no need for subclasses to call super().__init__\n\nsource\n\n\nAdd\n\n Add ()\n\nSame as nn.Module, but no need for subclasses to call super().__init__\n\nsource\n\n\nUnsqueeze\n\n Unsqueeze (dim=-1)\n\nSame as nn.Module, but no need for subclasses to call super().__init__\n\nsource\n\n\nSqueeze\n\n Squeeze (dim=-1)\n\nSame as nn.Module, but no need for subclasses to call super().__init__\n\nbs = 2\nnf = 5\nsl = 4\n\nt = torch.rand(bs, nf, sl)\ntest_eq(Permute(0,2,1)(t).shape, (bs, sl, nf))\ntest_eq(Max(1)(t).shape, (bs, sl))\ntest_eq(Transpose(1,2)(t).shape, (bs, sl, nf))\ntest_eq(Transpose(1,2, contiguous=True)(t).shape, (bs, sl, nf))\ntest_eq(View(-1, 2, 10)(t).shape, (bs, 1, 2, 10))\ntest_eq(Reshape(-1, 2, 10)(t).shape, (bs, 1, 2, 10))\ntest_eq(Reshape()(t).shape, (2, 20))\ntest_eq(Reshape(-1)(t).shape, (40,))\nTranspose(1,2), Permute(0,2,1), View(-1, 2, 10), Transpose(1,2, contiguous=True), Reshape(-1, 2, 10), Noop\n\n(Transpose(1, 2),\n Permute(dims=0, 2, 1),\n View(bs, -1, 2, 10),\n Transpose(dims=1, 2).contiguous(),\n Reshape(bs, -1, 2, 10),\n Sequential())\n\n\n\nsource\n\n\nDropPath\n\n DropPath (p=None)\n\nDrop paths (Stochastic Depth) per sample (when applied in main path of residual blocks).\nIt’s similar to Dropout but it drops individual connections instead of nodes. Original code in https://github.com/rwightman/pytorch-image-models (timm library)\n\nt = torch.ones(100,2,3)\ntest_eq(DropPath(0.)(t), t)\nassert DropPath(0.5)(t).max() >= 1\n\n\nsource\n\n\nSharpen\n\n Sharpen (T=0.5)\n\nThis is used to increase confidence in predictions - MixMatch paper\n\nn_samples = 1000\nn_classes = 3\n\nt = (torch.rand(n_samples, n_classes) - .5) * 10\nprobas = F.softmax(t, -1)\nsharpened_probas = Sharpen()(probas)\nplt.plot(probas.flatten().sort().values, color='r')\nplt.plot(sharpened_probas.flatten().sort().values, color='b')\nplt.show()\ntest_gt(sharpened_probas[n_samples//2:].max(-1).values.sum().item(), probas[n_samples//2:].max(-1).values.sum().item())\n\n\n\n\n\nsource\n\n\nSequential\n\n Sequential (*args)\n\nClass that allows you to pass one or multiple inputs\n\nsource\n\n\nTimeDistributed\n\n TimeDistributed (module, batch_first=False)\n\nBase class for all neural network modules.\nYour models should also subclass this class.\nModules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes::\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 20, 5)\n        self.conv2 = nn.Conv2d(20, 20, 5)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        return F.relu(self.conv2(x))\nSubmodules assigned in this way will be registered, and will have their parameters converted too when you call :meth:to, etc.\n.. note:: As per the example above, an __init__() call to the parent class must be made before assignment on the child.\n:ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool\n\nsource\n\n\nget_calibrator\n\n get_calibrator (calibrator=None, n_classes=1, **kwargs)\n\n\nsource\n\n\nMatrix_Scale\n\n Matrix_Scale (n_classes=1, dirichlet=False)\n\nUsed to perform Matrix Scaling (dirichlet=False) or Dirichlet calibration (dirichlet=True)\n\nsource\n\n\nVector_Scale\n\n Vector_Scale (n_classes=1, dirichlet=False)\n\nUsed to perform Vector Scaling (dirichlet=False) or Diagonal Dirichlet calibration (dirichlet=True)\n\nsource\n\n\nTemp_Scale\n\n Temp_Scale (temp=1.0, dirichlet=False)\n\nUsed to perform Temperature Scaling (dirichlet=False) or Single-parameter Dirichlet calibration (dirichlet=True)\n\nbs = 2\nc_out = 3\n\nt = torch.rand(bs, c_out)\nfor calibrator, cal_name in zip(['temp', 'vector', 'matrix'], ['Temp_Scale', 'Vector_Scale', 'Matrix_Scale']): \n    cal = get_calibrator(calibrator, n_classes=c_out)\n#     print(calibrator)\n#     print(cal.weight, cal.bias, '\\n')\n    test_eq(cal(t), t)\n    test_eq(cal.__class__.__name__, cal_name)\nfor calibrator, cal_name in zip(['dtemp', 'dvector', 'dmatrix'], ['Temp_Scale', 'Vector_Scale', 'Matrix_Scale']):\n    cal = get_calibrator(calibrator, n_classes=c_out)\n#     print(calibrator)\n#     print(cal.weight, cal.bias, '\\n')\n    test_eq(cal(t), F.log_softmax(t, dim=1))\n    test_eq(cal.__class__.__name__, cal_name)\n\n\nbs = 2\nc_out = 3\n\nt = torch.rand(bs, c_out)\n\ntest_eq(Temp_Scale()(t).shape, t.shape)\ntest_eq(Vector_Scale(c_out)(t).shape, t.shape)\ntest_eq(Matrix_Scale(c_out)(t).shape, t.shape)\ntest_eq(Temp_Scale(dirichlet=True)(t).shape, t.shape)\ntest_eq(Vector_Scale(c_out, dirichlet=True)(t).shape, t.shape)\ntest_eq(Matrix_Scale(c_out, dirichlet=True)(t).shape, t.shape)\n\ntest_eq(Temp_Scale()(t), t)\ntest_eq(Vector_Scale(c_out)(t), t)\ntest_eq(Matrix_Scale(c_out)(t), t)\n\n\nbs = 2\nc_out = 5\n\nt = torch.rand(bs, c_out)\ntest_eq(Vector_Scale(c_out)(t), t)\ntest_eq(Vector_Scale(c_out).weight.data, torch.ones(c_out))\ntest_eq(Vector_Scale(c_out).weight.requires_grad, True)\ntest_eq(type(Vector_Scale(c_out).weight), torch.nn.parameter.Parameter)\n\n\nbs = 2\nc_out = 3\nweight = 2\nbias = 1\n\nt = torch.rand(bs, c_out)\ntest_eq(Matrix_Scale(c_out)(t).shape, t.shape)\ntest_eq(Matrix_Scale(c_out).weight.requires_grad, True)\ntest_eq(type(Matrix_Scale(c_out).weight), torch.nn.parameter.Parameter)\n\n\nsource\n\n\nLogitAdjustmentLayer\n\n LogitAdjustmentLayer (class_priors)\n\nLogit Adjustment for imbalanced datasets\n\nbs, n_classes = 16, 3\nclass_priors = torch.rand(n_classes)\nlogits = torch.randn(bs, n_classes) * 2\ntest_eq(LogitAdjLayer(class_priors)(logits), logits + class_priors)\n\n\nsource\n\n\nMaxPPVPool1d\n\n MaxPPVPool1d ()\n\nDrop-in replacement for AdaptiveConcatPool1d - multiplies nf by 2\n\nsource\n\n\nPPAuc\n\n PPAuc (dim=-1)\n\nSame as nn.Module, but no need for subclasses to call super().__init__\n\nsource\n\n\nPPV\n\n PPV (dim=-1)\n\nSame as nn.Module, but no need for subclasses to call super().__init__\n\nbs = 2\nnf = 5\nsl = 4\n\nt = torch.rand(bs, nf, sl)\ntest_eq(MaxPPVPool1d()(t).shape, (bs, nf*2, 1))\ntest_eq(MaxPPVPool1d()(t).shape, AdaptiveConcatPool1d(1)(t).shape)\n\n\nsource\n\n\nAdaptiveWeightedAvgPool1d\n\n AdaptiveWeightedAvgPool1d (n_in, seq_len, mult=2, n_layers=2, ln=False,\n                            dropout=0.5, act=ReLU(), zero_init=True)\n\nGlobal Pooling layer that performs a weighted average along the temporal axis\nIt can be considered as a channel-wise form of local temporal attention. Inspired by the paper: Hyun, J., Seong, H., & Kim, E. (2019). Universal Pooling–A New Pooling Method for Convolutional Neural Networks. arXiv preprint arXiv:1907.11440.\n\nsource\n\n\nGAWP1d\n\n GAWP1d (n_in, seq_len, n_layers=2, ln=False, dropout=0.5, act=ReLU(),\n         zero_init=False)\n\nGlobal AdaptiveWeightedAvgPool1d + Flatten\n\nsource\n\n\nGACP1d\n\n GACP1d (output_size=1)\n\nGlobal AdaptiveConcatPool + Flatten\n\nsource\n\n\nGAP1d\n\n GAP1d (output_size=1)\n\nGlobal Adaptive Pooling + Flatten\n\nsource\n\n\ngwa_pool_head\n\n gwa_pool_head (n_in, c_out, seq_len, bn=True, fc_dropout=0.0)\n\n\nsource\n\n\nGlobalWeightedAveragePool1d\n\n GlobalWeightedAveragePool1d (n_in, seq_len)\n\nGlobal Weighted Average Pooling layer\nInspired by Building Efficient CNN Architecture for Offline Handwritten Chinese Character Recognition https://arxiv.org/pdf/1804.01259.pdf\n\nt = torch.randn(16, 64, 50)\nhead = gwa_pool_head(64, 5, 50)\ntest_eq(head(t).shape, (16, 5))\n\n\nsource\n\n\nattentional_pool_head\n\n attentional_pool_head (n_in, c_out, seq_len=None, bn=True, **kwargs)\n\n\nsource\n\n\nGAttP1d\n\n GAttP1d (n_in, c_out, bn=False)\n\nA sequential container. Modules will be added to it in the order they are passed in the constructor. Alternatively, an OrderedDict of modules can be passed in. The forward() method of [Sequential](https://timeseriesAI.github.io/models.layers.html#sequential) accepts any input and forwards it to the first module it contains. It then “chains” outputs to inputs sequentially for each subsequent module, finally returning the output of the last module.\nThe value a [Sequential](https://timeseriesAI.github.io/models.layers.html#sequential) provides over manually calling a sequence of modules is that it allows treating the whole container as a single module, such that performing a transformation on the [Sequential](https://timeseriesAI.github.io/models.layers.html#sequential) applies to each of the modules it stores (which are each a registered submodule of the [Sequential](https://timeseriesAI.github.io/models.layers.html#sequential)).\nWhat’s the difference between a [Sequential](https://timeseriesAI.github.io/models.layers.html#sequential) and a :class:torch.nn.ModuleList? A ModuleList is exactly what it sounds like–a list for storing Module s! On the other hand, the layers in a [Sequential](https://timeseriesAI.github.io/models.layers.html#sequential) are connected in a cascading way.\nExample::\n# Using Sequential to create a small model. When `model` is run,\n# input will first be passed to `Conv2d(1,20,5)`. The output of\n# `Conv2d(1,20,5)` will be used as the input to the first\n# `ReLU`; the output of the first `ReLU` will become the input\n# for `Conv2d(20,64,5)`. Finally, the output of\n# `Conv2d(20,64,5)` will be used as input to the second `ReLU`\nmodel = nn.Sequential(\n          nn.Conv2d(1,20,5),\n          nn.ReLU(),\n          nn.Conv2d(20,64,5),\n          nn.ReLU()\n        )\n\n# Using Sequential with OrderedDict. This is functionally the\n# same as the above code\nmodel = nn.Sequential(OrderedDict([\n          ('conv1', nn.Conv2d(1,20,5)),\n          ('relu1', nn.ReLU()),\n          ('conv2', nn.Conv2d(20,64,5)),\n          ('relu2', nn.ReLU())\n        ]))\n\nsource\n\n\nAttentionalPool1d\n\n AttentionalPool1d (n_in, c_out, bn=False)\n\nGlobal Adaptive Pooling layer inspired by Attentional Pooling for Action Recognition https://arxiv.org/abs/1711.01467\n\nbs, c_in, seq_len = 16, 1, 50\nc_out = 3\nt = torch.rand(bs, c_in, seq_len)\ntest_eq(GAP1d()(t).shape, (bs, c_in))\ntest_eq(GACP1d()(t).shape, (bs, c_in*2))\nbs, c_in, seq_len = 16, 4, 50\nt = torch.rand(bs, c_in, seq_len)\ntest_eq(GAP1d()(t).shape, (bs, c_in))\ntest_eq(GACP1d()(t).shape, (bs, c_in*2))\ntest_eq(GAWP1d(c_in, seq_len, n_layers=2, ln=False, dropout=0.5, act=nn.ReLU(), zero_init=False)(t).shape, (bs, c_in))\ntest_eq(GAWP1d(c_in, seq_len, n_layers=2, ln=False, dropout=0.5, act=nn.ReLU(), zero_init=False)(t).shape, (bs, c_in))\ntest_eq(GAWP1d(c_in, seq_len, n_layers=1, ln=False, dropout=0.5, zero_init=False)(t).shape, (bs, c_in))\ntest_eq(GAWP1d(c_in, seq_len, n_layers=1, ln=False, dropout=0.5, zero_init=True)(t).shape, (bs, c_in))\ntest_eq(AttentionalPool1d(c_in, c_out)(t).shape, (bs, c_out, 1))\n\n\nbs, c_in, seq_len = 16, 128, 50\nc_out = 14\nt = torch.rand(bs, c_in, seq_len)\nattp = attentional_pool_head(c_in, c_out)\ntest_eq(attp(t).shape, (bs, c_out))\n\n\nsource\n\n\nPoolingLayer\n\n PoolingLayer (method='cls', seq_len=None, token=True, seq_last=True)\n\nSame as nn.Module, but no need for subclasses to call super().__init__\n\nt = torch.arange(24).reshape(2, 3, 4).float()\ntest_eq(PoolingLayer('cls', token=True, seq_last=True)(t), tensor([[ 0.,  4.,  8.], [12., 16., 20.]]))\ntest_eq(PoolingLayer('max', token=True, seq_last=True)(t), tensor([[ 3.,  7., 11.], [15., 19., 23.]]))\ntest_close(PoolingLayer('mean', token=True, seq_last=True)(t), tensor([[ 2.,  6., 10.], [14., 18., 22.]]))\ntest_close(PoolingLayer('max-mean', token=True, seq_last=True)(t), tensor([[ 3.,  7., 11.,  2.,  6., 10.],\n                                                                           [15., 19., 23., 14., 18., 22.]]))\ntest_close(PoolingLayer('flatten', token=True, seq_last=True)(t), tensor([[ 1.,  2.,  3.,  5.,  6.,  7.,  9., 10., 11.],\n                                                                          [13., 14., 15., 17., 18., 19., 21., 22., 23.]]))\ntest_eq(PoolingLayer('linear', seq_len=4, token=True, seq_last=True)(t).shape, (2, 3))\ntest_eq(PoolingLayer('max', token=False, seq_last=True)(t), tensor([[ 3.,  7., 11.], [15., 19., 23.]]))\ntest_close(PoolingLayer('mean', token=False, seq_last=True)(t), tensor([[ 1.5000,  5.5000,  9.5000],\n                                                                        [13.5000, 17.5000, 21.5000]]))\ntest_close(PoolingLayer('max-mean', token=False, seq_last=True)(t), tensor([[ 3.,  7., 11.,  1.5000,  5.5000,  9.5000],\n                                                                            [15., 19., 23., 13.5000, 17.5000, 21.5000]]))\ntest_close(PoolingLayer('flatten', token=False, seq_last=True)(t), tensor([[ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11.],\n                                                                           [12., 13., 14., 15., 16., 17., 18., 19., 20., 21., 22., 23.]]))\ntest_eq(PoolingLayer('linear', seq_len=4, token=False, seq_last=True)(t).shape, (2, 3))\n\n\nt = torch.arange(24).reshape(2, 3, 4).swapaxes(1,2).float()\ntest_eq(PoolingLayer('cls', token=True, seq_last=False)(t), tensor([[ 0.,  4.,  8.], [12., 16., 20.]]))\ntest_eq(PoolingLayer('max', token=True, seq_last=False)(t), tensor([[ 3.,  7., 11.], [15., 19., 23.]]))\ntest_close(PoolingLayer('mean', token=True, seq_last=False)(t), tensor([[ 2.,  6., 10.], [14., 18., 22.]]))\ntest_close(PoolingLayer('max-mean', token=True, seq_last=False)(t), tensor([[ 3.,  7., 11.,  2.,  6., 10.],\n                                                                           [15., 19., 23., 14., 18., 22.]]))\ntest_close(PoolingLayer('flatten', token=True, seq_last=False)(t), tensor([[ 1.,  5.,  9.,  2.,  6., 10.,  3.,  7., 11.],\n                                                                           [13., 17., 21., 14., 18., 22., 15., 19., 23.]]))\nt = torch.arange(24).reshape(2, 3, 4).swapaxes(1,2).float()\ntest_eq(PoolingLayer('conv1d', seq_len=4, token=False, seq_last=False)(t).shape, (2, 3))\ntest_eq(PoolingLayer('max', token=False, seq_last=False)(t), tensor([[ 3.,  7., 11.], [15., 19., 23.]]))\ntest_close(PoolingLayer('mean', token=False, seq_last=False)(t), tensor([[ 1.5000,  5.5000,  9.5000],\n                                                                        [13.5000, 17.5000, 21.5000]]))\ntest_close(PoolingLayer('max-mean', token=False, seq_last=False)(t), tensor([[ 3.,  7., 11.,  1.5000,  5.5000,  9.5000],\n                                                                            [15., 19., 23., 13.5000, 17.5000, 21.5000]]))\ntest_close(PoolingLayer('flatten', token=False, seq_last=False)(t), tensor([[ 0.,  4.,  8.,  1.,  5.,  9.,  2.,  6., 10.,  3.,  7., 11.],\n                                                                            [12., 16., 20., 13., 17., 21., 14., 18., 22., 15., 19., 23.]]))\ntest_eq(PoolingLayer('conv1d', seq_len=4, token=False, seq_last=False)(t).shape, (2, 3))\n\n\nsource\n\n\nReGLU\n\n ReGLU ()\n\nSame as nn.Module, but no need for subclasses to call super().__init__\n\nsource\n\n\nGEGLU\n\n GEGLU ()\n\nSame as nn.Module, but no need for subclasses to call super().__init__\n\nsource\n\n\nget_act_fn\n\n get_act_fn (act, **act_kwargs)\n\n\ntest_eq(get_act_fn(nn.ReLU).__repr__(), \"ReLU()\")\ntest_eq(get_act_fn(nn.ReLU()).__repr__(), \"ReLU()\")\ntest_eq(get_act_fn(nn.LeakyReLU, negative_slope=0.05).__repr__(), \"LeakyReLU(negative_slope=0.05)\")\ntest_eq(get_act_fn('reglu').__repr__(), \"ReGLU()\")\ntest_eq(get_act_fn('leakyrelu', negative_slope=0.05).__repr__(), \"LeakyReLU(negative_slope=0.05)\")\n\n\nsource\n\n\nRevIN\n\n RevIN (c_in:int, affine:bool=True, subtract_last:bool=False, dim:int=2,\n        eps:float=1e-05)\n\nReversible Instance Normalization layer adapted from\nKim, T., Kim, J., Tae, Y., Park, C., Choi, J. H., & Choo, J. (2021, September). Reversible instance normalization for accurate time-series forecasting against distribution shift. In International Conference on Learning Representations. Original code: https://github.com/ts-kim/RevIN\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nc_in\nint\n\n#features (aka variables or channels)\n\n\naffine\nbool\nTrue\nflag to incidate if RevIN has learnable weight and bias\n\n\nsubtract_last\nbool\nFalse\n\n\n\ndim\nint\n2\nint or tuple of dimensions used to calculate mean and std\n\n\neps\nfloat\n1e-05\nepsilon - parameter added for numerical stability\n\n\n\n\nsource\n\n\nRevIN\n\n RevIN (c_in:int, affine:bool=True, subtract_last:bool=False, dim:int=2,\n        eps:float=1e-05)\n\nReversible Instance Normalization layer adapted from\nKim, T., Kim, J., Tae, Y., Park, C., Choi, J. H., & Choo, J. (2021, September). Reversible instance normalization for accurate time-series forecasting against distribution shift. In International Conference on Learning Representations. Original code: https://github.com/ts-kim/RevIN\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nc_in\nint\n\n#features (aka variables or channels)\n\n\naffine\nbool\nTrue\nflag to incidate if RevIN has learnable weight and bias\n\n\nsubtract_last\nbool\nFalse\n\n\n\ndim\nint\n2\nint or tuple of dimensions used to calculate mean and std\n\n\neps\nfloat\n1e-05\nepsilon - parameter added for numerical stability\n\n\n\n\nt = ((torch.rand(16, 5, 100) - .25) * torch.Tensor([.01, .1, 1, 10, 100]).reshape(1, -1, 1)).cumsum(-1)\nt_clone = t.clone()\nl = RevIN(5)\nt_norm = l(t, torch.tensor(True))\nt_denorm = l(t_norm, torch.tensor(False))\ntest_close(t_clone, t_denorm, eps=1e-3)\n\n\nmodel = RevIN(5, affine=True)\ntry:\n    scripted_model = torch.jit.script(model)\n    file_path = f\"test_scripted_model.pt\"\n    torch.jit.save(scripted_model, file_path)\n    scripted_model = torch.jit.load(file_path)\n\n    inp = ((torch.rand(16, 5, 100) - .25) * torch.Tensor([.01, .1, 1, 10, 100]).reshape(1, -1, 1)).cumsum(-1)\n    normed_output = model(inp, torch.tensor(True))\n    demormed_output = model(normed_output, torch.tensor(False))\n    scripted_normed_output = scripted_model(inp, torch.tensor(True))\n    scripted_denormed_output = scripted_model(scripted_normed_output, torch.tensor(False))\n    test_close(normed_output, scripted_normed_output)\n    test_close(demormed_output, scripted_denormed_output)\n    os.remove(file_path)\n    del scripted_model\n    gc.collect()\n    print('scripting ok')\nexcept Exception as e:\n    print(f'scripting failed: {e}')\n\nscripting ok\n\n\n\nsource\n\n\ncreate_pool_head\n\n create_pool_head (n_in, c_out, seq_len=None, concat_pool=False,\n                   fc_dropout=0.0, bn=False, y_range=None, **kwargs)\n\n\nbs = 16\nnf = 12\nc_out = 2\nseq_len = 20\nt = torch.rand(bs, nf, seq_len)\ntest_eq(create_pool_head(nf, c_out, seq_len, fc_dropout=0.5)(t).shape, (bs, c_out))\ntest_eq(create_pool_head(nf, c_out, seq_len, concat_pool=True, fc_dropout=0.5)(t).shape, (bs, c_out))\ncreate_pool_head(nf, c_out, seq_len, concat_pool=True, bn=True, fc_dropout=.5)\n\nSequential(\n  (0): GACP1d(\n    (gacp): AdaptiveConcatPool1d(\n      (ap): AdaptiveAvgPool1d(output_size=1)\n      (mp): AdaptiveMaxPool1d(output_size=1)\n    )\n    (flatten): Reshape(bs)\n  )\n  (1): LinBnDrop(\n    (0): BatchNorm1d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (1): Dropout(p=0.5, inplace=False)\n    (2): Linear(in_features=24, out_features=2, bias=False)\n  )\n)\n\n\n\nsource\n\n\nmax_pool_head\n\n max_pool_head (n_in, c_out, seq_len, fc_dropout=0.0, bn=False,\n                y_range=None, **kwargs)\n\n\nbs = 16\nnf = 12\nc_out = 2\nseq_len = 20\nt = torch.rand(bs, nf, seq_len)\ntest_eq(max_pool_head(nf, c_out, seq_len, fc_dropout=0.5)(t).shape, (bs, c_out))\n\n\nsource\n\n\ncreate_pool_plus_head\n\n create_pool_plus_head (*args, lin_ftrs=None, fc_dropout=0.0,\n                        concat_pool=True, bn_final=False, lin_first=False,\n                        y_range=None)\n\n\nbs = 16\nnf = 12\nc_out = 2\nseq_len = 20\nt = torch.rand(bs, nf, seq_len)\ntest_eq(create_pool_plus_head(nf, c_out, seq_len, fc_dropout=0.5)(t).shape, (bs, c_out))\ntest_eq(create_pool_plus_head(nf, c_out, concat_pool=True, fc_dropout=0.5)(t).shape, (bs, c_out))\ncreate_pool_plus_head(nf, c_out, seq_len, fc_dropout=0.5)\n\nSequential(\n  (0): AdaptiveConcatPool1d(\n    (ap): AdaptiveAvgPool1d(output_size=1)\n    (mp): AdaptiveMaxPool1d(output_size=1)\n  )\n  (1): Reshape(bs)\n  (2): BatchNorm1d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (3): Dropout(p=0.25, inplace=False)\n  (4): Linear(in_features=24, out_features=512, bias=False)\n  (5): ReLU(inplace=True)\n  (6): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (7): Dropout(p=0.5, inplace=False)\n  (8): Linear(in_features=512, out_features=2, bias=False)\n)\n\n\n\nsource\n\n\ncreate_conv_head\n\n create_conv_head (*args, adaptive_size=None, y_range=None)\n\n\nbs = 16\nnf = 12\nc_out = 2\nseq_len = 20\nt = torch.rand(bs, nf, seq_len)\ntest_eq(create_conv_head(nf, c_out, seq_len)(t).shape, (bs, c_out))\ntest_eq(create_conv_head(nf, c_out, adaptive_size=50)(t).shape, (bs, c_out))\ncreate_conv_head(nf, c_out, 50)\n\nSequential(\n  (0): ConvBlock(\n    (0): Conv1d(12, 6, kernel_size=(1,), stride=(1,), bias=False)\n    (1): BatchNorm1d(6, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (2): ReLU()\n  )\n  (1): ConvBlock(\n    (0): Conv1d(6, 3, kernel_size=(1,), stride=(1,), bias=False)\n    (1): BatchNorm1d(3, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (2): ReLU()\n  )\n  (2): ConvBlock(\n    (0): Conv1d(3, 2, kernel_size=(1,), stride=(1,), bias=False)\n    (1): BatchNorm1d(2, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (2): ReLU()\n  )\n  (3): GAP1d(\n    (gap): AdaptiveAvgPool1d(output_size=1)\n    (flatten): Reshape(bs)\n  )\n)\n\n\n\nsource\n\n\ncreate_mlp_head\n\n create_mlp_head (nf, c_out, seq_len=None, flatten=True, fc_dropout=0.0,\n                  bn=False, lin_first=False, y_range=None)\n\n\nbs = 16\nnf = 12\nc_out = 2\nseq_len = 20\nt = torch.rand(bs, nf, seq_len)\ntest_eq(create_mlp_head(nf, c_out, seq_len, fc_dropout=0.5)(t).shape, (bs, c_out))\nt = torch.rand(bs, nf, seq_len)\ncreate_mlp_head(nf, c_out, seq_len, bn=True, fc_dropout=.5)\n\nSequential(\n  (0): Reshape(bs)\n  (1): LinBnDrop(\n    (0): BatchNorm1d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (1): Dropout(p=0.5, inplace=False)\n    (2): Linear(in_features=240, out_features=2, bias=False)\n  )\n)\n\n\n\nsource\n\n\ncreate_fc_head\n\n create_fc_head (nf, c_out, seq_len=None, flatten=True, lin_ftrs=None,\n                 y_range=None, fc_dropout=0.0, bn=False, bn_final=False,\n                 act=ReLU(inplace=True))\n\n\nbs = 16\nnf = 12\nc_out = 2\nseq_len = 20\nt = torch.rand(bs, nf, seq_len)\ntest_eq(create_fc_head(nf, c_out, seq_len, fc_dropout=0.5)(t).shape, (bs, c_out))\ncreate_mlp_head(nf, c_out, seq_len, bn=True, fc_dropout=.5)\n\nSequential(\n  (0): Reshape(bs)\n  (1): LinBnDrop(\n    (0): BatchNorm1d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (1): Dropout(p=0.5, inplace=False)\n    (2): Linear(in_features=240, out_features=2, bias=False)\n  )\n)\n\n\n\nsource\n\n\ncreate_rnn_head\n\n create_rnn_head (*args, fc_dropout=0.0, bn=False, y_range=None)\n\n\nbs = 16\nnf = 12\nc_out = 2\nseq_len = 20\nt = torch.rand(bs, nf, seq_len)\ntest_eq(create_rnn_head(nf, c_out, seq_len, fc_dropout=0.5)(t).shape, (bs, c_out))\ncreate_rnn_head(nf, c_out, seq_len, bn=True, fc_dropout=.5)\n\nSequential(\n  (0): LastStep()\n  (1): LinBnDrop(\n    (0): BatchNorm1d(12, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (1): Dropout(p=0.5, inplace=False)\n    (2): Linear(in_features=12, out_features=2, bias=False)\n  )\n)\n\n\n\nsource\n\n\nimputation_head\n\n imputation_head (c_in, c_out, seq_len=None, ks=1, y_range=None,\n                  fc_dropout=0.0)\n\n\nbs = 16\nnf = 12\nni = 2\nseq_len = 20\nt = torch.rand(bs, nf, seq_len)\nhead = imputation_head(nf, ni, seq_len=None, ks=1, y_range=None, fc_dropout=0.)\ntest_eq(head(t).shape, (bs, ni, seq_len))\nhead = imputation_head(nf, ni, seq_len=None, ks=1, y_range=(.3,.7), fc_dropout=0.)\ntest_ge(head(t).min(), .3)\ntest_le(head(t).max(), .7)\ny_range = (tensor([0.1000, 0.1000, 0.1000, 0.1000, 0.2000, 0.2000, 0.2000, 0.2000, 0.3000,\n                   0.3000, 0.3000, 0.3000]),\n           tensor([0.6000, 0.6000, 0.6000, 0.6000, 0.7000, 0.7000, 0.7000, 0.7000, 0.8000,\n                   0.8000, 0.8000, 0.8000]))\ntest_ge(head(t).min(), .1)\ntest_le(head(t).max(), .9)\nhead = imputation_head(nf, ni, seq_len=None, ks=1, y_range=y_range, fc_dropout=0.)\nhead\n\nSequential(\n  (0): Dropout(p=0.0, inplace=False)\n  (1): Conv1d(12, 2, kernel_size=(1,), stride=(1,))\n  (2): fastai.layers.SigmoidRange(low=tensor([0.1000, 0.1000, 0.1000, 0.1000, 0.2000, 0.2000, 0.2000, 0.2000, 0.3000,\n          0.3000, 0.3000, 0.3000]), high=tensor([0.6000, 0.6000, 0.6000, 0.6000, 0.7000, 0.7000, 0.7000, 0.7000, 0.8000,\n          0.8000, 0.8000, 0.8000]))\n)\n\n\n\nsource\n\n\ncreate_conv_lin_nd_head\n\n create_conv_lin_nd_head (n_in, n_out, seq_len, d, conv_first=True,\n                          conv_bn=False, lin_bn=False, fc_dropout=0.0,\n                          **kwargs)\n\nModule to create a nd output head\n\nbs = 16\nnf = 32\nc = 5\nseq_len = 10\nd = 2\ntarg = torch.randint(0, c, (bs,d))\nt = torch.randn(bs, nf, seq_len)\nhead = conv_lin_nd_head(nf, c, seq_len, d, conv_first=True, fc_dropout=.5)\ninp = head(t)\ntest_eq(inp.shape, (bs, d, c))\nloss = CrossEntropyLossFlat()(inp, targ)\nloss, head\n\n(TensorBase(1.7244, grad_fn=<AliasBackward0>),\n create_conv_lin_nd_head(\n   (0): Conv1d(32, 5, kernel_size=(1,), stride=(1,))\n   (1): Dropout(p=0.5, inplace=False)\n   (2): Linear(in_features=10, out_features=2, bias=True)\n   (3): Transpose(-1, -2)\n   (4): Reshape(bs, 2, 5)\n ))\n\n\n\nbs = 16\nnf = 32\nc = 5\nseq_len = 10\nd = [2, 8]\ntarg = torch.randint(0, c, [bs]+d)\nt = torch.randn(bs, nf, seq_len)\nhead = conv_lin_nd_head(nf, c, seq_len, d, conv_first=False, fc_dropout=.5)\ninp = head(t)\ntest_eq(inp.shape, [bs]+d+[c])\nloss = CrossEntropyLossFlat()(inp, targ)\nloss, head\n\n(TensorBase(1.7228, grad_fn=<AliasBackward0>),\n create_conv_lin_nd_head(\n   (0): Dropout(p=0.5, inplace=False)\n   (1): Linear(in_features=10, out_features=16, bias=True)\n   (2): Conv1d(32, 5, kernel_size=(1,), stride=(1,))\n   (3): Transpose(-1, -2)\n   (4): Reshape(bs, 2, 8, 5)\n ))\n\n\n\nbs = 16\nnf = 32\nc = 1\nseq_len = 10\nd = 2\ntarg = torch.rand(bs, d)\nt = torch.randn(bs, nf, seq_len)\nhead = conv_lin_nd_head(nf, c, seq_len, d, conv_first=False, fc_dropout=.5)\ninp = head(t)\ntest_eq(inp.shape, (bs, d))\nloss = L1LossFlat()(inp, targ)\nloss, head\n\n(TensorBase(0.5513, grad_fn=<AliasBackward0>),\n create_conv_lin_nd_head(\n   (0): Dropout(p=0.5, inplace=False)\n   (1): Linear(in_features=10, out_features=2, bias=True)\n   (2): Conv1d(32, 1, kernel_size=(1,), stride=(1,))\n   (3): Transpose(-1, -2)\n   (4): Reshape(bs, 2)\n ))\n\n\n\nbs = 16\nnf = 32\nc = 1\nseq_len = 10\nd = [2,3]\ntarg = torch.rand(bs, *d)\nt = torch.randn(bs, nf, seq_len)\nhead = conv_lin_nd_head(nf, c, seq_len, d, conv_first=False, fc_dropout=.5)\ninp = head(t)\ntest_eq(inp.shape, [bs]+d)\nloss = L1LossFlat()(inp, targ)\nloss, head\n\n(TensorBase(0.6637, grad_fn=<AliasBackward0>),\n create_conv_lin_nd_head(\n   (0): Dropout(p=0.5, inplace=False)\n   (1): Linear(in_features=10, out_features=6, bias=True)\n   (2): Conv1d(32, 1, kernel_size=(1,), stride=(1,))\n   (3): Transpose(-1, -2)\n   (4): Reshape(bs, 2, 3)\n ))\n\n\n\nsource\n\n\nlin_nd_head\n\n lin_nd_head (n_in, n_out, seq_len=None, d=None, flatten=False,\n              use_bn=False, fc_dropout=0.0)\n\nModule to create a nd output head with linear layers\n\nbs = 16\nnf = 32\nseq_len = 50\nx = torch.normal(0, 1, (bs, nf, seq_len))\n\nfor use_bn in [False, True]:\n    for fc_dropout in [0, 0.2]:\n        for flatten in [False, True]:\n            for c in [1, 3]:\n                for d in [None, (50,), (50,10), (30,5), (50,2,3), (30,2,3)]:\n                    for q_len in [1, seq_len]:\n                        head = lin_nd_head(nf, c, q_len, d, flatten=flatten, use_bn=use_bn, fc_dropout=fc_dropout)\n                        test_eq(head(x).shape, (bs, ) + (d if d is not None else ()) + ((c,) if c != 1 else ()))\n\n\nbs = 16\nnf = 32\nc = 5\nseq_len = 10\nd = 2\ntarg = torch.randint(0, c, (bs,d))\nt = torch.randn(bs, nf, seq_len)\nhead = lin_nd_head(nf, c, seq_len, d, fc_dropout=.5)\ninp = head(t)\ntest_eq(inp.shape, (bs, d, c))\nloss = CrossEntropyLossFlat()(inp, targ)\nloss, head\n\n(TensorBase(1.8514, grad_fn=<AliasBackward0>),\n lin_nd_head(\n   (0): Dropout(p=0.5, inplace=False)\n   (1): Reshape(bs)\n   (2): Linear(in_features=320, out_features=10, bias=True)\n   (3): Reshape(bs, 2, 5)\n ))\n\n\n\nbs = 16\nnf = 32\nc = 5\nseq_len = 10\nd = [2, 8]\ntarg = torch.randint(0, c, [bs]+d)\nt = torch.randn(bs, nf, seq_len)\nhead = lin_nd_head(nf, c, seq_len, d, fc_dropout=.5)\ninp = head(t)\ntest_eq(inp.shape, [bs]+d+[c])\nloss = CrossEntropyLossFlat()(inp, targ)\nloss, head\n\n(TensorBase(2.0023, grad_fn=<AliasBackward0>),\n lin_nd_head(\n   (0): Dropout(p=0.5, inplace=False)\n   (1): Reshape(bs)\n   (2): Linear(in_features=320, out_features=80, bias=True)\n   (3): Reshape(bs, 2, 8, 5)\n ))\n\n\n\nbs = 16\nnf = 32\nc = 1\nseq_len = 10\nd = 2\ntarg = torch.rand(bs, d)\nt = torch.randn(bs, nf, seq_len)\nhead = lin_nd_head(nf, c, seq_len, d, fc_dropout=.5)\ninp = head(t)\ntest_eq(inp.shape, (bs, d))\nloss = L1LossFlat()(inp, targ)\nloss, head\n\n(TensorBase(0.8024, grad_fn=<AliasBackward0>),\n lin_nd_head(\n   (0): Dropout(p=0.5, inplace=False)\n   (1): Reshape(bs)\n   (2): Linear(in_features=320, out_features=2, bias=True)\n   (3): Reshape(bs, 2)\n ))\n\n\n\nbs = 16\nnf = 32\nc = 1\nseq_len = 10\nd = [2,3]\ntarg = torch.rand(bs, *d)\nt = torch.randn(bs, nf, seq_len)\nhead = lin_nd_head(nf, c, seq_len, d, fc_dropout=.5)\ninp = head(t)\ntest_eq(inp.shape, [bs]+d)\nloss = L1LossFlat()(inp, targ)\nloss, head\n\n(TensorBase(0.7623, grad_fn=<AliasBackward0>),\n lin_nd_head(\n   (0): Dropout(p=0.5, inplace=False)\n   (1): Reshape(bs)\n   (2): Linear(in_features=320, out_features=6, bias=True)\n   (3): Reshape(bs, 2, 3)\n ))\n\n\n\nsource\n\n\ncreate_conv_3d_head\n\n create_conv_3d_head (n_in, n_out, seq_len, d, use_bn=False, **kwargs)\n\nModule to create a nd output head with a convolutional layer\n\nbs = 16\nnf = 32\nc = 5\nseq_len = 10\nd = 10\ntarg = torch.randint(0, c, (bs,d))\nt = torch.randn(bs, nf, seq_len)\nhead = conv_3d_head(nf, c, seq_len, d)\ninp = head(t)\ntest_eq(inp.shape, (bs, d, c))\nloss = CrossEntropyLossFlat()(inp, targ)\nloss, head\n\n(TensorBase(1.6927, grad_fn=<AliasBackward0>),\n create_conv_3d_head(\n   (0): ConvBlock(\n     (0): Conv1d(32, 5, kernel_size=(1,), stride=(1,))\n   )\n   (1): Transpose(-1, -2)\n ))\n\n\n\nbs = 16\nnf = 32\nc = 1\nseq_len = 10\nd = 10\ntarg = torch.rand(bs, d)\nt = torch.randn(bs, nf, seq_len)\nhead = conv_3d_head(nf, c, seq_len, d)\ninp = head(t)\ntest_eq(inp.shape, (bs, d))\nloss = L1LossFlat()(inp, targ)\nloss, head\n\n(TensorBase(0.6315, grad_fn=<AliasBackward0>),\n create_conv_3d_head(\n   (0): ConvBlock(\n     (0): Conv1d(32, 1, kernel_size=(1,), stride=(1,))\n   )\n   (1): Transpose(-1, -2)\n   (2): Squeeze(dim=-1)\n ))\n\n\n\nsource\n\n\nuniversal_pool_head\n\n universal_pool_head (n_in, c_out, seq_len, mult=2, pool_n_layers=2,\n                      pool_ln=True, pool_dropout=0.5, pool_act=ReLU(),\n                      zero_init=True, bn=True, fc_dropout=0.0)\n\n\nbs, c_in, seq_len = 16, 128, 50\nc_out = 14\nt = torch.rand(bs, c_in, seq_len)\nuph = universal_pool_head(c_in, c_out, seq_len)\ntest_eq(uph(t).shape, (bs, c_out))\nuph = universal_pool_head(c_in, c_out, seq_len, 2)\ntest_eq(uph(t).shape, (bs, c_out))\n\n\nbs, c_in, seq_len = 16, 128, 50\nc_out = 14\nd = 5\nt = torch.rand(bs, c_in, seq_len)\nfor head in heads: \n    print(head.__name__)\n    if head.__name__ == \"create_conv_3d_head\":\n        h = head(c_in, c_out, seq_len, seq_len)\n        test_eq(h(t).shape, (bs, seq_len, c_out))\n    elif 'nd' in head.__name__: \n        h = head(c_in, c_out, seq_len, d)\n        test_eq(h(t).shape, (bs, d, c_out))\n    else: \n        h = head(c_in, c_out, seq_len)\n        test_eq(h(t).shape, (bs, c_out))\n\ncreate_mlp_head\ncreate_fc_head\naverage_pool_head\nmax_pool_head\nconcat_pool_head\ncreate_pool_plus_head\ncreate_conv_head\ncreate_rnn_head\ncreate_conv_lin_nd_head\nlin_nd_head\ncreate_conv_3d_head\nattentional_pool_head\nuniversal_pool_head\ngwa_pool_head\n\n\n\nsource\n\n\nSqueezeExciteBlock\n\n SqueezeExciteBlock (ni, reduction=16)\n\nSame as nn.Module, but no need for subclasses to call super().__init__\n\nbs = 2\nni = 32\nsl = 4\nt = torch.rand(bs, ni, sl)\ntest_eq(SqueezeExciteBlock(ni)(t).shape, (bs, ni, sl))\n\n\nsource\n\n\nGaussianNoise\n\n GaussianNoise (sigma=0.1, is_relative_detach=True)\n\nGaussian noise regularizer.\nArgs: sigma (float, optional): relative standard deviation used to generate the noise. Relative means that it will be multiplied by the magnitude of the value your are adding the noise to. This means that sigma can be the same regardless of the scale of the vector. is_relative_detach (bool, optional): whether to detach the variable before computing the scale of the noise. If False then the scale of the noise won’t be seen as a constant but something to optimize: this will bias the network to generate vectors with smaller values.\n\nt = torch.ones(2,3,4)\ntest_ne(GaussianNoise()(t), t)\ntest_eq(GaussianNoise()(t).shape, t.shape)\nt = torch.ones(2,3)\ntest_ne(GaussianNoise()(t), t)\ntest_eq(GaussianNoise()(t).shape, t.shape)\nt = torch.ones(2)\ntest_ne(GaussianNoise()(t), t)\ntest_eq(GaussianNoise()(t).shape, t.shape)\n\n\nsource\n\n\nTokenLayer\n\n TokenLayer (token=True)\n\nSame as nn.Module, but no need for subclasses to call super().__init__\n\nsource\n\n\nPositionwiseFeedForward\n\n PositionwiseFeedForward (dim, dropout=0.0, act='reglu', mlp_ratio=1)\n\nA sequential container. Modules will be added to it in the order they are passed in the constructor. Alternatively, an OrderedDict of modules can be passed in. The forward() method of [Sequential](https://timeseriesAI.github.io/models.layers.html#sequential) accepts any input and forwards it to the first module it contains. It then “chains” outputs to inputs sequentially for each subsequent module, finally returning the output of the last module.\nThe value a [Sequential](https://timeseriesAI.github.io/models.layers.html#sequential) provides over manually calling a sequence of modules is that it allows treating the whole container as a single module, such that performing a transformation on the [Sequential](https://timeseriesAI.github.io/models.layers.html#sequential) applies to each of the modules it stores (which are each a registered submodule of the [Sequential](https://timeseriesAI.github.io/models.layers.html#sequential)).\nWhat’s the difference between a [Sequential](https://timeseriesAI.github.io/models.layers.html#sequential) and a :class:torch.nn.ModuleList? A ModuleList is exactly what it sounds like–a list for storing Module s! On the other hand, the layers in a [Sequential](https://timeseriesAI.github.io/models.layers.html#sequential) are connected in a cascading way.\nExample::\n# Using Sequential to create a small model. When `model` is run,\n# input will first be passed to `Conv2d(1,20,5)`. The output of\n# `Conv2d(1,20,5)` will be used as the input to the first\n# `ReLU`; the output of the first `ReLU` will become the input\n# for `Conv2d(20,64,5)`. Finally, the output of\n# `Conv2d(20,64,5)` will be used as input to the second `ReLU`\nmodel = nn.Sequential(\n          nn.Conv2d(1,20,5),\n          nn.ReLU(),\n          nn.Conv2d(20,64,5),\n          nn.ReLU()\n        )\n\n# Using Sequential with OrderedDict. This is functionally the\n# same as the above code\nmodel = nn.Sequential(OrderedDict([\n          ('conv1', nn.Conv2d(1,20,5)),\n          ('relu1', nn.ReLU()),\n          ('conv2', nn.Conv2d(20,64,5)),\n          ('relu2', nn.ReLU())\n        ]))\n\nt = torch.randn(2,3,10)\nm = PositionwiseFeedForward(10, dropout=0., act='reglu', mlp_ratio=1)\ntest_eq(m(t).shape, t.shape)\nm = PositionwiseFeedForward(10, dropout=0., act='smelu', mlp_ratio=1)\ntest_eq(m(t).shape, t.shape)\n\n\nsource\n\n\nScaledDotProductAttention\n\n ScaledDotProductAttention (d_model, n_heads, attn_dropout=0.0,\n                            res_attention=False, lsa=False)\n\nScaled Dot-Product Attention module (Attention is all you need by Vaswani et al., 2017) with optional residual attention from previous layer (Realformer: Transformer likes residual attention by He et al, 2020) and locality self sttention (Vision Transformer for Small-Size Datasets by Lee et al, 2021)\n\nB = 16\nC = 10\nM = 1500 # seq_len\n\nn_heads = 1\nD = 128 # model dimension\nN = 512 # max_seq_len - latent's index dimension\nd_k = D // n_heads\n\nxb = torch.randn(B, C, M)\nxb = (xb - xb.mean()) / xb.std()\n\n# Attention\n# input (Q)\nlin = nn.Linear(M, N, bias=False)\nQ = lin(xb).transpose(1,2)\ntest_eq(Q.shape, (B, N, C))\n\n# q\nto_q = nn.Linear(C, D, bias=False)\nq = to_q(Q)\nq = nn.LayerNorm(D)(q)\n\n# k, v\ncontext = xb.transpose(1,2)\nto_kv = nn.Linear(C, D * 2, bias=False)\nk, v = to_kv(context).chunk(2, dim = -1)\nk = k.transpose(-1, -2)\nk = nn.LayerNorm(M)(k)\nv = nn.LayerNorm(D)(v)\n\ntest_eq(q.shape, (B, N, D))\ntest_eq(k.shape, (B, D, M))\ntest_eq(v.shape, (B, M, D))\n\noutput, attn, scores = ScaledDotProductAttention(D, n_heads, res_attention=True)(q.unsqueeze(1), k.unsqueeze(1), v.unsqueeze(1))\ntest_eq(output.shape, (B, 1, N, D))\ntest_eq(attn.shape, (B, 1, N, M))\ntest_eq(scores.shape, (B, 1, N, M))\nscores.mean(), scores.std()\n\n(tensor(-1.1921e-10, grad_fn=<MeanBackward0>),\n tensor(1.0393, grad_fn=<StdBackward0>))\n\n\n\nsource\n\n\nMultiheadAttention\n\n MultiheadAttention (d_model, n_heads, d_k=None, d_v=None,\n                     res_attention=False, attn_dropout=0.0,\n                     proj_dropout=0.0, qkv_bias=True, lsa=False)\n\nSame as nn.Module, but no need for subclasses to call super().__init__\n\nq = torch.rand([16, 3, 50, 8]) \nk = torch.rand([16, 3, 50, 8]).transpose(-1, -2)\nv = torch.rand([16, 3, 50, 6])\nattn_mask = torch.triu(torch.ones(50, 50)) # shape: q_len x q_len\nkey_padding_mask = torch.zeros(16, 50)\nkey_padding_mask[[1, 3, 6, 15], -10:] = 1\nkey_padding_mask = key_padding_mask.bool()\nprint('attn_mask', attn_mask.shape, 'key_padding_mask', key_padding_mask.shape)\noutput, attn = ScaledDotProductAttention(24, 3, attn_dropout=.1)(q, k, v, attn_mask=attn_mask, key_padding_mask=key_padding_mask)\noutput.shape, attn.shape\n\nattn_mask torch.Size([50, 50]) key_padding_mask torch.Size([16, 50])\n\n\n(torch.Size([16, 3, 50, 6]), torch.Size([16, 3, 50, 50]))\n\n\n\nt = torch.rand(16, 50, 128)\noutput, attn = MultiheadAttention(d_model=128, n_heads=3, d_k=8, d_v=6)(t, t, t, key_padding_mask=key_padding_mask, attn_mask=attn_mask)\noutput.shape, attn.shape\n\n(torch.Size([16, 50, 128]), torch.Size([16, 3, 50, 50]))\n\n\nTest multi-head attention with self-locality attention\n\n# lsa (locality self-sttention)\nt = torch.rand(16, 50, 128)\nattn_mask = torch.eye(50).reshape(1, 1, 50, 50).bool()\noutput, attn = MultiheadAttention(d_model=128, n_heads=8, lsa=True)(t, t, t, key_padding_mask=key_padding_mask, attn_mask=attn_mask)\noutput.shape, attn.shape\n\n(torch.Size([16, 50, 128]), torch.Size([16, 8, 50, 50]))\n\n\n\nt = torch.rand(16, 50, 128)\natt_mask = (torch.rand((50, 50)) > .85).float()\natt_mask[att_mask == 1] = -np.inf\n\nmha = MultiheadAttention(d_model=128, n_heads=3, d_k=8, d_v=6)\noutput, attn = mha(t, t, t, attn_mask=att_mask)\ntest_eq(torch.isnan(output).sum().item(), 0)\ntest_eq(torch.isnan(attn).sum().item(), 0)\nloss = output[:2, :].sum()\ntest_eq(torch.isnan(loss).sum().item(), 0)\nloss.backward()\nfor n, p in mha.named_parameters(): \n    if p.grad is not None:\n        test_eq(torch.isnan(p.grad).sum().item(), 0)\n\n\nt = torch.rand(16, 50, 128)\nattn_mask = (torch.rand((50, 50)) > .85)\n\n# True values will be masked\nmha = MultiheadAttention(d_model=128, n_heads=3, d_k=8, d_v=6)\noutput, attn = mha(t, t, t, attn_mask=att_mask)\ntest_eq(torch.isnan(output).sum().item(), 0)\ntest_eq(torch.isnan(attn).sum().item(), 0)\nloss = output[:2, :].sum()\ntest_eq(torch.isnan(loss).sum().item(), 0)\nloss.backward()\nfor n, p in mha.named_parameters(): \n    if p.grad is not None:\n        test_eq(torch.isnan(p.grad).sum().item(), 0)\n\n\nsource\n\n\nMultiConv1d\n\n MultiConv1d (ni, nf=None, kss=[1, 3, 5, 7], keep_original=False,\n              separable=False, dim=1, **kwargs)\n\nModule that applies multiple convolutions with different kernel sizes\n\nt = torch.rand(16, 6, 37)\ntest_eq(MultiConv1d(6, None, kss=[1,3,5], keep_original=True)(t).shape, [16, 24, 37])\ntest_eq(MultiConv1d(6, 36, kss=[1,3,5], keep_original=False)(t).shape, [16, 36, 37])\ntest_eq(MultiConv1d(6, None, kss=[1,3,5], keep_original=True, dim=-1)(t).shape, [16, 6, 37*4])\ntest_eq(MultiConv1d(6, 60, kss=[1,3,5], keep_original=True)(t).shape, [16, 60, 37])\ntest_eq(MultiConv1d(6, 60, kss=[1,3,5], separable=True)(t).shape, [16, 60, 37])\n\n\nsource\n\n\nLSTMOutput\n\n LSTMOutput ()\n\nSame as nn.Module, but no need for subclasses to call super().__init__\n\nt = ([1], [2], [3])\ntest_eq(LSTMOutput()(t), [1])\n\n\nsource\n\n\nemb_sz_rule\n\n emb_sz_rule (n_cat)\n\nRule of thumb to pick embedding size corresponding to n_cat (original from fastai)\n\ntest_eq(emb_sz_rule(7), 5)\n\n\nsource\n\n\nTSEmbedding\n\n TSEmbedding (ni, nf, std=0.01, padding_idx=None)\n\nEmbedding layer with truncated normal initialization adapted from fastai\n\nsource\n\n\nMultiEmbedding\n\n MultiEmbedding (c_in, n_cat_embeds, cat_embed_dims=None, cat_pos=None,\n                 std=0.01, cat_padding_idxs=None)\n\nSame as nn.Module, but no need for subclasses to call super().__init__\n\na = alphabet[np.random.randint(0,3,40)]\nb = ALPHABET[np.random.randint(6,10,40)]\nc = np.random.rand(40).reshape(4,1,10)\nmap_a = {k:v for v,k in enumerate(np.unique(a))}\nmap_b = {k:v for v,k in enumerate(np.unique(b))}\nn_embeds = [len(m.keys()) for m in [map_a, map_b]]\nszs = [emb_sz_rule(n) for n in n_embeds]\na = np.asarray(a.map(map_a)).reshape(4,1,10)\nb = np.asarray(b.map(map_b)).reshape(4,1,10)\ninp = torch.from_numpy(np.concatenate((c,a,b), 1)).float()\nmemb = MultiEmbedding(3, n_embeds, cat_pos=[1,2])\n# registered buffers are part of the state_dict() but not module.parameters()\nassert all([(k in memb.state_dict().keys()) for k in ['cat_pos', 'cont_pos']])\nembeddings = memb(inp)\nprint(n_embeds, szs, inp.shape, embeddings.shape)\ntest_eq(embeddings.shape, (inp.shape[0],sum(szs)+1,inp.shape[-1]))\n\n[3, 4] [3, 3] torch.Size([4, 3, 10]) torch.Size([4, 7, 10])\n\n\n\nme = MultiEmbedding(3, 4, cat_pos=2)\ntest_eq(me.cat_embed[0].weight.shape, (4,3))\ntest_eq(me.cat_pos.cpu().item(), 2)"
  },
  {
    "objectID": "models.patchtst.html",
    "href": "models.patchtst.html",
    "title": "PatchTST",
    "section": "",
    "text": "This is an unofficial PyTorch implementation of PatchTST created by Ignacio Oguiza (oguiza@timeseriesAI.co) based on:\nIn this notebook, we are going to use a new state-of-the-art model called PatchTST (Nie et al, 2022) to create a long-term time series forecast.\nHere are some paper details:\n\nNie, Y., Nguyen, N. H., Sinthong, P., & Kalagnanam, J. (2022). A Time Series is Worth 64 Words: Long-term Forecasting with Transformers. arXiv preprint arXiv:2211.14730.\nOfficial implementation:: https://github.com/yuqinie98/PatchTST\n\n@article{Yuqietal-2022-PatchTST,\n  title={A Time Series is Worth 64 Words: Long-term Forecasting with Transformers},\n  author={Yuqi Nie and \n          Nam H. Nguyen and \n          Phanwadee Sinthong and \n          Jayant Kalagnanam},\n  journal={arXiv preprint arXiv:2211.14730},\n  year={2022}\n}\nPatchTST has shown some impressive results across some of the most widely used long-term datasets for benchmarking:\n\n\n\nimage.png\n\n\n\nsource\n\nSeriesDecomposition\n\n SeriesDecomposition (kernel_size:int)\n\nSeries decomposition block\n\n\n\n\nType\nDetails\n\n\n\n\nkernel_size\nint\nthe size of the window\n\n\n\n\nsource\n\n\nMovingAverage\n\n MovingAverage (kernel_size:int)\n\nMoving average block to highlight the trend of time series\n\n\n\n\nType\nDetails\n\n\n\n\nkernel_size\nint\nthe size of the window\n\n\n\n\nsource\n\n\nFlatten_Head\n\n Flatten_Head (individual, n_vars, nf, pred_dim)\n\nBase class for all neural network modules.\nYour models should also subclass this class.\nModules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes::\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 20, 5)\n        self.conv2 = nn.Conv2d(20, 20, 5)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        return F.relu(self.conv2(x))\nSubmodules assigned in this way will be registered, and will have their parameters converted too when you call :meth:to, etc.\n.. note:: As per the example above, an __init__() call to the parent class must be made before assignment on the child.\n:ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool\n\nsource\n\n\nPatchTST\n\n PatchTST (c_in, c_out, seq_len, pred_dim=None, n_layers=2, n_heads=8,\n           d_model=512, d_ff=2048, dropout=0.05, attn_dropout=0.0,\n           patch_len=16, stride=8, padding_patch=True, revin=True,\n           affine=False, individual=False, subtract_last=False,\n           decomposition=False, kernel_size=25, activation='gelu',\n           norm='BatchNorm', pre_norm=False, res_attention=True,\n           store_attn=False)\n\nBase class for all neural network modules.\nYour models should also subclass this class.\nModules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes::\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 20, 5)\n        self.conv2 = nn.Conv2d(20, 20, 5)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        return F.relu(self.conv2(x))\nSubmodules assigned in this way will be registered, and will have their parameters converted too when you call :meth:to, etc.\n.. note:: As per the example above, an __init__() call to the parent class must be made before assignment on the child.\n\nivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nc_in\n\n\nnumber of input channels\n\n\nc_out\n\n\nused for compatibility\n\n\nseq_len\n\n\ninput sequence length\n\n\npred_dim\nNoneType\nNone\nprediction sequence length\n\n\nn_layers\nint\n2\nnumber of encoder layers\n\n\nn_heads\nint\n8\nnumber of heads\n\n\nd_model\nint\n512\ndimension of model\n\n\nd_ff\nint\n2048\ndimension of fully connected network (fcn)\n\n\ndropout\nfloat\n0.05\ndropout applied to all linear layers in the encoder\n\n\nattn_dropout\nfloat\n0.0\ndropout applied to the attention scores\n\n\npatch_len\nint\n16\npatch_len\n\n\nstride\nint\n8\nstride\n\n\npadding_patch\nbool\nTrue\nflag to indicate if padded is added if necessary\n\n\nrevin\nbool\nTrue\nRevIN\n\n\naffine\nbool\nFalse\nRevIN affine\n\n\nindividual\nbool\nFalse\nindividual head\n\n\nsubtract_last\nbool\nFalse\nsubtract_last\n\n\ndecomposition\nbool\nFalse\napply decomposition\n\n\nkernel_size\nint\n25\ndecomposition kernel size\n\n\nactivation\nstr\ngelu\nactivation function of intermediate layer, relu or gelu.\n\n\nnorm\nstr\nBatchNorm\ntype of normalization layer used in the encoder\n\n\npre_norm\nbool\nFalse\nflag to indicate if normalization is applied as the first step in the sublayers\n\n\nres_attention\nbool\nTrue\nflag to indicate if Residual MultiheadAttention should be used\n\n\nstore_attn\nbool\nFalse\ncan be used to visualize attention weights\n\n\n\n\nfrom fastcore.test import test_eq\nfrom tsai.models.utils import count_parameters\n\nbs = 32\nc_in = 9  # aka channels, features, variables, dimensions\nc_out = 1\nseq_len = 60\npred_dim = 20\n\nxb = torch.randn(bs, c_in, seq_len)\n\narch_config=dict(\n        n_layers=3,  # number of encoder layers\n        n_heads=16,  # number of heads\n        d_model=128,  # dimension of model\n        d_ff=256,  # dimension of fully connected network (fcn)\n        attn_dropout=0.,\n        dropout=0.2,  # dropout applied to all linear layers in the encoder\n        patch_len=16,  # patch_len\n        stride=8,  # stride\n    )\n\nmodel = PatchTST(c_in, c_out, seq_len, pred_dim, **arch_config)\ntest_eq(model.to(xb.device)(xb).shape, [bs, c_in, pred_dim])\nprint(f'model parameters: {count_parameters(model)}')\n\nmodel parameters: 418470\n\n\n\n\nTest conversion to Torchscript\n\nimport gc\nimport os\nimport torch\nimport torch.nn as nn\nfrom fastcore.test import test_eq, test_close\n\n\nbs = 1\nnew_bs = 8\nc_in = 3\nc_out = 1\nseq_len = 96\npred_dim = 20\n\n# module\nmodel = PatchTST(c_in, c_out, seq_len, pred_dim)\nmodel = model.eval()\n\n# input data\ninp = torch.rand(bs, c_in, seq_len)\nnew_inp = torch.rand(new_bs, c_in, seq_len)\n\n# original\ntry:\n    output = model(inp)\n    new_output = model(new_inp)\n    print(f'{\"original\":10}: ok')\nexcept:\n    print(f'{\"original\":10}: failed')\n\n# tracing\ntry:\n    traced_model = torch.jit.trace(model, inp)\n    file_path = f\"_test_traced_model.pt\"\n    torch.jit.save(traced_model, file_path)\n    traced_model = torch.jit.load(file_path)\n    test_eq(output, traced_model(inp))\n    test_eq(new_output, traced_model(new_inp))\n    os.remove(file_path)\n    del traced_model\n    gc.collect()\n    print(f'{\"tracing\":10}: ok')\nexcept:\n    print(f'{\"tracing\":10}: failed')\n\n# scripting\ntry:\n    scripted_model = torch.jit.script(model)\n    file_path = f\"_test_scripted_model.pt\"\n    torch.jit.save(scripted_model, file_path)\n    scripted_model = torch.jit.load(file_path)\n    test_eq(output, scripted_model(inp))\n    test_eq(new_output, scripted_model(new_inp))\n    os.remove(file_path)\n    del scripted_model\n    gc.collect()\n    print(f'{\"scripting\":10}: ok')\nexcept:\n    print(f'{\"scripting\":10}: failed')\n\noriginal  : ok\ntracing   : ok\nscripting : failed\n\n\n\n\nTest conversion to onnx\n\ntry:\n    import onnx\n    import onnxruntime as ort\n    \n    try:\n        file_path = \"_model_cpu.onnx\"\n        torch.onnx.export(model.cpu(),               # model being run\n                        inp,                       # model input (or a tuple for multiple inputs)\n                        file_path,                 # where to save the model (can be a file or file-like object)\n                        input_names = ['input'],   # the model's input names\n                        output_names = ['output'], # the model's output names\n                        dynamic_axes={\n                            'input'  : {0 : 'batch_size'}, \n                            'output' : {0 : 'batch_size'}} # variable length axes\n                        )\n\n\n        # Load the model and check it's ok\n        onnx_model = onnx.load(file_path)\n        onnx.checker.check_model(onnx_model)\n        del onnx_model\n        gc.collect()\n\n        # New session\n        ort_sess = ort.InferenceSession(file_path)\n        output_onnx = ort_sess.run(None, {'input': inp.numpy()})[0]\n        test_close(output.detach().numpy(), output_onnx)\n        new_output_onnx = ort_sess.run(None, {'input': new_inp.numpy()})[0]\n        test_close(new_output.detach().numpy(), new_output_onnx)\n        os.remove(file_path)\n        print(f'{\"onnx\":10}: ok')\n    except:\n        print(f'{\"onnx\":10}: failed')\n\nexcept ImportError:\n    print('onnx and onnxruntime are not installed. Please install them to run this test')\n\nonnx and onnxruntime are not installed. Please install them to run this test"
  },
  {
    "objectID": "export.html",
    "href": "export.html",
    "title": "nb2py",
    "section": "",
    "text": "nb2py will allow you to convert the notebook (.ipynb) where the function is executed to a python script.\n\nThe conversion applies these rules:\n\nThe notebook will be automatically saved when the function is executed.\nOnly code cells will be converted (not markdown cells).\nA header will be added to indicate the script has been automatically generated. It also indicates where the original ipynb is.\nCells with a #hide flag won’t be converted. Flag variants like # hide, #Hide, #HIDE, … are also acceptable.\nEmpty cells and unnecessary empty lines within cells will be removed.\nBy default the script will be created with the same name and in the same folder of the original notebook. But you can pass a dir folder and a different name if you wish.\nIf a script with the same name already exists, it will be overwriten.\n\nThis code is required to identify flags in the notebook. We are looking for #hide flags.\nThis code automatically gets the name of the notebook. It’s been tested to work on Jupyter notebooks, Jupyter Lab and Google Colab.\n\nsource\n\nget_script_path\n\n get_script_path (nb_name=None)\n\n\nsource\n\n\nnb_name_to_py\n\n nb_name_to_py (nb_name)\n\n\nsource\n\n\nget_nb_path\n\n get_nb_path ()\n\nReturns the absolute path of the notebook, or raises a FileNotFoundError exception if it cannot be determined.\n\nsource\n\n\nget_colab_nb_name\n\n get_colab_nb_name ()\n\n\nsource\n\n\nget_nb_name\n\n get_nb_name (d=None)\n\nReturns the short name of the notebook w/o the .ipynb extension, or raises a FileNotFoundError exception if it cannot be determined.\nThis code is used when trying to save a file to google drive. We first need to mount the drive.\n\nsource\n\n\nnb2py\n\n nb2py (nb:str<absoluteorrelativefullpathtothenotebookyouwanttoconverttoap\n        ythonscript>=None, folder:str<absoluteorrelativepathtofolderofthes\n        criptyouwillcreate.Defaultstocurrentnb'sdirectory>=None, name:str<\n        nameofthescriptyouwanttocreate.Defaultstocurrentnbname.ipynbby.py>\n        =None, save:<savesthenbbeforeconvertingittoascript>=True,\n        run:<importandrunthescript>=False,\n        verbose:<controlsverbosity>=True)\n\nConverts a notebook to a python script in a predefined folder.\n\nif not is_colab():\n    nb = None\n    folder = None\n    name = None\n    pyname = nb2py(nb=nb, folder=folder, name=name)\n    if pyname is not None: \n        assert os.path.isfile(pyname)\n        os.remove(pyname)\n        assert not os.path.isfile(pyname)\n\n    nb = '000_export.ipynb'\n    folder = None\n    name = None\n    pyname = nb2py(nb=nb, folder=folder, name=name)\n    if pyname is not None: \n        assert os.path.isfile(pyname)\n        os.remove(pyname)\n        assert not os.path.isfile(pyname)\n\n    nb = '../nbs/000_export'\n    folder = None\n    name = None\n    pyname = nb2py(nb=nb, folder=folder, name=name)\n    if pyname is not None: \n        assert os.path.isfile(pyname)\n        os.remove(pyname)\n        assert not os.path.isfile(pyname)\n\n    nb = None\n    folder = '../test_export/'\n    name = None\n    pyname = nb2py(nb=nb, folder=folder, name=name)\n    if pyname is not None: \n        assert os.path.isfile(pyname)\n        shutil.rmtree(folder)\n        assert not os.path.isfile(pyname)\n\n\n\n\n000_export.ipynb saved at 2022-11-09 11:03:19.\n000_export.ipynb converted to /Users/nacho/notebooks/tsai/nbs/000_export.py\n000_export.ipynb converted to 000_export.py\n000_export.ipynb converted to ../nbs/000_export.py\n\n\n\n\n\n000_export.ipynb saved at 2022-11-09 11:03:21.\n000_export.ipynb converted to ../test_export/000_export.py"
  },
  {
    "objectID": "inference.html",
    "href": "inference.html",
    "title": "Inference",
    "section": "",
    "text": "Code required for inference.\n\n\nsource\n\nLearner.get_X_preds\n\n Learner.get_X_preds (X, y=None, bs=64, with_input=False,\n                      with_decoded=True, with_loss=False)\n\nGet the predictions and targets, optionally with_input and with_loss.\nwith_decoded will also return the decoded predictions (it reverses the transforms applied).\nThe order of the output is the following:\n\ninput (optional): if with_input is True\nprobabiblities (for classification) or predictions (for regression)\ntarget: if y is provided. Otherwise None.\npredictions: predicted labels. Predictions will be decoded if with_decoded=True.\nloss (optional): if with_loss is set to True and y is not None.\n\n\nfrom tsai.data.external import get_UCR_data\n\n\ndsid = 'OliveOil'\nX, y, splits = get_UCR_data(dsid, split_data=False)\nX_test = X[splits[1]]\ny_test = y[splits[1]]\n\n\nlearn = load_learner(\"./models/test.pth\")\n\n⚠️ Warning: load_learner (from fastai) requires all your custom code be in the exact same place as when exporting your Learner (the main script, or the module you imported it from).\n\ntest_probas, test_targets, test_preds = learn.get_X_preds(X_test, with_decoded=True)\ntest_probas, test_targets, test_preds\n\n(tensor([[0.2553, 0.2733, 0.2219, 0.2494],\n         [0.2553, 0.2733, 0.2220, 0.2494],\n         [0.2554, 0.2733, 0.2220, 0.2494],\n         [0.2553, 0.2733, 0.2220, 0.2494],\n         [0.2553, 0.2733, 0.2220, 0.2494],\n         [0.2553, 0.2733, 0.2220, 0.2494],\n         [0.2553, 0.2733, 0.2220, 0.2494],\n         [0.2553, 0.2733, 0.2220, 0.2494],\n         [0.2553, 0.2733, 0.2220, 0.2494],\n         [0.2553, 0.2733, 0.2220, 0.2494],\n         [0.2553, 0.2733, 0.2220, 0.2494],\n         [0.2554, 0.2733, 0.2220, 0.2494],\n         [0.2553, 0.2733, 0.2220, 0.2494],\n         [0.2553, 0.2733, 0.2220, 0.2494],\n         [0.2553, 0.2733, 0.2220, 0.2494],\n         [0.2553, 0.2734, 0.2220, 0.2492],\n         [0.2553, 0.2733, 0.2220, 0.2493],\n         [0.2553, 0.2733, 0.2220, 0.2494],\n         [0.2553, 0.2733, 0.2220, 0.2494],\n         [0.2553, 0.2733, 0.2220, 0.2495],\n         [0.2553, 0.2733, 0.2220, 0.2494],\n         [0.2553, 0.2733, 0.2220, 0.2494],\n         [0.2553, 0.2732, 0.2220, 0.2494],\n         [0.2553, 0.2732, 0.2220, 0.2495],\n         [0.2553, 0.2733, 0.2220, 0.2494],\n         [0.2553, 0.2732, 0.2220, 0.2495],\n         [0.2553, 0.2733, 0.2220, 0.2494],\n         [0.2553, 0.2733, 0.2220, 0.2494],\n         [0.2553, 0.2733, 0.2220, 0.2494],\n         [0.2553, 0.2733, 0.2220, 0.2494]]),\n None,\n array(['2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2',\n        '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2',\n        '2', '2', '2', '2'], dtype='<U1'))\n\n\n\ntest_probas2, test_targets2, test_preds2 = learn.get_X_preds(X_test, y_test, with_decoded=True)\ntest_probas2, test_targets2, test_preds2\n\n(tensor([[0.2553, 0.2733, 0.2219, 0.2494],\n         [0.2553, 0.2733, 0.2220, 0.2494],\n         [0.2554, 0.2733, 0.2220, 0.2494],\n         [0.2553, 0.2733, 0.2220, 0.2494],\n         [0.2553, 0.2733, 0.2220, 0.2494],\n         [0.2553, 0.2733, 0.2220, 0.2494],\n         [0.2553, 0.2733, 0.2220, 0.2494],\n         [0.2553, 0.2733, 0.2220, 0.2494],\n         [0.2553, 0.2733, 0.2220, 0.2494],\n         [0.2553, 0.2733, 0.2220, 0.2494],\n         [0.2553, 0.2733, 0.2220, 0.2494],\n         [0.2554, 0.2733, 0.2220, 0.2494],\n         [0.2553, 0.2733, 0.2220, 0.2494],\n         [0.2553, 0.2733, 0.2220, 0.2494],\n         [0.2553, 0.2733, 0.2220, 0.2494],\n         [0.2553, 0.2734, 0.2220, 0.2492],\n         [0.2553, 0.2733, 0.2220, 0.2493],\n         [0.2553, 0.2733, 0.2220, 0.2494],\n         [0.2553, 0.2733, 0.2220, 0.2494],\n         [0.2553, 0.2733, 0.2220, 0.2495],\n         [0.2553, 0.2733, 0.2220, 0.2494],\n         [0.2553, 0.2733, 0.2220, 0.2494],\n         [0.2553, 0.2732, 0.2220, 0.2494],\n         [0.2553, 0.2732, 0.2220, 0.2495],\n         [0.2553, 0.2733, 0.2220, 0.2494],\n         [0.2553, 0.2732, 0.2220, 0.2495],\n         [0.2553, 0.2733, 0.2220, 0.2494],\n         [0.2553, 0.2733, 0.2220, 0.2494],\n         [0.2553, 0.2733, 0.2220, 0.2494],\n         [0.2553, 0.2733, 0.2220, 0.2494]]),\n tensor([0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3,\n         3, 3, 3, 3, 3, 3]),\n array(['2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2',\n        '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2',\n        '2', '2', '2', '2'], dtype='<U1'))\n\n\n\ntest_probas3, test_targets3, test_preds3, test_losses3 = learn.get_X_preds(X_test, y_test, with_loss=True, with_decoded=True)\ntest_probas3, test_targets3, test_preds3, test_losses3\n\n(tensor([[0.2553, 0.2733, 0.2219, 0.2494],\n         [0.2553, 0.2733, 0.2220, 0.2494],\n         [0.2554, 0.2733, 0.2220, 0.2494],\n         [0.2553, 0.2733, 0.2220, 0.2494],\n         [0.2553, 0.2733, 0.2220, 0.2494],\n         [0.2553, 0.2733, 0.2220, 0.2494],\n         [0.2553, 0.2733, 0.2220, 0.2494],\n         [0.2553, 0.2733, 0.2220, 0.2494],\n         [0.2553, 0.2733, 0.2220, 0.2494],\n         [0.2553, 0.2733, 0.2220, 0.2494],\n         [0.2553, 0.2733, 0.2220, 0.2494],\n         [0.2554, 0.2733, 0.2220, 0.2494],\n         [0.2553, 0.2733, 0.2220, 0.2494],\n         [0.2553, 0.2733, 0.2220, 0.2494],\n         [0.2553, 0.2733, 0.2220, 0.2494],\n         [0.2553, 0.2734, 0.2220, 0.2492],\n         [0.2553, 0.2733, 0.2220, 0.2493],\n         [0.2553, 0.2733, 0.2220, 0.2494],\n         [0.2553, 0.2733, 0.2220, 0.2494],\n         [0.2553, 0.2733, 0.2220, 0.2495],\n         [0.2553, 0.2733, 0.2220, 0.2494],\n         [0.2553, 0.2733, 0.2220, 0.2494],\n         [0.2553, 0.2732, 0.2220, 0.2494],\n         [0.2553, 0.2732, 0.2220, 0.2495],\n         [0.2553, 0.2733, 0.2220, 0.2494],\n         [0.2553, 0.2732, 0.2220, 0.2495],\n         [0.2553, 0.2733, 0.2220, 0.2494],\n         [0.2553, 0.2733, 0.2220, 0.2494],\n         [0.2553, 0.2733, 0.2220, 0.2494],\n         [0.2553, 0.2733, 0.2220, 0.2494]]),\n tensor([0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3,\n         3, 3, 3, 3, 3, 3]),\n array(['2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2',\n        '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2',\n        '2', '2', '2', '2'], dtype='<U1'),\n TensorBase([1.3652, 1.3651, 1.3651, 1.3651, 1.3651, 1.2973, 1.2973, 1.2972,\n             1.2973, 1.2973, 1.2973, 1.2972, 1.2973, 1.2973, 1.5053, 1.5049,\n             1.5050, 1.5052, 1.3885, 1.3885, 1.3886, 1.3886, 1.3885, 1.3885,\n             1.3886, 1.3884, 1.3886, 1.3886, 1.3887, 1.3885]))\n\n\n\nfrom fastcore.test import test_eq\n\n\ntest_eq(test_probas, test_probas2)\ntest_eq(test_preds, test_preds2)\ntest_eq(test_probas, test_probas3)\ntest_eq(test_preds, test_preds3)"
  },
  {
    "objectID": "models.rnnplus.html",
    "href": "models.rnnplus.html",
    "title": "RNNPlus",
    "section": "",
    "text": "These are RNN, LSTM and GRU PyTorch implementations created by Ignacio Oguiza - oguiza@timeseriesAI.co\nThe idea of including a feature extractor to the RNN network comes from the solution developed by the UPSTAGE team (https://www.kaggle.com/songwonho, https://www.kaggle.com/limerobot and https://www.kaggle.com/jungikhyo). They finished in 3rd position in Kaggle’s Google Brain - Ventilator Pressure Prediction competition. They used a Conv1d + Stacked LSTM architecture.\nsource"
  },
  {
    "objectID": "models.rnnplus.html#converting-a-model-to-torchscript",
    "href": "models.rnnplus.html#converting-a-model-to-torchscript",
    "title": "RNNPlus",
    "section": "Converting a model to TorchScript",
    "text": "Converting a model to TorchScript\n\nmodel = GRUPlus(c_in, c_out, hidden_size=100, n_layers=2, bidirectional=True, rnn_dropout=.5, fc_dropout=.5)\nmodel.eval()\ninp = torch.rand(1, c_in, 50)\noutput = model(inp)\nprint(output)\n\ntensor([[-0.0677, -0.0857]], grad_fn=<AddmmBackward0>)\n\n\n\nTracing\n\n# save to gpu, cpu or both\ntraced_cpu = torch.jit.trace(model.cpu(), inp)\nprint(traced_cpu)\ntorch.jit.save(traced_cpu, \"cpu.pt\")\n\n# load cpu or gpu model\ntraced_cpu = torch.jit.load(\"cpu.pt\")\ntest_eq(traced_cpu(inp), output)\n\n!rm \"cpu.pt\"\n\nGRUPlus(\n  original_name=GRUPlus\n  (backbone): _RNN_Backbone(\n    original_name=_RNN_Backbone\n    (to_cat_embed): Identity(original_name=Identity)\n    (feature_extractor): Identity(original_name=Identity)\n    (rnn): Sequential(\n      original_name=Sequential\n      (0): GRU(original_name=GRU)\n      (1): LSTMOutput(original_name=LSTMOutput)\n    )\n    (transpose): Transpose(original_name=Transpose)\n  )\n  (head): Sequential(\n    original_name=Sequential\n    (0): LastStep(original_name=LastStep)\n    (1): Dropout(original_name=Dropout)\n    (2): Linear(original_name=Linear)\n  )\n)"
  },
  {
    "objectID": "models.rnnplus.html#converting-a-model-to-onnx",
    "href": "models.rnnplus.html#converting-a-model-to-onnx",
    "title": "RNNPlus",
    "section": "Converting a model to ONNX",
    "text": "Converting a model to ONNX\nimport onnx\n\ntorch.onnx.export(model.cpu(),               # model being run\n                  inp,                       # model input (or a tuple for multiple inputs)\n                  \"cpu.onnx\",                # where to save the model (can be a file or file-like object)\n                  export_params=True,        # store the trained parameter weights inside the model file\n                  verbose=False,\n                  opset_version=13,          # the ONNX version to export the model to\n                  do_constant_folding=True,  # whether to execute constant folding for optimization\n                  input_names = ['input'],   # the model's input names\n                  output_names = ['output'], # the model's output names\n                  dynamic_axes={\n                      'input'  : {0 : 'batch_size'}, \n                      'output' : {0 : 'batch_size'}} # variable length axes\n                 )\n\n\nonnx_model = onnx.load(\"cpu.onnx\")           # Load the model and check it's ok\nonnx.checker.check_model(onnx_model)\nimport onnxruntime as ort\n\nort_sess = ort.InferenceSession('cpu.onnx')\nout = ort_sess.run(None, {'input': inp.numpy()})\n\ninput_name = ort_sess.get_inputs()[0].name\noutput_name = ort_sess.get_outputs()[0].name\ninput_dims = ort_sess.get_inputs()[0].shape\n\ntest_close(out, output.detach().numpy())\n!rm \"cpu.onnx\""
  },
  {
    "objectID": "models.rocket.html",
    "href": "models.rocket.html",
    "title": "ROCKET",
    "section": "",
    "text": "ROCKET (RandOm Convolutional KErnel Transform) functions for univariate and multivariate time series.\n\n\nsource\n\nRocketClassifier\n\n RocketClassifier (num_kernels=10000, normalize_input=True,\n                   random_state=None, alphas=array([1.e-03, 1.e-02,\n                   1.e-01, 1.e+00, 1.e+01, 1.e+02, 1.e+03]),\n                   normalize_features=True, memory=None, verbose=False,\n                   scoring=None, class_weight=None, **kwargs)\n\nTime series classification using ROCKET features and a linear classifier\n\nsource\n\n\nload_rocket\n\n load_rocket (fname='Rocket', path='./models')\n\n\nsource\n\n\nRocketRegressor\n\n RocketRegressor (num_kernels=10000, normalize_input=True,\n                  random_state=None, alphas=array([1.e-03, 1.e-02, 1.e-01,\n                  1.e+00, 1.e+01, 1.e+02, 1.e+03]),\n                  normalize_features=True, memory=None, verbose=False,\n                  scoring=None, **kwargs)\n\nTime series regression using ROCKET features and a linear regressor\n\n# Univariate classification with sklearn-type API\ndsid = 'OliveOil'\nfname = 'RocketClassifier'\nX_train, y_train, X_test, y_test = get_UCR_data(dsid, Xdtype='float64')\ncls = RocketClassifier()\ncls.fit(X_train, y_train)\ncls.save(fname)\ndel cls\ncls = load_rocket(fname)\nprint(cls.score(X_test, y_test))\n\n0.9333333333333333\n\n\n\n# Multivariate classification with sklearn-type API\ndsid = 'NATOPS'\nfname = 'RocketClassifier'\nX_train, y_train, X_test, y_test = get_UCR_data(dsid, Xdtype='float64')\ncls = RocketClassifier()\ncls.fit(X_train, y_train)\ncls.save(fname)\ndel cls\ncls = load_rocket(fname)\nprint(cls.score(X_test, y_test))\n\n0.8833333333333333\n\n\n\nfrom sklearn.metrics import mean_squared_error\n\n\n# Univariate regression with sklearn-type API\ndsid = 'Covid3Month'\nfname = 'RocketRegressor'\nX_train, y_train, X_test, y_test = get_Monash_regression_data(dsid, Xdtype='float64')\nif X_train is not None: \n    rmse_scorer = make_scorer(mean_squared_error, greater_is_better=False)\n    reg = RocketRegressor(scoring=rmse_scorer)\n    reg.fit(X_train, y_train)\n    reg.save(fname)\n    del reg\n    reg = load_rocket(fname)\n    y_pred = reg.predict(X_test)\n    print(mean_squared_error(y_test, y_pred, squared=False))\n\n0.04256122787164726\n\n\n\n# Multivariate regression with sklearn-type API\ndsid = 'AppliancesEnergy'\nfname = 'RocketRegressor'\nX_train, y_train, X_test, y_test = get_Monash_regression_data(dsid, Xdtype='float64')\nif X_train is not None: \n    rmse_scorer = make_scorer(mean_squared_error, greater_is_better=False)\n    reg = RocketRegressor(scoring=rmse_scorer)\n    reg.fit(X_train, y_train)\n    reg.save(fname)\n    del reg\n    reg = load_rocket(fname)\n    y_pred = reg.predict(X_test)\n    print(mean_squared_error(y_test, y_pred, squared=False))\n\n2.302914529495251"
  },
  {
    "objectID": "models.tsperceiver.html",
    "href": "models.tsperceiver.html",
    "title": "TSPerceiver",
    "section": "",
    "text": "This implementation is inspired by:\nJaegle, A., Gimeno, F., Brock, A., Zisserman, A., Vinyals, O., & Carreira, J. (2021).\nPerceiver: General Perception with Iterative Attention. arXiv preprint arXiv:2103.03206.\nPaper: https://arxiv.org/pdf/2103.03206.pdf\nOfficial repo: Not available as og April, 2021.\n\nsource\n\nTSPerceiver\n\n TSPerceiver (c_in, c_out, seq_len, cat_szs=0, n_cont=0, n_latents=512,\n              d_latent=128, d_context=None, n_layers=6,\n              self_per_cross_attn=1, share_weights=True, cross_n_heads=1,\n              self_n_heads=8, d_head=None, attn_dropout=0.0,\n              fc_dropout=0.0, concat_pool=False)\n\nSame as nn.Module, but no need for subclasses to call super().__init__\n\nfrom tsai.basics import *\nfrom tsai.data.all import *\n\n\ndsid = 'OliveOil'\nX, y, splits = get_UCR_data(dsid, split_data=False)\nts_features_df = get_ts_features(X, y)\nts_features_df.shape\n\nFeature Extraction: 100%|██████████████████████████████████████████| 30/30 [00:00<00:00, 189.16it/s]\n\n\n(60, 11)\n\n\n\n# raw ts\ntfms  = [None, [Categorize()]]\nbatch_tfms = TSStandardize(by_sample=True)\nts_dls = get_ts_dls(X, y, splits=splits, tfms=tfms, batch_tfms=batch_tfms)\n\n# ts features\ncat_names = None\ncont_names = ts_features_df.columns[:-2]\ny_names = 'target'\ntab_dls = get_tabular_dls(ts_features_df, cat_names=cat_names, cont_names=cont_names, y_names=y_names, splits=splits)\n\n# mixed\nmixed_dls = get_mixed_dls(ts_dls, tab_dls)\nxb, yb = mixed_dls.one_batch()\n\n\nmodel = TSPerceiver(ts_dls.vars, ts_dls.c, ts_dls.len, cat_szs=0, \n                    # n_cont=0, \n                    n_cont=xb[1][1].shape[1], \n                    n_latents=128, d_latent=128, n_layers=3, self_per_cross_attn=1, share_weights=True,\n                    cross_n_heads=16, self_n_heads=16, d_head=None, attn_dropout=0., fc_dropout=0.).to(device)\ntest_eq(model(xb).shape, (yb.shape[0], len(np.unique(y))))"
  },
  {
    "objectID": "models.utils.html",
    "href": "models.utils.html",
    "title": "Model utilities",
    "section": "",
    "text": "Utility functions used to build PyTorch timeseries models.\n\n\nsource\n\napply_idxs\n\n apply_idxs (o, idxs)\n\nFunction to apply indices to zarr, dask and numpy arrays\n\nsource\n\n\nSeqTokenizer\n\n SeqTokenizer (c_in, embed_dim, token_size=60, norm=False)\n\nGenerates non-overlapping tokens from sub-sequences within a sequence by applying a sliding window\n\nsource\n\n\nget_embed_size\n\n get_embed_size (n_cat, rule='log2')\n\n\ntest_eq(get_embed_size(35), 6)\n\n\nsource\n\n\nhas_weight_or_bias\n\n has_weight_or_bias (l)\n\n\nsource\n\n\nhas_weight\n\n has_weight (l)\n\n\nsource\n\n\nhas_bias\n\n has_bias (l)\n\n\nsource\n\n\nis_conv\n\n is_conv (l)\n\n\nsource\n\n\nis_affine_layer\n\n is_affine_layer (l)\n\n\nsource\n\n\nis_conv_linear\n\n is_conv_linear (l)\n\n\nsource\n\n\nis_bn\n\n is_bn (l)\n\n\nsource\n\n\nis_linear\n\n is_linear (l)\n\n\nsource\n\n\nis_layer\n\n is_layer (*args)\n\n\nsource\n\n\nget_layers\n\n get_layers (model, cond=<function noop>, full=True)\n\n\nsource\n\n\ncheck_weight\n\n check_weight (m, cond=<function noop>, verbose=False)\n\n\nsource\n\n\ncheck_bias\n\n check_bias (m, cond=<function noop>, verbose=False)\n\n\nsource\n\n\nget_nf\n\n get_nf (m)\n\nGet nf from model’s first linear layer in head\n\nsource\n\n\nts_splitter\n\n ts_splitter (m)\n\nSplit of a model between body and head\n\nsource\n\n\ntransfer_weights\n\n transfer_weights (model, weights_path:pathlib.Path,\n                   device:torch.device=None, exclude_head:bool=True)\n\nUtility function that allows to easily transfer weights between models. Taken from the great self-supervised repository created by Kerem Turgutlu. https://github.com/KeremTurgutlu/self_supervised/blob/d87ebd9b4961c7da0efd6073c42782bbc61aaa2e/self_supervised/utils.py\n\nsource\n\n\nbuild_ts_model\n\n build_ts_model (arch, c_in=None, c_out=None, seq_len=None, d=None,\n                 dls=None, device=None, verbose=False, pretrained=False,\n                 weights_path=None, exclude_head=True, cut=-1, init=None,\n                 arch_config={}, **kwargs)\n\n\nsource\n\n\ncount_parameters\n\n count_parameters (model, trainable=True)\n\n\nsource\n\n\nbuild_tsimage_model\n\n build_tsimage_model (arch, c_in=None, c_out=None, dls=None,\n                      pretrained=False, device=None, verbose=False,\n                      init=None, arch_config={}, **kwargs)\n\n\nsource\n\n\nbuild_tabular_model\n\n build_tabular_model (arch, dls, layers=None, emb_szs=None, n_out=None,\n                      y_range=None, device=None, arch_config={}, **kwargs)\n\n\nfrom tsai.data.external import get_UCR_data\nfrom tsai.data.core import TSCategorize, get_ts_dls\nfrom tsai.data.preprocessing import TSStandardize\nfrom tsai.models.InceptionTime import *\n\n\nX, y, splits = get_UCR_data('NATOPS', split_data=False)\ntfms = [None, TSCategorize()]\nbatch_tfms = TSStandardize()\ndls = get_ts_dls(X, y, splits, tfms=tfms, batch_tfms=batch_tfms)\nmodel = build_ts_model(InceptionTime, dls=dls)\ntest_eq(count_parameters(model), 460038)\n\n\nsource\n\n\nget_clones\n\n get_clones (module, N)\n\n\nm = nn.Conv1d(3,4,3)\nget_clones(m, 3)\n\nModuleList(\n  (0): Conv1d(3, 4, kernel_size=(3,), stride=(1,))\n  (1): Conv1d(3, 4, kernel_size=(3,), stride=(1,))\n  (2): Conv1d(3, 4, kernel_size=(3,), stride=(1,))\n)\n\n\n\nsource\n\n\nsplit_model\n\n split_model (m)\n\n\nsource\n\n\noutput_size_calculator\n\n output_size_calculator (mod, c_in, seq_len=None)\n\n\nc_in = 3\nseq_len = 30\nm = nn.Conv1d(3, 12, kernel_size=3, stride=2)\nnew_c_in, new_seq_len = output_size_calculator(m, c_in, seq_len)\ntest_eq((new_c_in, new_seq_len), (12, 14))\n\n[W NNPACK.cpp:53] Could not initialize NNPACK! Reason: Unsupported hardware.\n\n\n\nsource\n\n\nchange_model_head\n\n change_model_head (model, custom_head, **kwargs)\n\nReplaces a model’s head by a custom head as long as the model has a head, head_nf, c_out and seq_len attributes\n\nsource\n\n\ntrue_forecaster\n\n true_forecaster (o, split, horizon=1)\n\n\nsource\n\n\nnaive_forecaster\n\n naive_forecaster (o, split, horizon=1)\n\n\na = np.random.rand(20).cumsum()\nsplit = np.arange(10, 20)\na, naive_forecaster(a, split, 1), true_forecaster(a, split, 1)\n\n(array([0.64747126, 0.92030628, 1.16086401, 1.83041763, 1.84921046,\n        2.39911269, 3.06319539, 3.44398946, 3.97081575, 4.61491852,\n        5.55827365, 5.86428861, 6.4842958 , 7.06224848, 7.59103433,\n        7.83621709, 8.5838799 , 8.65897873, 9.43034829, 9.97025373]),\n array([4.61491852, 5.55827365, 5.86428861, 6.4842958 , 7.06224848,\n        7.59103433, 7.83621709, 8.5838799 , 8.65897873, 9.43034829]),\n array([5.55827365, 5.86428861, 6.4842958 , 7.06224848, 7.59103433,\n        7.83621709, 8.5838799 , 8.65897873, 9.43034829, 9.97025373]))"
  },
  {
    "objectID": "data.tabular.html",
    "href": "data.tabular.html",
    "title": "Time Series Tabular Data",
    "section": "",
    "text": "Main Tabular functions used throughout the library. This is helpful when you have additional time series data like metadata, time series features, etc.\n\n\nsource\n\nget_tabular_ds\n\n get_tabular_ds (df, procs=[<class 'fastai.tabular.core.Categorify'>,\n                 <class 'fastai.tabular.core.FillMissing'>, <class\n                 'fastai.data.transforms.Normalize'>], cat_names=None,\n                 cont_names=None, y_names=None, groupby=None,\n                 y_block=None, splits=None, do_setup=True, inplace=False,\n                 reduce_memory=True, device=None)\n\n\nsource\n\n\nget_tabular_dls\n\n get_tabular_dls (df, procs=[<class 'fastai.tabular.core.Categorify'>,\n                  <class 'fastai.tabular.core.FillMissing'>, <class\n                  'fastai.data.transforms.Normalize'>], cat_names=None,\n                  cont_names=None, y_names=None, bs=64, y_block=None,\n                  splits=None, do_setup=True, inplace=False,\n                  reduce_memory=True, device=None,\n                  path:Union[str,pathlib.Path]='.')\n\n\nsource\n\n\npreprocess_df\n\n preprocess_df (df, procs=[<class 'fastai.tabular.core.Categorify'>,\n                <class 'fastai.tabular.core.FillMissing'>, <class\n                'fastai.data.transforms.Normalize'>], cat_names=None,\n                cont_names=None, y_names=None, sample_col=None,\n                reduce_memory=True)\n\n\npath = untar_data(URLs.ADULT_SAMPLE)\ndf = pd.read_csv(path/'adult.csv')\n# df['salary'] = np.random.rand(len(df)) # uncomment to simulate a cont dependent variable\n\ncat_names = ['workclass', 'education', 'education-num', 'marital-status', 'occupation', 'relationship', 'race', 'sex',\n             'capital-gain', 'capital-loss', 'native-country']\ncont_names = ['age', 'fnlwgt', 'hours-per-week']\ntarget = ['salary']\nsplits = RandomSplitter()(range_of(df))\n\ndls = get_tabular_dls(df, cat_names=cat_names, cont_names=cont_names, y_names='salary', splits=splits, bs=512, device=device)\ndls.show_batch()\n\n\n\n  \n    \n      \n      workclass\n      education\n      education-num\n      marital-status\n      occupation\n      relationship\n      race\n      sex\n      capital-gain\n      capital-loss\n      native-country\n      age\n      fnlwgt\n      hours-per-week\n      salary\n    \n  \n  \n    \n      0\n      Private\n      Some-college\n      10.0\n      Divorced\n      Exec-managerial\n      Not-in-family\n      White\n      Male\n      0\n      0\n      United-States\n      48.000000\n      190072.000005\n      50.000000\n      >=50k\n    \n    \n      1\n      Self-emp-not-inc\n      Some-college\n      10.0\n      Married-civ-spouse\n      Sales\n      Husband\n      White\n      Male\n      0\n      0\n      United-States\n      72.000001\n      284120.002964\n      40.000000\n      <50k\n    \n    \n      2\n      Private\n      Some-college\n      10.0\n      Married-civ-spouse\n      Protective-serv\n      Husband\n      Black\n      Male\n      0\n      0\n      United-States\n      72.000001\n      53684.002497\n      40.000000\n      <50k\n    \n    \n      3\n      Self-emp-inc\n      Some-college\n      10.0\n      Married-civ-spouse\n      Farming-fishing\n      Husband\n      White\n      Male\n      0\n      0\n      United-States\n      47.000000\n      337049.998875\n      40.000000\n      <50k\n    \n    \n      4\n      Private\n      HS-grad\n      9.0\n      Divorced\n      Craft-repair\n      Not-in-family\n      White\n      Male\n      0\n      0\n      United-States\n      46.000000\n      207677.000707\n      30.000000\n      <50k\n    \n    \n      5\n      Private\n      5th-6th\n      3.0\n      Divorced\n      Priv-house-serv\n      Unmarried\n      White\n      Female\n      0\n      0\n      Mexico\n      45.000000\n      265082.999142\n      35.000000\n      <50k\n    \n    \n      6\n      Private\n      Assoc-acdm\n      12.0\n      Never-married\n      Other-service\n      Not-in-family\n      White\n      Female\n      0\n      0\n      United-States\n      28.000000\n      150296.001328\n      79.999999\n      <50k\n    \n    \n      7\n      Private\n      HS-grad\n      9.0\n      Married-civ-spouse\n      Exec-managerial\n      Husband\n      White\n      Male\n      0\n      0\n      United-States\n      50.000000\n      94080.999353\n      40.000000\n      >=50k\n    \n    \n      8\n      Private\n      Assoc-voc\n      11.0\n      Married-civ-spouse\n      Exec-managerial\n      Husband\n      White\n      Male\n      0\n      0\n      Germany\n      58.000000\n      235624.000302\n      40.000000\n      >=50k\n    \n    \n      9\n      Private\n      HS-grad\n      9.0\n      Never-married\n      Other-service\n      Unmarried\n      Black\n      Female\n      0\n      0\n      Japan\n      29.000000\n      419721.008996\n      40.000000\n      <50k\n    \n  \n\n\n\n\nmetrics = mae if dls.c == 1 else accuracy\nlearn = tabular_learner(dls, layers=[200, 100], y_range=None, metrics=metrics)\nlearn.fit(1, 1e-2)\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      accuracy\n      time\n    \n  \n  \n    \n      0\n      0.349525\n      0.288922\n      0.866093\n      00:05\n    \n  \n\n\n\n\nlearn.dls.one_batch()\n\n(tensor([[  5,  12,   9,  ...,   1,   1,  21],\n         [  1,  10,  13,  ...,   1,   1,   3],\n         [  5,   4,   2,  ...,   1,   1,   6],\n         ...,\n         [  5,   6,   4,  ...,   1,   1,  40],\n         [  3,  10,  13,  ...,   1,   1,  40],\n         [  5,  12,   9,  ..., 116,   1,  40]]),\n tensor([[-0.2593,  0.1234,  1.1829],\n         [-0.9913, -1.4041, -0.0347],\n         [-0.1129,  0.4583, -0.0347],\n         ...,\n         [-1.5769, -0.1989,  0.3712],\n         [ 0.4727, -1.4400,  0.3712],\n         [ 1.5708, -0.2222, -0.0347]]),\n tensor([[1],\n         [0],\n         [0],\n         [0],\n         [0],\n         [0],\n         [0],\n         [0],\n         [0],\n         [0],\n         [0],\n         [1],\n         [0],\n         [0],\n         [0],\n         [0],\n         [0],\n         [1],\n         [0],\n         [0],\n         [1],\n         [0],\n         [0],\n         [0],\n         [1],\n         [1],\n         [1],\n         [0],\n         [0],\n         [1],\n         [1],\n         [0],\n         [0],\n         [0],\n         [1],\n         [0],\n         [0],\n         [0],\n         [0],\n         [0],\n         [1],\n         [0],\n         [0],\n         [0],\n         [0],\n         [0],\n         [1],\n         [1],\n         [1],\n         [1],\n         [0],\n         [0],\n         [0],\n         [0],\n         [0],\n         [0],\n         [0],\n         [0],\n         [0],\n         [0],\n         [1],\n         [0],\n         [0],\n         [1],\n         [0],\n         [1],\n         [0],\n         [0],\n         [0],\n         [0],\n         [0],\n         [1],\n         [1],\n         [0],\n         [0],\n         [0],\n         [1],\n         [0],\n         [0],\n         [1],\n         [0],\n         [0],\n         [0],\n         [0],\n         [0],\n         [0],\n         [0],\n         [0],\n         [1],\n         [0],\n         [0],\n         [0],\n         [0],\n         [0],\n         [0],\n         [0],\n         [0],\n         [1],\n         [0],\n         [0],\n         [1],\n         [0],\n         [0],\n         [0],\n         [0],\n         [0],\n         [0],\n         [0],\n         [0],\n         [0],\n         [0],\n         [1],\n         [1],\n         [0],\n         [1],\n         [1],\n         [0],\n         [0],\n         [0],\n         [0],\n         [0],\n         [0],\n         [1],\n         [0],\n         [0],\n         [0],\n         [0],\n         [1],\n         [0],\n         [0],\n         [0],\n         [0],\n         [0],\n         [0],\n         [0],\n         [0],\n         [0],\n         [0],\n         [0],\n         [0],\n         [1],\n         [0],\n         [1],\n         [0],\n         [1],\n         [0],\n         [0],\n         [1],\n         [0],\n         [1],\n         [1],\n         [0],\n         [1],\n         [0],\n         [0],\n         [0],\n         [0],\n         [1],\n         [1],\n         [1],\n         [1],\n         [1],\n         [0],\n         [0],\n         [0],\n         [0],\n         [0],\n         [0],\n         [0],\n         [1],\n         [0],\n         [0],\n         [0],\n         [0],\n         [0],\n         [0],\n         [1],\n         [0],\n         [0],\n         [0],\n         [0],\n         [0],\n         [0],\n         [1],\n         [0],\n         [1],\n         [1],\n         [0],\n         [0],\n         [0],\n         [1],\n         [1],\n         [1],\n         [0],\n         [0],\n         [0],\n         [0],\n         [0],\n         [1],\n         [0],\n         [0],\n         [0],\n         [1],\n         [0],\n         [0],\n         [0],\n         [0],\n         [0],\n         [0],\n         [1],\n         [0],\n         [0],\n         [0],\n         [0],\n         [0],\n         [0],\n         [1],\n         [1],\n         [0],\n         [1],\n         [0],\n         [0],\n         [0],\n         [1],\n         [0],\n         [0],\n         [0],\n         [0],\n         [0],\n         [0],\n         [0],\n         [1],\n         [1],\n         [0],\n         [0],\n         [0],\n         [0],\n         [0],\n         [0],\n         [0],\n         [0],\n         [0],\n         [1],\n         [0],\n         [1],\n         [1],\n         [0],\n         [0],\n         [1],\n         [0],\n         [0],\n         [0],\n         [0],\n         [0],\n         [0],\n         [1],\n         [1],\n         [1],\n         [0],\n         [0],\n         [0],\n         [0],\n         [0],\n         [0],\n         [0],\n         [0],\n         [0],\n         [0],\n         [0],\n         [0],\n         [0],\n         [0],\n         [0],\n         [0],\n         [0],\n         [0],\n         [0],\n         [0],\n         [1],\n         [0],\n         [0],\n         [1],\n         [0],\n         [1],\n         [0],\n         [1],\n         [1],\n         [0],\n         [0],\n         [0],\n         [0],\n         [0],\n         [0],\n         [1],\n         [0],\n         [1],\n         [1],\n         [1],\n         [0],\n         [0],\n         [0],\n         [1],\n         [1],\n         [1],\n         [0],\n         [1],\n         [1],\n         [0],\n         [1],\n         [1],\n         [1],\n         [1],\n         [0],\n         [0],\n         [0],\n         [0],\n         [0],\n         [0],\n         [0],\n         [0],\n         [0],\n         [0],\n         [0],\n         [0],\n         [0],\n         [0],\n         [1],\n         [1],\n         [0],\n         [0],\n         [0],\n         [0],\n         [0],\n         [0],\n         [0],\n         [1],\n         [0],\n         [0],\n         [0],\n         [0],\n         [0],\n         [0],\n         [0],\n         [0],\n         [1],\n         [0],\n         [0],\n         [0],\n         [0],\n         [0],\n         [0],\n         [0],\n         [0],\n         [0],\n         [1],\n         [0],\n         [0],\n         [0],\n         [0],\n         [0],\n         [0],\n         [0],\n         [0],\n         [0],\n         [0],\n         [0],\n         [0],\n         [0],\n         [0],\n         [0],\n         [0],\n         [1],\n         [0],\n         [0],\n         [0],\n         [0],\n         [0],\n         [1],\n         [0],\n         [0],\n         [0],\n         [1],\n         [0],\n         [0],\n         [1],\n         [0],\n         [1],\n         [1],\n         [0],\n         [1],\n         [0],\n         [0],\n         [0],\n         [0],\n         [0],\n         [0],\n         [0],\n         [1],\n         [0],\n         [0],\n         [0],\n         [0],\n         [0],\n         [0],\n         [0],\n         [0],\n         [0],\n         [0],\n         [0],\n         [1],\n         [0],\n         [0],\n         [0],\n         [1],\n         [0],\n         [0],\n         [0],\n         [0],\n         [0],\n         [0],\n         [0],\n         [1],\n         [0],\n         [0],\n         [1],\n         [0],\n         [1],\n         [1],\n         [1],\n         [0],\n         [1],\n         [0],\n         [1],\n         [0],\n         [0],\n         [1],\n         [0],\n         [0],\n         [0],\n         [0],\n         [0],\n         [1],\n         [0],\n         [1],\n         [1],\n         [0],\n         [1],\n         [0],\n         [0],\n         [0],\n         [0],\n         [0],\n         [0],\n         [0],\n         [0],\n         [0],\n         [1],\n         [0],\n         [1],\n         [0],\n         [0],\n         [0],\n         [1],\n         [0],\n         [0],\n         [0],\n         [0],\n         [1],\n         [1],\n         [0],\n         [0],\n         [1],\n         [0],\n         [0],\n         [0],\n         [0],\n         [0],\n         [0],\n         [1],\n         [0],\n         [0],\n         [0],\n         [0],\n         [0],\n         [0],\n         [1],\n         [0],\n         [1],\n         [0],\n         [0],\n         [0],\n         [1],\n         [0],\n         [0],\n         [0],\n         [0],\n         [0],\n         [0],\n         [0],\n         [0],\n         [0],\n         [0],\n         [1],\n         [0],\n         [0],\n         [0],\n         [0],\n         [1],\n         [0],\n         [0],\n         [1],\n         [1]], dtype=torch.int8))\n\n\n\nlearn.model\n\nTabularModel(\n  (embeds): ModuleList(\n    (0): Embedding(10, 6)\n    (1): Embedding(17, 8)\n    (2): Embedding(17, 8)\n    (3): Embedding(8, 5)\n    (4): Embedding(16, 8)\n    (5): Embedding(7, 5)\n    (6): Embedding(6, 4)\n    (7): Embedding(3, 3)\n    (8): Embedding(117, 23)\n    (9): Embedding(90, 20)\n    (10): Embedding(43, 13)\n  )\n  (emb_drop): Dropout(p=0.0, inplace=False)\n  (bn_cont): BatchNorm1d(3, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (layers): Sequential(\n    (0): LinBnDrop(\n      (0): Linear(in_features=106, out_features=200, bias=False)\n      (1): ReLU(inplace=True)\n      (2): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (1): LinBnDrop(\n      (0): Linear(in_features=200, out_features=100, bias=False)\n      (1): ReLU(inplace=True)\n      (2): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (2): LinBnDrop(\n      (0): Linear(in_features=100, out_features=2, bias=True)\n    )\n  )\n)\n\n\n\npath = untar_data(URLs.ADULT_SAMPLE)\ndf = pd.read_csv(path/'adult.csv')\ncat_names = ['workclass', 'education', 'education-num', 'marital-status', 'occupation', 'relationship', 'race', 'sex',\n             'capital-gain', 'capital-loss', 'native-country']\ncont_names = ['age', 'fnlwgt', 'hours-per-week']\ntarget = ['salary']\ndf, procs = preprocess_df(df, procs=[Categorify, FillMissing, Normalize], cat_names=cat_names, cont_names=cont_names, y_names=target, \n                          sample_col=None, reduce_memory=True)\ndf.head()\n\n\n\n\n\n  \n    \n      \n      workclass\n      education\n      education-num\n      marital-status\n      occupation\n      relationship\n      race\n      sex\n      capital-gain\n      capital-loss\n      native-country\n      age\n      fnlwgt\n      hours-per-week\n      salary\n    \n  \n  \n    \n      0\n      5\n      8\n      12\n      3\n      0\n      6\n      5\n      1\n      1\n      48\n      40\n      0.763796\n      -0.838084\n      -0.035429\n      1\n    \n    \n      1\n      5\n      13\n      14\n      1\n      5\n      2\n      5\n      2\n      101\n      1\n      40\n      0.397233\n      0.444987\n      0.369519\n      1\n    \n    \n      2\n      5\n      12\n      0\n      1\n      0\n      5\n      3\n      1\n      1\n      1\n      40\n      -0.042642\n      -0.886734\n      -0.683348\n      0\n    \n    \n      3\n      6\n      15\n      15\n      3\n      11\n      1\n      2\n      2\n      1\n      1\n      40\n      -0.042642\n      -0.728873\n      -0.035429\n      1\n    \n    \n      4\n      7\n      6\n      0\n      3\n      9\n      6\n      3\n      1\n      1\n      1\n      40\n      0.250608\n      -1.018314\n      0.774468\n      0\n    \n  \n\n\n\n\n\nprocs.classes, procs.means, procs.stds\n\n({'workclass': ['#na#', ' ?', ' Federal-gov', ' Local-gov', ' Never-worked', ' Private', ' Self-emp-inc', ' Self-emp-not-inc', ' State-gov', ' Without-pay'],\n  'education': ['#na#', ' 10th', ' 11th', ' 12th', ' 1st-4th', ' 5th-6th', ' 7th-8th', ' 9th', ' Assoc-acdm', ' Assoc-voc', ' Bachelors', ' Doctorate', ' HS-grad', ' Masters', ' Preschool', ' Prof-school', ' Some-college'],\n  'education-num': ['#na#', 1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0, 11.0, 12.0, 13.0, 14.0, 15.0, 16.0],\n  'marital-status': ['#na#', ' Divorced', ' Married-AF-spouse', ' Married-civ-spouse', ' Married-spouse-absent', ' Never-married', ' Separated', ' Widowed'],\n  'occupation': ['#na#', ' ?', ' Adm-clerical', ' Armed-Forces', ' Craft-repair', ' Exec-managerial', ' Farming-fishing', ' Handlers-cleaners', ' Machine-op-inspct', ' Other-service', ' Priv-house-serv', ' Prof-specialty', ' Protective-serv', ' Sales', ' Tech-support', ' Transport-moving'],\n  'relationship': ['#na#', ' Husband', ' Not-in-family', ' Other-relative', ' Own-child', ' Unmarried', ' Wife'],\n  'race': ['#na#', ' Amer-Indian-Eskimo', ' Asian-Pac-Islander', ' Black', ' Other', ' White'],\n  'sex': ['#na#', ' Female', ' Male'],\n  'capital-gain': ['#na#', 0, 114, 401, 594, 914, 991, 1055, 1086, 1111, 1151, 1173, 1409, 1424, 1455, 1471, 1506, 1639, 1797, 1831, 1848, 2009, 2036, 2050, 2062, 2105, 2174, 2176, 2202, 2228, 2290, 2329, 2346, 2354, 2387, 2407, 2414, 2463, 2538, 2580, 2597, 2635, 2653, 2829, 2885, 2907, 2936, 2961, 2964, 2977, 2993, 3103, 3137, 3273, 3325, 3411, 3418, 3432, 3456, 3464, 3471, 3674, 3781, 3818, 3887, 3908, 3942, 4064, 4101, 4386, 4416, 4508, 4650, 4687, 4787, 4865, 4931, 4934, 5013, 5060, 5178, 5455, 5556, 5721, 6097, 6360, 6418, 6497, 6514, 6723, 6767, 6849, 7298, 7430, 7443, 7688, 7896, 7978, 8614, 9386, 9562, 10520, 10566, 10605, 11678, 13550, 14084, 14344, 15020, 15024, 15831, 18481, 20051, 22040, 25124, 25236, 27828, 34095, 41310, 99999],\n  'capital-loss': ['#na#', 0, 155, 213, 323, 419, 625, 653, 810, 880, 974, 1092, 1138, 1258, 1340, 1380, 1408, 1411, 1485, 1504, 1539, 1564, 1573, 1579, 1590, 1594, 1602, 1617, 1628, 1648, 1651, 1668, 1669, 1672, 1719, 1721, 1726, 1735, 1740, 1741, 1755, 1762, 1816, 1825, 1844, 1848, 1876, 1887, 1902, 1944, 1974, 1977, 1980, 2001, 2002, 2042, 2051, 2057, 2080, 2129, 2149, 2163, 2174, 2179, 2201, 2205, 2206, 2231, 2238, 2246, 2258, 2267, 2282, 2339, 2352, 2377, 2392, 2415, 2444, 2457, 2467, 2472, 2489, 2547, 2559, 2603, 2754, 2824, 3004, 3683, 3770, 3900, 4356],\n  'native-country': ['#na#', ' ?', ' Cambodia', ' Canada', ' China', ' Columbia', ' Cuba', ' Dominican-Republic', ' Ecuador', ' El-Salvador', ' England', ' France', ' Germany', ' Greece', ' Guatemala', ' Haiti', ' Holand-Netherlands', ' Honduras', ' Hong', ' Hungary', ' India', ' Iran', ' Ireland', ' Italy', ' Jamaica', ' Japan', ' Laos', ' Mexico', ' Nicaragua', ' Outlying-US(Guam-USVI-etc)', ' Peru', ' Philippines', ' Poland', ' Portugal', ' Puerto-Rico', ' Scotland', ' South', ' Taiwan', ' Thailand', ' Trinadad&Tobago', ' United-States', ' Vietnam', ' Yugoslavia']},\n {'age': 38.58164675532078,\n  'fnlwgt': 189778.36651208502,\n  'hours-per-week': 40.437455852092995},\n {'age': 13.640223192304274,\n  'fnlwgt': 105548.3568809908,\n  'hours-per-week': 12.347239175707989})"
  },
  {
    "objectID": "optuna.html",
    "href": "optuna.html",
    "title": "Optuna",
    "section": "",
    "text": "A hyperparameter optimization framework\n\nOptuna is an automatic hyperparameter optimization software framework, particularly designed for machine learning. It features an imperative, define-by-run style user API. Thanks to our define-by-run API, the code written with Optuna enjoys high modularity, and the user of Optuna can dynamically construct the search spaces for the hyperparameters.\n\ndef run_optuna_study(objective, resume=None, study_type=None, multivariate=True, search_space=None, evaluate=None, seed=None, sampler=None, pruner=None, \n                     study_name=None, direction='maximize', load_if_exists=False, n_trials=None, timeout=None, gc_after_trial=False, show_progress_bar=True, \n                     save_study=True, path='optuna', show_plots=True):\n    r\"\"\"Creates and runs an optuna study.\n\n    Args: \n        objective:          A callable that implements objective function.\n        resume:             Path to a previously saved study.\n        study_type:         Type of study selected (bayesian, gridsearch, randomsearch). Based on this a sampler will be build if sampler is None. \n                            If a sampler is passed, this has no effect.\n        multivariate:       If this is True, the multivariate TPE is used when suggesting parameters. The multivariate TPE is reported to outperform \n                            the independent TPE.\n        search_space:       Search space required when running a gridsearch (if you don't pass a sampler).\n        evaluate:           Allows you to pass a specific set of hyperparameters that will be evaluated.\n        seed:               Fixed seed used by samplers.\n        sampler:            A sampler object that implements background algorithm for value suggestion. If None is specified, TPESampler is used during \n                            single-objective optimization and NSGAIISampler during multi-objective optimization. See also samplers.\n        pruner:             A pruner object that decides early stopping of unpromising trials. If None is specified, MedianPruner is used as the default. \n                            See also pruners.\n        study_name:         Study’s name. If this argument is set to None, a unique name is generated automatically.\n        direction:          A sequence of directions during multi-objective optimization.\n        n_trials:           The number of trials. If this argument is set to None, there is no limitation on the number of trials. If timeout is also set to \n                            None, the study continues to create trials until it receives a termination signal such as Ctrl+C or SIGTERM.\n        timeout:            Stop study after the given number of second(s). If this argument is set to None, the study is executed without time limitation. \n                            If n_trials is also set to None, the study continues to create trials until it receives a termination signal such as \n                            Ctrl+C or SIGTERM.\n        gc_after_trial:     Flag to execute garbage collection at the end of each trial. By default, garbage collection is enabled, just in case. \n                            You can turn it off with this argument if memory is safely managed in your objective function.\n        show_progress_bar:  Flag to show progress bars or not. To disable progress bar, set this False.\n        save_study:         Save your study when finished/ interrupted.\n        path:               Folder where the study will be saved.\n        show_plots:         Flag to control whether plots are shown at the end of the study.\n    \"\"\"\n    \n    try: import optuna\n    except ImportError: raise ImportError('You need to install optuna to use run_optuna_study')\n\n    # Sampler\n    if sampler is None:\n        if study_type is None or \"bayes\" in study_type.lower(): \n            sampler = optuna.samplers.TPESampler(seed=seed, multivariate=multivariate)\n        elif \"grid\" in study_type.lower():\n            assert search_space, f\"you need to pass a search_space dict to run a gridsearch\"\n            sampler = optuna.samplers.GridSampler(search_space)\n        elif \"random\" in study_type.lower(): \n            sampler = optuna.samplers.RandomSampler(seed=seed)\n    assert sampler, \"you need to either select a study type (bayesian, gridsampler, randomsampler) or pass a sampler\"\n\n    # Study\n    if resume: \n        try:\n            study = joblib.load(resume)\n        except: \n            print(f\"joblib.load({resume}) couldn't recover any saved study. Check the path.\")\n            return\n        print(\"Best trial until now:\")\n        print(\" Value: \", study.best_trial.value)\n        print(\" Params: \")\n        for key, value in study.best_trial.params.items():\n            print(f\"    {key}: {value}\")\n    else: \n        study = optuna.create_study(sampler=sampler, pruner=pruner, study_name=study_name, direction=direction)\n    if evaluate: study.enqueue_trial(evaluate)\n    try:\n        study.optimize(objective, n_trials=n_trials, timeout=timeout, gc_after_trial=gc_after_trial, show_progress_bar=show_progress_bar)\n    except KeyboardInterrupt:\n        pass\n\n    # Save\n    if save_study:\n        full_path = Path(path)/f'{study.study_name}.pkl'\n        full_path.parent.mkdir(parents=True, exist_ok=True)\n        joblib.dump(study, full_path)\n        print(f'\\nOptuna study saved to {full_path}')\n        print(f\"To reload the study run: study = joblib.load('{full_path}')\")\n\n    # Plots\n    if show_plots and len(study.trials) > 1:\n        try: display(optuna.visualization.plot_optimization_history(study))\n        except: pass\n        try: display(optuna.visualization.plot_param_importances(study))\n        except: pass\n        try: display(optuna.visualization.plot_slice(study))\n        except: pass\n        try: display(optuna.visualization.plot_parallel_coordinate(study))\n        except: pass\n\n    # Study stats\n    try:\n        pruned_trials = [t for t in study.trials if t.state == optuna.trial.TrialState.PRUNED]\n        complete_trials = [t for t in study.trials if t.state == optuna.trial.TrialState.COMPLETE]\n        print(f\"\\nStudy statistics    : \")\n        print(f\"  Study name        : {study.study_name}\")\n        print(f\"  # finished trials : {len(study.trials)}\")\n        print(f\"  # pruned trials   : {len(pruned_trials)}\")\n        print(f\"  # complete trials : {len(complete_trials)}\")\n        \n        print(f\"\\nBest trial          :\")\n        trial = study.best_trial\n        print(f\"  value             : {trial.value}\")\n        print(f\"  best_params = {trial.params}\\n\")\n    except:\n        print('\\nNo finished trials yet.')\n    return study\n\n\nsource\n\nrun_optuna_study\n\n run_optuna_study (objective, resume=None, study_type=None,\n                   multivariate=True, search_space=None, evaluate=None,\n                   seed=None, sampler=None, pruner=None, study_name=None,\n                   direction='maximize', load_if_exists=False,\n                   n_trials=None, timeout=None, gc_after_trial=False,\n                   show_progress_bar=True, save_study=True, path='optuna',\n                   show_plots=True)\n\nCreates and runs an optuna study.\nArgs: objective: A callable that implements objective function. resume: Path to a previously saved study. study_type: Type of study selected (bayesian, gridsearch, randomsearch). Based on this a sampler will be build if sampler is None. If a sampler is passed, this has no effect. multivariate: If this is True, the multivariate TPE is used when suggesting parameters. The multivariate TPE is reported to outperform the independent TPE. search_space: Search space required when running a gridsearch (if you don’t pass a sampler). evaluate: Allows you to pass a specific set of hyperparameters that will be evaluated. seed: Fixed seed used by samplers. sampler: A sampler object that implements background algorithm for value suggestion. If None is specified, TPESampler is used during single-objective optimization and NSGAIISampler during multi-objective optimization. See also samplers. pruner: A pruner object that decides early stopping of unpromising trials. If None is specified, MedianPruner is used as the default. See also pruners. study_name: Study’s name. If this argument is set to None, a unique name is generated automatically. direction: A sequence of directions during multi-objective optimization. n_trials: The number of trials. If this argument is set to None, there is no limitation on the number of trials. If timeout is also set to None, the study continues to create trials until it receives a termination signal such as Ctrl+C or SIGTERM. timeout: Stop study after the given number of second(s). If this argument is set to None, the study is executed without time limitation. If n_trials is also set to None, the study continues to create trials until it receives a termination signal such as Ctrl+C or SIGTERM. gc_after_trial: Flag to execute garbage collection at the end of each trial. By default, garbage collection is enabled, just in case. You can turn it off with this argument if memory is safely managed in your objective function. show_progress_bar: Flag to show progress bars or not. To disable progress bar, set this False. save_study: Save your study when finished/ interrupted. path: Folder where the study will be saved. show_plots: Flag to control whether plots are shown at the end of the study."
  },
  {
    "objectID": "models.xceptiontimeplus.html",
    "href": "models.xceptiontimeplus.html",
    "title": "XceptionTimePlus",
    "section": "",
    "text": "This is an unofficial PyTorch implementation by Ignacio Oguiza - oguiza@timeseriesAI.co modified on:\nFawaz, H. I., Lucas, B., Forestier, G., Pelletier, C., Schmidt, D. F., Weber, J. & Petitjean, F. (2019). InceptionTime: Finding AlexNet for Time Series Classification. arXiv preprint arXiv:1909.04939.\nOfficial InceptionTime tensorflow implementation: https://github.com/hfawaz/InceptionTime\n\nsource\n\nXceptionTimePlus\n\n XceptionTimePlus (c_in, c_out, seq_len=None, nf=16, nb_filters=None,\n                   coord=False, norm='Batch', concat_pool=False,\n                   adaptive_size=50, custom_head=None, residual=True,\n                   zero_norm=False, act=<class\n                   'torch.nn.modules.activation.ReLU'>, act_kwargs={})\n\nA sequential container. Modules will be added to it in the order they are passed in the constructor. Alternatively, an OrderedDict of modules can be passed in. The forward() method of [Sequential](https://timeseriesAI.github.io/models.layers.html#sequential) accepts any input and forwards it to the first module it contains. It then “chains” outputs to inputs sequentially for each subsequent module, finally returning the output of the last module.\nThe value a [Sequential](https://timeseriesAI.github.io/models.layers.html#sequential) provides over manually calling a sequence of modules is that it allows treating the whole container as a single module, such that performing a transformation on the [Sequential](https://timeseriesAI.github.io/models.layers.html#sequential) applies to each of the modules it stores (which are each a registered submodule of the [Sequential](https://timeseriesAI.github.io/models.layers.html#sequential)).\nWhat’s the difference between a [Sequential](https://timeseriesAI.github.io/models.layers.html#sequential) and a :class:torch.nn.ModuleList? A ModuleList is exactly what it sounds like–a list for storing Module s! On the other hand, the layers in a [Sequential](https://timeseriesAI.github.io/models.layers.html#sequential) are connected in a cascading way.\nExample::\n# Using Sequential to create a small model. When `model` is run,\n# input will first be passed to `Conv2d(1,20,5)`. The output of\n# `Conv2d(1,20,5)` will be used as the input to the first\n# `ReLU`; the output of the first `ReLU` will become the input\n# for `Conv2d(20,64,5)`. Finally, the output of\n# `Conv2d(20,64,5)` will be used as input to the second `ReLU`\nmodel = nn.Sequential(\n          nn.Conv2d(1,20,5),\n          nn.ReLU(),\n          nn.Conv2d(20,64,5),\n          nn.ReLU()\n        )\n\n# Using Sequential with OrderedDict. This is functionally the\n# same as the above code\nmodel = nn.Sequential(OrderedDict([\n          ('conv1', nn.Conv2d(1,20,5)),\n          ('relu1', nn.ReLU()),\n          ('conv2', nn.Conv2d(20,64,5)),\n          ('relu2', nn.ReLU())\n        ]))\n\nsource\n\n\nXceptionBlockPlus\n\n XceptionBlockPlus (ni, nf, residual=True, coord=False, norm='Batch',\n                    zero_norm=False, act=<class\n                    'torch.nn.modules.activation.ReLU'>, act_kwargs={},\n                    ks=40, kss=None, bottleneck=True, separable=True,\n                    bn_1st=True, norm_act=False)\n\nSame as nn.Module, but no need for subclasses to call super().__init__\n\nsource\n\n\nXceptionModulePlus\n\n XceptionModulePlus (ni, nf, ks=40, kss=None, bottleneck=True,\n                     coord=False, separable=True, norm='Batch',\n                     zero_norm=False, bn_1st=True, act=<class\n                     'torch.nn.modules.activation.ReLU'>, act_kwargs={},\n                     norm_act=False)\n\nSame as nn.Module, but no need for subclasses to call super().__init__\n\nbs = 16\nvars = 3\nseq_len = 12\nc_out = 2\nxb = torch.rand(bs, vars, seq_len)\n\n\ntest_eq(XceptionTimePlus(vars,c_out)(xb).shape, [bs, c_out])\ntest_eq(XceptionTimePlus(vars,c_out, nf=32)(xb).shape, [bs, c_out])\ntest_eq(XceptionTimePlus(vars,c_out, bottleneck=False)(xb).shape, [bs, c_out])\ntest_eq(XceptionTimePlus(vars,c_out, residual=False)(xb).shape, [bs, c_out])\ntest_eq(XceptionTimePlus(vars,c_out, coord=True)(xb).shape, [bs, c_out])\ntest_eq(XceptionTimePlus(vars,c_out, concat_pool=True)(xb).shape, [bs, c_out])\ntest_eq(count_parameters(XceptionTimePlus(3, 2)), 399540)\n\n\nm = XceptionTimePlus(2,3)\ntest_eq(check_weight(m, is_bn)[0].sum(), 5)\ntest_eq(len(check_bias(m, is_conv)[0]), 0)\nm = XceptionTimePlus(2,3, zero_norm=True)\ntest_eq(check_weight(m, is_bn)[0].sum(), 5)\nm = XceptionTimePlus(2,3, zero_norm=True, norm_act=True)\ntest_eq(check_weight(m, is_bn)[0].sum(), 7)\n\n\nm = XceptionTimePlus(2,3, coord=True)\ntest_eq(len(get_layers(m, cond=is_layer(AddCoords1d))), 25)\ntest_eq(len(get_layers(m, cond=is_layer(nn.Conv1d))), 37)\nm = XceptionTimePlus(2,3, bottleneck=False, coord=True)\ntest_eq(len(get_layers(m, cond=is_layer(AddCoords1d))), 21)\ntest_eq(len(get_layers(m, cond=is_layer(nn.Conv1d))), 33)\n\n\nm = XceptionTimePlus(vars, c_out, seq_len=seq_len, custom_head=mlp_head)\ntest_eq(m(xb).shape, [bs, c_out])\n\n\nXceptionTimePlus(vars, c_out, coord=True)\n\nXceptionTimePlus(\n  (backbone): XceptionBlockPlus(\n    (xception): ModuleList(\n      (0): XceptionModulePlus(\n        (bottleneck): ConvBlock(\n          (0): AddCoords1d()\n          (1): Conv1d(4, 16, kernel_size=(1,), stride=(1,), bias=False)\n        )\n        (convs): ModuleList(\n          (0): ConvBlock(\n            (0): AddCoords1d()\n            (1): SeparableConv1d(\n              (depthwise_conv): Conv1d(17, 17, kernel_size=(39,), stride=(1,), padding=(19,), groups=17, bias=False)\n              (pointwise_conv): Conv1d(17, 16, kernel_size=(1,), stride=(1,), bias=False)\n            )\n          )\n          (1): ConvBlock(\n            (0): AddCoords1d()\n            (1): SeparableConv1d(\n              (depthwise_conv): Conv1d(17, 17, kernel_size=(19,), stride=(1,), padding=(9,), groups=17, bias=False)\n              (pointwise_conv): Conv1d(17, 16, kernel_size=(1,), stride=(1,), bias=False)\n            )\n          )\n          (2): ConvBlock(\n            (0): AddCoords1d()\n            (1): SeparableConv1d(\n              (depthwise_conv): Conv1d(17, 17, kernel_size=(9,), stride=(1,), padding=(4,), groups=17, bias=False)\n              (pointwise_conv): Conv1d(17, 16, kernel_size=(1,), stride=(1,), bias=False)\n            )\n          )\n        )\n        (mp_conv): Sequential(\n          (0): MaxPool1d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)\n          (1): ConvBlock(\n            (0): AddCoords1d()\n            (1): Conv1d(4, 16, kernel_size=(1,), stride=(1,), bias=False)\n          )\n        )\n        (concat): Concat(dim=1)\n      )\n      (1): XceptionModulePlus(\n        (bottleneck): ConvBlock(\n          (0): AddCoords1d()\n          (1): Conv1d(65, 32, kernel_size=(1,), stride=(1,), bias=False)\n        )\n        (convs): ModuleList(\n          (0): ConvBlock(\n            (0): AddCoords1d()\n            (1): SeparableConv1d(\n              (depthwise_conv): Conv1d(33, 33, kernel_size=(39,), stride=(1,), padding=(19,), groups=33, bias=False)\n              (pointwise_conv): Conv1d(33, 32, kernel_size=(1,), stride=(1,), bias=False)\n            )\n          )\n          (1): ConvBlock(\n            (0): AddCoords1d()\n            (1): SeparableConv1d(\n              (depthwise_conv): Conv1d(33, 33, kernel_size=(19,), stride=(1,), padding=(9,), groups=33, bias=False)\n              (pointwise_conv): Conv1d(33, 32, kernel_size=(1,), stride=(1,), bias=False)\n            )\n          )\n          (2): ConvBlock(\n            (0): AddCoords1d()\n            (1): SeparableConv1d(\n              (depthwise_conv): Conv1d(33, 33, kernel_size=(9,), stride=(1,), padding=(4,), groups=33, bias=False)\n              (pointwise_conv): Conv1d(33, 32, kernel_size=(1,), stride=(1,), bias=False)\n            )\n          )\n        )\n        (mp_conv): Sequential(\n          (0): MaxPool1d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)\n          (1): ConvBlock(\n            (0): AddCoords1d()\n            (1): Conv1d(65, 32, kernel_size=(1,), stride=(1,), bias=False)\n          )\n        )\n        (concat): Concat(dim=1)\n      )\n      (2): XceptionModulePlus(\n        (bottleneck): ConvBlock(\n          (0): AddCoords1d()\n          (1): Conv1d(129, 64, kernel_size=(1,), stride=(1,), bias=False)\n        )\n        (convs): ModuleList(\n          (0): ConvBlock(\n            (0): AddCoords1d()\n            (1): SeparableConv1d(\n              (depthwise_conv): Conv1d(65, 65, kernel_size=(39,), stride=(1,), padding=(19,), groups=65, bias=False)\n              (pointwise_conv): Conv1d(65, 64, kernel_size=(1,), stride=(1,), bias=False)\n            )\n          )\n          (1): ConvBlock(\n            (0): AddCoords1d()\n            (1): SeparableConv1d(\n              (depthwise_conv): Conv1d(65, 65, kernel_size=(19,), stride=(1,), padding=(9,), groups=65, bias=False)\n              (pointwise_conv): Conv1d(65, 64, kernel_size=(1,), stride=(1,), bias=False)\n            )\n          )\n          (2): ConvBlock(\n            (0): AddCoords1d()\n            (1): SeparableConv1d(\n              (depthwise_conv): Conv1d(65, 65, kernel_size=(9,), stride=(1,), padding=(4,), groups=65, bias=False)\n              (pointwise_conv): Conv1d(65, 64, kernel_size=(1,), stride=(1,), bias=False)\n            )\n          )\n        )\n        (mp_conv): Sequential(\n          (0): MaxPool1d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)\n          (1): ConvBlock(\n            (0): AddCoords1d()\n            (1): Conv1d(129, 64, kernel_size=(1,), stride=(1,), bias=False)\n          )\n        )\n        (concat): Concat(dim=1)\n      )\n      (3): XceptionModulePlus(\n        (bottleneck): ConvBlock(\n          (0): AddCoords1d()\n          (1): Conv1d(257, 128, kernel_size=(1,), stride=(1,), bias=False)\n        )\n        (convs): ModuleList(\n          (0): ConvBlock(\n            (0): AddCoords1d()\n            (1): SeparableConv1d(\n              (depthwise_conv): Conv1d(129, 129, kernel_size=(39,), stride=(1,), padding=(19,), groups=129, bias=False)\n              (pointwise_conv): Conv1d(129, 128, kernel_size=(1,), stride=(1,), bias=False)\n            )\n          )\n          (1): ConvBlock(\n            (0): AddCoords1d()\n            (1): SeparableConv1d(\n              (depthwise_conv): Conv1d(129, 129, kernel_size=(19,), stride=(1,), padding=(9,), groups=129, bias=False)\n              (pointwise_conv): Conv1d(129, 128, kernel_size=(1,), stride=(1,), bias=False)\n            )\n          )\n          (2): ConvBlock(\n            (0): AddCoords1d()\n            (1): SeparableConv1d(\n              (depthwise_conv): Conv1d(129, 129, kernel_size=(9,), stride=(1,), padding=(4,), groups=129, bias=False)\n              (pointwise_conv): Conv1d(129, 128, kernel_size=(1,), stride=(1,), bias=False)\n            )\n          )\n        )\n        (mp_conv): Sequential(\n          (0): MaxPool1d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)\n          (1): ConvBlock(\n            (0): AddCoords1d()\n            (1): Conv1d(257, 128, kernel_size=(1,), stride=(1,), bias=False)\n          )\n        )\n        (concat): Concat(dim=1)\n      )\n    )\n    (shortcut): ModuleList(\n      (0): ConvBlock(\n        (0): AddCoords1d()\n        (1): Conv1d(4, 128, kernel_size=(1,), stride=(1,), bias=False)\n        (2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n      (1): ConvBlock(\n        (0): AddCoords1d()\n        (1): Conv1d(129, 512, kernel_size=(1,), stride=(1,), bias=False)\n        (2): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (act): ModuleList(\n      (0): ReLU()\n      (1): ReLU()\n    )\n    (add): Add\n  )\n  (head): Sequential(\n    (0): AdaptiveAvgPool1d(output_size=50)\n    (1): ConvBlock(\n      (0): AddCoords1d()\n      (1): Conv1d(513, 256, kernel_size=(1,), stride=(1,), bias=False)\n      (2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (3): ReLU()\n    )\n    (2): ConvBlock(\n      (0): AddCoords1d()\n      (1): Conv1d(257, 128, kernel_size=(1,), stride=(1,), bias=False)\n      (2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (3): ReLU()\n    )\n    (3): ConvBlock(\n      (0): AddCoords1d()\n      (1): Conv1d(129, 2, kernel_size=(1,), stride=(1,), bias=False)\n      (2): BatchNorm1d(2, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (3): ReLU()\n    )\n    (4): GAP1d(\n      (gap): AdaptiveAvgPool1d(output_size=1)\n      (flatten): Reshape(bs)\n    )\n  )\n)"
  },
  {
    "objectID": "models.positional_encoders.html",
    "href": "models.positional_encoders.html",
    "title": "Positional encoders",
    "section": "",
    "text": "This includes some variations of positional encoders used with Transformers."
  },
  {
    "objectID": "models.positional_encoders.html#imports",
    "href": "models.positional_encoders.html#imports",
    "title": "Positional encoders",
    "section": "Imports",
    "text": "Imports"
  },
  {
    "objectID": "models.positional_encoders.html#positional-encoders",
    "href": "models.positional_encoders.html#positional-encoders",
    "title": "Positional encoders",
    "section": "Positional encoders",
    "text": "Positional encoders\n\nsource\n\nPositionalEncoding\n\n PositionalEncoding (q_len, d_model, normalize=True)\n\n\npe = PositionalEncoding(1000, 512).detach().cpu().numpy()\nplt.pcolormesh(pe, cmap='viridis')\nplt.title('PositionalEncoding')\nplt.colorbar()\nplt.show()\npe.mean(), pe.std(), pe.min(), pe.max(), pe.shape\n\n\n\n\n(5.438924e-10, 0.09999991, -0.18388666, 0.11518021, (1000, 512))\n\n\n\nsource\n\n\nCoord2dPosEncoding\n\n Coord2dPosEncoding (q_len, d_model, exponential=False, normalize=True,\n                     eps=0.001, verbose=False)\n\n\ncpe = Coord2dPosEncoding(1000, 512, exponential=True, normalize=True).cpu().numpy()\nplt.pcolormesh(cpe, cmap='viridis')\nplt.title('Coord2dPosEncoding')\nplt.colorbar()\nplt.show()\nplt.plot(cpe.mean(0))\nplt.show()\nplt.plot(cpe.mean(1))\nplt.show()\ncpe.mean(), cpe.std(), cpe.min(), cpe.max()\n\n\n\n\n\n\n\n\n\n\n(3.695488e-09, 0.09999991, -0.22459325, 0.22487777)\n\n\n\nsource\n\n\nCoord1dPosEncoding\n\n Coord1dPosEncoding (q_len, exponential=False, normalize=True)\n\n\ncpe = Coord1dPosEncoding(1000, exponential=True, normalize=True).detach().cpu().numpy()\nplt.pcolormesh(cpe, cmap='viridis')\nplt.title('Coord1dPosEncoding')\nplt.colorbar()\nplt.show()\nplt.plot(cpe.mean(1))\nplt.show()\ncpe.mean(), cpe.std(), cpe.min(), cpe.max(), cpe.shape\n\n\n\n\n\n\n\n(0.0, 0.099949986, -0.2820423, 0.14113107, (1000, 1))\n\n\n\ncpe = Coord1dPosEncoding(1000, exponential=True, normalize=True).detach().cpu().numpy()\nplt.pcolormesh(cpe, cmap='viridis')\nplt.title('Coord1dPosEncoding')\nplt.colorbar()\nplt.show()\nplt.plot(cpe.mean(1))\nplt.show()\ncpe.mean(), cpe.std(), cpe.min(), cpe.max()\n\n\n\n\n\n\n\n(0.0, 0.099949986, -0.2820423, 0.14113107)"
  },
  {
    "objectID": "models.tcn.html",
    "href": "models.tcn.html",
    "title": "TCN",
    "section": "",
    "text": "This is an unofficial PyTorch implementation by Ignacio Oguiza (oguiza@timeseriesAI.co) based on:\n\nBai, S., Kolter, J. Z., & Koltun, V. (2018). An empirical evaluation of generic convolutional and recurrent networks for sequence modeling. arXiv preprint arXiv:1803.01271.\nOfficial TCN PyTorch implementation: https://github.com/locuslab/TCN\n\n\nsource\n\nTCN\n\n TCN (c_in, c_out, layers=[25, 25, 25, 25, 25, 25, 25, 25], ks=7,\n      conv_dropout=0.0, fc_dropout=0.0)\n\nSame as nn.Module, but no need for subclasses to call super().__init__\n\nsource\n\n\nTemporalConvNet\n\n TemporalConvNet (c_in, layers, ks=2, dropout=0.0)\n\n\nsource\n\n\nTemporalBlock\n\n TemporalBlock (ni, nf, ks, stride, dilation, padding, dropout=0.0)\n\nSame as nn.Module, but no need for subclasses to call super().__init__\n\nbs = 16\nnvars = 3\nseq_len = 128\nc_out = 2\nxb = torch.rand(bs, nvars, seq_len)\nmodel = TCN(nvars, c_out, fc_dropout=.5)\ntest_eq(model(xb).shape, (bs, c_out))\nmodel = TCN(nvars, c_out, conv_dropout=.2)\ntest_eq(model(xb).shape, (bs, c_out))\nmodel = TCN(nvars, c_out)\ntest_eq(model(xb).shape, (bs, c_out))\nmodel\n\nTCN(\n  (tcn): Sequential(\n    (0): TemporalBlock(\n      (conv1): Conv1d(3, 25, kernel_size=(7,), stride=(1,), padding=(6,))\n      (chomp1): Chomp1d()\n      (relu1): ReLU()\n      (dropout1): Dropout(p=0.0, inplace=False)\n      (conv2): Conv1d(25, 25, kernel_size=(7,), stride=(1,), padding=(6,))\n      (chomp2): Chomp1d()\n      (relu2): ReLU()\n      (dropout2): Dropout(p=0.0, inplace=False)\n      (net): Sequential(\n        (0): Conv1d(3, 25, kernel_size=(7,), stride=(1,), padding=(6,))\n        (1): Chomp1d()\n        (2): ReLU()\n        (3): Dropout(p=0.0, inplace=False)\n        (4): Conv1d(25, 25, kernel_size=(7,), stride=(1,), padding=(6,))\n        (5): Chomp1d()\n        (6): ReLU()\n        (7): Dropout(p=0.0, inplace=False)\n      )\n      (downsample): Conv1d(3, 25, kernel_size=(1,), stride=(1,))\n      (relu): ReLU()\n    )\n    (1): TemporalBlock(\n      (conv1): Conv1d(25, 25, kernel_size=(7,), stride=(1,), padding=(12,), dilation=(2,))\n      (chomp1): Chomp1d()\n      (relu1): ReLU()\n      (dropout1): Dropout(p=0.0, inplace=False)\n      (conv2): Conv1d(25, 25, kernel_size=(7,), stride=(1,), padding=(12,), dilation=(2,))\n      (chomp2): Chomp1d()\n      (relu2): ReLU()\n      (dropout2): Dropout(p=0.0, inplace=False)\n      (net): Sequential(\n        (0): Conv1d(25, 25, kernel_size=(7,), stride=(1,), padding=(12,), dilation=(2,))\n        (1): Chomp1d()\n        (2): ReLU()\n        (3): Dropout(p=0.0, inplace=False)\n        (4): Conv1d(25, 25, kernel_size=(7,), stride=(1,), padding=(12,), dilation=(2,))\n        (5): Chomp1d()\n        (6): ReLU()\n        (7): Dropout(p=0.0, inplace=False)\n      )\n      (relu): ReLU()\n    )\n    (2): TemporalBlock(\n      (conv1): Conv1d(25, 25, kernel_size=(7,), stride=(1,), padding=(24,), dilation=(4,))\n      (chomp1): Chomp1d()\n      (relu1): ReLU()\n      (dropout1): Dropout(p=0.0, inplace=False)\n      (conv2): Conv1d(25, 25, kernel_size=(7,), stride=(1,), padding=(24,), dilation=(4,))\n      (chomp2): Chomp1d()\n      (relu2): ReLU()\n      (dropout2): Dropout(p=0.0, inplace=False)\n      (net): Sequential(\n        (0): Conv1d(25, 25, kernel_size=(7,), stride=(1,), padding=(24,), dilation=(4,))\n        (1): Chomp1d()\n        (2): ReLU()\n        (3): Dropout(p=0.0, inplace=False)\n        (4): Conv1d(25, 25, kernel_size=(7,), stride=(1,), padding=(24,), dilation=(4,))\n        (5): Chomp1d()\n        (6): ReLU()\n        (7): Dropout(p=0.0, inplace=False)\n      )\n      (relu): ReLU()\n    )\n    (3): TemporalBlock(\n      (conv1): Conv1d(25, 25, kernel_size=(7,), stride=(1,), padding=(48,), dilation=(8,))\n      (chomp1): Chomp1d()\n      (relu1): ReLU()\n      (dropout1): Dropout(p=0.0, inplace=False)\n      (conv2): Conv1d(25, 25, kernel_size=(7,), stride=(1,), padding=(48,), dilation=(8,))\n      (chomp2): Chomp1d()\n      (relu2): ReLU()\n      (dropout2): Dropout(p=0.0, inplace=False)\n      (net): Sequential(\n        (0): Conv1d(25, 25, kernel_size=(7,), stride=(1,), padding=(48,), dilation=(8,))\n        (1): Chomp1d()\n        (2): ReLU()\n        (3): Dropout(p=0.0, inplace=False)\n        (4): Conv1d(25, 25, kernel_size=(7,), stride=(1,), padding=(48,), dilation=(8,))\n        (5): Chomp1d()\n        (6): ReLU()\n        (7): Dropout(p=0.0, inplace=False)\n      )\n      (relu): ReLU()\n    )\n    (4): TemporalBlock(\n      (conv1): Conv1d(25, 25, kernel_size=(7,), stride=(1,), padding=(96,), dilation=(16,))\n      (chomp1): Chomp1d()\n      (relu1): ReLU()\n      (dropout1): Dropout(p=0.0, inplace=False)\n      (conv2): Conv1d(25, 25, kernel_size=(7,), stride=(1,), padding=(96,), dilation=(16,))\n      (chomp2): Chomp1d()\n      (relu2): ReLU()\n      (dropout2): Dropout(p=0.0, inplace=False)\n      (net): Sequential(\n        (0): Conv1d(25, 25, kernel_size=(7,), stride=(1,), padding=(96,), dilation=(16,))\n        (1): Chomp1d()\n        (2): ReLU()\n        (3): Dropout(p=0.0, inplace=False)\n        (4): Conv1d(25, 25, kernel_size=(7,), stride=(1,), padding=(96,), dilation=(16,))\n        (5): Chomp1d()\n        (6): ReLU()\n        (7): Dropout(p=0.0, inplace=False)\n      )\n      (relu): ReLU()\n    )\n    (5): TemporalBlock(\n      (conv1): Conv1d(25, 25, kernel_size=(7,), stride=(1,), padding=(192,), dilation=(32,))\n      (chomp1): Chomp1d()\n      (relu1): ReLU()\n      (dropout1): Dropout(p=0.0, inplace=False)\n      (conv2): Conv1d(25, 25, kernel_size=(7,), stride=(1,), padding=(192,), dilation=(32,))\n      (chomp2): Chomp1d()\n      (relu2): ReLU()\n      (dropout2): Dropout(p=0.0, inplace=False)\n      (net): Sequential(\n        (0): Conv1d(25, 25, kernel_size=(7,), stride=(1,), padding=(192,), dilation=(32,))\n        (1): Chomp1d()\n        (2): ReLU()\n        (3): Dropout(p=0.0, inplace=False)\n        (4): Conv1d(25, 25, kernel_size=(7,), stride=(1,), padding=(192,), dilation=(32,))\n        (5): Chomp1d()\n        (6): ReLU()\n        (7): Dropout(p=0.0, inplace=False)\n      )\n      (relu): ReLU()\n    )\n    (6): TemporalBlock(\n      (conv1): Conv1d(25, 25, kernel_size=(7,), stride=(1,), padding=(384,), dilation=(64,))\n      (chomp1): Chomp1d()\n      (relu1): ReLU()\n      (dropout1): Dropout(p=0.0, inplace=False)\n      (conv2): Conv1d(25, 25, kernel_size=(7,), stride=(1,), padding=(384,), dilation=(64,))\n      (chomp2): Chomp1d()\n      (relu2): ReLU()\n      (dropout2): Dropout(p=0.0, inplace=False)\n      (net): Sequential(\n        (0): Conv1d(25, 25, kernel_size=(7,), stride=(1,), padding=(384,), dilation=(64,))\n        (1): Chomp1d()\n        (2): ReLU()\n        (3): Dropout(p=0.0, inplace=False)\n        (4): Conv1d(25, 25, kernel_size=(7,), stride=(1,), padding=(384,), dilation=(64,))\n        (5): Chomp1d()\n        (6): ReLU()\n        (7): Dropout(p=0.0, inplace=False)\n      )\n      (relu): ReLU()\n    )\n    (7): TemporalBlock(\n      (conv1): Conv1d(25, 25, kernel_size=(7,), stride=(1,), padding=(768,), dilation=(128,))\n      (chomp1): Chomp1d()\n      (relu1): ReLU()\n      (dropout1): Dropout(p=0.0, inplace=False)\n      (conv2): Conv1d(25, 25, kernel_size=(7,), stride=(1,), padding=(768,), dilation=(128,))\n      (chomp2): Chomp1d()\n      (relu2): ReLU()\n      (dropout2): Dropout(p=0.0, inplace=False)\n      (net): Sequential(\n        (0): Conv1d(25, 25, kernel_size=(7,), stride=(1,), padding=(768,), dilation=(128,))\n        (1): Chomp1d()\n        (2): ReLU()\n        (3): Dropout(p=0.0, inplace=False)\n        (4): Conv1d(25, 25, kernel_size=(7,), stride=(1,), padding=(768,), dilation=(128,))\n        (5): Chomp1d()\n        (6): ReLU()\n        (7): Dropout(p=0.0, inplace=False)\n      )\n      (relu): ReLU()\n    )\n  )\n  (gap): GAP1d(\n    (gap): AdaptiveAvgPool1d(output_size=1)\n    (flatten): Flatten(full=False)\n  )\n  (linear): Linear(in_features=25, out_features=2, bias=True)\n)"
  },
  {
    "objectID": "models.xcm.html",
    "href": "models.xcm.html",
    "title": "XCM",
    "section": "",
    "text": "An Explainable Convolutional Neural Network for Multivariate Time Series Classification\n\nThis is an unofficial PyTorch implementation of XCM created by Ignacio Oguiza (oguiza@timeseriesAI.co)\n\nsource\n\nXCM\n\n XCM (c_in:int, c_out:int, seq_len:Optional[int]=None, nf:int=128,\n      window_perc:float=1.0, flatten:bool=False, custom_head:<built-\n      infunctioncallable>=None, concat_pool:bool=False,\n      fc_dropout:float=0.0, bn:bool=False, y_range:tuple=None, **kwargs)\n\nSame as nn.Module, but no need for subclasses to call super().__init__\n\nfrom tsai.data.basics import *\nfrom tsai.learner import *\n\n\ndsid = 'NATOPS'\nX, y, splits = get_UCR_data(dsid, split_data=False)\ntfms = [None, TSCategorize()]\ndls = get_ts_dls(X, y, splits=splits, tfms=tfms)\nmodel =  XCM(dls.vars, dls.c, dls.len)\nlearn = ts_learner(dls, model, metrics=accuracy)\nxb, yb = dls.one_batch()\n\nbs, c_in, seq_len = xb.shape\nc_out = len(np.unique(yb.cpu().numpy()))\n\nmodel = XCM(c_in, c_out, seq_len, fc_dropout=.5)\ntest_eq(model.to(xb.device)(xb).shape, (bs, c_out))\nmodel = XCM(c_in, c_out, seq_len, concat_pool=True)\ntest_eq(model.to(xb.device)(xb).shape, (bs, c_out))\nmodel = XCM(c_in, c_out, seq_len)\ntest_eq(model.to(xb.device)(xb).shape, (bs, c_out))\nmodel\n\nXCM(\n  (conv2dblock): Sequential(\n    (0): Unsqueeze(dim=1)\n    (1): Conv2dSame(\n      (conv2d_same): Conv2d(1, 128, kernel_size=(1, 51), stride=(1, 1))\n    )\n    (2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (3): ReLU()\n  )\n  (conv2d1x1block): Sequential(\n    (0): Conv2d(128, 1, kernel_size=(1, 1), stride=(1, 1))\n    (1): ReLU()\n    (2): Squeeze(dim=1)\n  )\n  (conv1dblock): Sequential(\n    (0): Conv1d(24, 128, kernel_size=(51,), stride=(1,), padding=(25,))\n    (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (2): ReLU()\n  )\n  (conv1d1x1block): Sequential(\n    (0): Conv1d(128, 1, kernel_size=(1,), stride=(1,))\n    (1): ReLU()\n  )\n  (concat): Concat(dim=1)\n  (conv1d): Sequential(\n    (0): Conv1d(25, 128, kernel_size=(51,), stride=(1,), padding=(25,))\n    (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (2): ReLU()\n  )\n  (head): Sequential(\n    (0): GAP1d(\n      (gap): AdaptiveAvgPool1d(output_size=1)\n      (flatten): Reshape(bs)\n    )\n    (1): LinBnDrop(\n      (0): Linear(in_features=128, out_features=6, bias=True)\n    )\n  )\n)\n\n\n\nmodel.show_gradcam(xb, yb)\n\n\n\n\n\n\n\n\nmodel.show_gradcam(xb[0], yb[0])\n\n[W NNPACK.cpp:53] Could not initialize NNPACK! Reason: Unsupported hardware.\n\n\n\n\n\n\n\n\n\nbs = 16\nn_vars = 3\nseq_len = 12\nc_out = 10\nxb = torch.rand(bs, n_vars, seq_len)\nnew_head = partial(conv_lin_nd_head, d=(5, 2))\nnet = XCM(n_vars, c_out, seq_len, custom_head=new_head)\nprint(net.to(xb.device)(xb).shape)\nnet.head\n\ntorch.Size([16, 5, 2, 10])\n\n\ncreate_conv_lin_nd_head(\n  (0): Conv1d(128, 10, kernel_size=(1,), stride=(1,))\n  (1): Linear(in_features=12, out_features=10, bias=True)\n  (2): Transpose(-1, -2)\n  (3): Reshape(bs, 5, 2, 10)\n)\n\n\n\nbs = 16\nn_vars = 3\nseq_len = 12\nc_out = 2\nxb = torch.rand(bs, n_vars, seq_len)\nnet = XCM(n_vars, c_out, seq_len)\nchange_model_head(net, create_pool_plus_head, concat_pool=False)\nprint(net.to(xb.device)(xb).shape)\nnet.head\n\ntorch.Size([16, 2])\n\n\nSequential(\n  (0): AdaptiveAvgPool1d(output_size=1)\n  (1): Reshape(bs)\n  (2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (3): Linear(in_features=128, out_features=512, bias=False)\n  (4): ReLU(inplace=True)\n  (5): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (6): Linear(in_features=512, out_features=2, bias=False)\n)"
  },
  {
    "objectID": "data.validation.html",
    "href": "data.validation.html",
    "title": "Spliting data",
    "section": "",
    "text": "Functions required to perform cross-validation and transform unique time series sequence into multiple samples ready to be used by a time series model.\n\n\nsource\n\nRandomSplitter\n\n RandomSplitter (valid_pct=0.2, seed=None)\n\nCreate function that splits items between train/val with valid_pct randomly.\n\nsource\n\n\nbalance_idx\n\n balance_idx (o, shuffle=False, strategy='oversample', random_state=None,\n              verbose=False)\n\n\nsource\n\n\nleakage_finder\n\n leakage_finder (*splits, verbose=True)\n\nYou can pass splits as a tuple, or train, valid, …\n\nsource\n\n\ncheck_splits_overlap\n\n check_splits_overlap (splits)\n\n\nsource\n\n\ncheck_overlap\n\n check_overlap (a, b, c=None)\n\nChecks if there’s overlap between array-like objects\n\na = np.arange(10)\nb = np.arange(10, 20)\ntest_eq(check_overlap(a, b), False)\na = np.arange(10)\nb = np.arange(9, 20)\ntest_eq(check_overlap(a, b), [9])\na = np.arange(10)\nb = np.arange(10, 20)\nc = np.arange(20, 30)\ntest_eq(check_overlap(a, b, c), False)\na = np.arange(10)\nb = np.arange(10, 20)\nc = np.arange(10, 30)\ntest_eq(check_overlap(a, b, c), ([], [], [10, 11, 12, 13, 14, 15, 16, 17, 18, 19]))\n\n\ny = np.concatenate([[i] * np.random.randint(10, 100) for i in range(5)])\ntrain_split = np.random.choice(len(y), int(len(y) * .8), False)\nc, v = np.unique(y[train_split], return_counts=True)\nprint(f\"{'imbalanced:':25} {c} {v}\")\n\noversampled_train_split = train_split[balance_idx(y[train_split], strategy=\"oversample\")]\nosc, osv = np.unique(y[oversampled_train_split], return_counts=True)\nprint(f\"{'balanced (oversample):':25} {osc} {osv}\")\ntest_eq(osv, [max(v)] * len(v))\n\nundersampled_train_split = train_split[balance_idx(y[train_split], strategy=\"undersample\")]\nusc, usv = np.unique(y[undersampled_train_split], return_counts=True)\nprint(f\"{'balanced (undersample):':25} {usc} {usv}\")\ntest_eq(usv, [min(v)] * len(v))\n\nimbalanced:               [0 1 2 3 4] [65 58 17 72 72]\nbalanced (oversample):    [0 1 2 3 4] [72 72 72 72 72]\nbalanced (undersample):   [0 1 2 3 4] [17 17 17 17 17]\n\n\n\nl = L(list(concat(np.zeros(5), np.ones(10)).astype(int)))\nbalanced_idx = balance_idx(l)\ntest_eq(np.mean(l[balanced_idx]), 0.5)\ntest_eq(isinstance(balanced_idx, L), True)\n\nl = list(concat(np.zeros(5), np.ones(10)).astype(int))\nbalanced_idx = balance_idx(l)\ntest_eq(np.mean(L(l)[balanced_idx]), 0.5)\ntest_eq(isinstance(balanced_idx, L), True)\n\na = concat(np.zeros(5), np.ones(10)).astype(int)\nbalanced_idx = balance_idx(a)\ntest_eq(np.mean(a[balanced_idx]), 0.5)\ntest_eq(isinstance(balanced_idx, L), True)\n\nt = concat(torch.zeros(5), torch.ones(10))\nbalanced_idx = balance_idx(t, shuffle=True)\ntest_eq(t[balanced_idx].mean(), 0.5)\ntest_eq(isinstance(balanced_idx, L), True)\n\n\na, b = np.arange(100_000), np.arange(100_000, 200_000)\n\n\nsoft_labels = True\nfilter_pseudolabels = .5\nbalanced_pseudolabels = True\n\npseudolabels = torch.rand(1000, 3)\npseudolabels = torch.softmax(pseudolabels, -1) if soft_labels else torch.argmax(pseudolabels, -1)\nhpl = torch.argmax(pseudolabels, -1) if soft_labels else pseudolabels\n\nif filter_pseudolabels and pseudolabels.ndim > 1: \n    error = 1 - pseudolabels.max(-1).values\n    filt_pl_idx = np.arange(len(error))[error < filter_pseudolabels]\n    filt_pl = pseudolabels[error < filter_pseudolabels]\n    assert len(filt_pl) > 0, 'no filtered pseudolabels'\n    filt_hpl = torch.argmax(filt_pl, -1)\nelse: \n    filt_pl_idx = np.arange(len(pseudolabels))\n    filt_pl = filt_hpl = pseudolabels\n\n\npl_split = filt_pl_idx[balance_idx(filt_hpl)] if balanced_pseudolabels else filt_pl_idx\ntest_eq(hpl[pl_split].float().mean(), np.mean(np.unique(hpl)))\n\n\nsource\n\n\nTrainValidTestSplitter\n\n TrainValidTestSplitter (n_splits:int=1, valid_size:Union[float,int]=0.2,\n                         test_size:Union[float,int]=0.0,\n                         train_only:bool=False, stratify:bool=True,\n                         balance:bool=False, strategy:str='oversample',\n                         shuffle:bool=True,\n                         random_state:Optional[int]=None,\n                         verbose:bool=False, **kwargs)\n\nSplit items into random train, valid (and test optional) subsets.\n\nsource\n\n\nplot_splits\n\n plot_splits (splits)\n\n\nsource\n\n\nget_splits\n\n get_splits (o, n_splits:int=1, valid_size:float=0.2, test_size:float=0.0,\n             train_only:bool=False,\n             train_size:Union[NoneType,float,int]=None,\n             balance:bool=False, strategy:str='oversample',\n             shuffle:bool=True, stratify:bool=True,\n             check_splits:bool=True, random_state:Optional[int]=None,\n             show_plot:bool=True, verbose:bool=False)\n\nArguments: o : object to which splits will be applied, usually target. n_splits : number of folds. Must be an int >= 1. valid_size : size of validation set. Only used if n_splits = 1. If n_splits > 1 valid_size = (1. - test_size) / n_splits. test_size : size of test set. Default = 0. train_only : if True valid set == train set. This may be useful for debugging purposes. train_size : size of the train set used. Default = None (the remainder after assigning both valid and test). Useful for to get learning curves with different train sizes or get a small batch to debug a neural net. balance : whether to balance data so that train always contain the same number of items per class. strategy : strategy to balance data (“undersample” or “oversample”). Default = “oversample”. shuffle : whether to shuffle data before splitting into batches. Note that the samples within each split will be shuffle. stratify : whether to create folds preserving the percentage of samples for each class. check_splits : whether to perform leakage and completion checks. random_state : when shuffle is True, random_state affects the ordering of the indices. Pass an int for reproducible output. show_plot : plot the split distribution\n\nn_splits                = 5\nvalid_size              = 0.2\ntest_size               = 0.2\ntrain_only              = False  # set to True for debugging (valid = train)\ntrain_size              = 5000\nstratify                = True\nbalance                 = False\nshuffle                 = True\npredefined_splits       = None\nshow_plot               = True \n\n\ncheck_splits = True\nrandom_state = 23\n\ny = np.random.randint(0, 3, 10000) + 100\n\nsplits = get_splits(y, n_splits=n_splits, valid_size=valid_size, test_size=test_size, shuffle=shuffle, balance=balance, stratify=stratify,\n                    train_only=train_only, train_size=train_size, check_splits=check_splits, random_state=random_state, show_plot=show_plot, verbose=True)\nsplits\n\n\n\n\n(((#5000) [1831,2914,3921,1429,4989,7747,4959,7494,2576,9060...],\n  (#1600) [4159,1554,8525,5852,5670,3368,2112,9453,9910,5499...],\n  (#2000) [7806,3235,373,8579,9844,594,5411,5341,4372,3711...]),\n ((#5000) [5527,9244,3643,3495,7725,9325,3983,2123,3149,80...],\n  (#1600) [8971,9717,8245,2312,6256,3065,5466,9913,5232,7159...],\n  (#2000) [7806,3235,373,8579,9844,594,5411,5341,4372,3711...]),\n ((#5000) [6834,5325,6822,4833,5963,6938,9803,9835,6878,288...],\n  (#1600) [6332,2914,5353,3820,738,9086,3351,5785,7926,3460...],\n  (#2000) [7806,3235,373,8579,9844,594,5411,5341,4372,3711...]),\n ((#5000) [7456,4171,1003,6638,1840,3693,561,8380,3111,6118...],\n  (#1600) [7517,13,2935,8428,2783,7980,3938,9245,7358,6305...],\n  (#2000) [7806,3235,373,8579,9844,594,5411,5341,4372,3711...]),\n ((#5000) [3634,697,6435,5431,7187,2231,2442,5717,9370,9669...],\n  (#1600) [2016,7372,106,405,1319,8302,9551,3047,7255,1709...],\n  (#2000) [7806,3235,373,8579,9844,594,5411,5341,4372,3711...]))\n\n\n\ntrain_size=256\ny = np.random.randint(0, 3, 1000) + 100\nsplits = get_splits(y, train_size=train_size, train_only=True)\ntest_eq(splits[0], splits[1])\ntest_eq(len(splits[0]), train_size)\nsplits\n\nvalid == train\n\n\n\n\n\n((#256) [604,296,256,112,22,44,656,638,698,891...],\n (#256) [604,296,256,112,22,44,656,638,698,891...])\n\n\n\nsource\n\n\nget_walk_forward_splits\n\n get_walk_forward_splits (o, n_splits=1, train_size=None, valid_size=0.2,\n                          test_size=0.0, anchored=False, gap=0.0,\n                          test_after_valid=True, random_state=None,\n                          show_plot=True)\n\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\no\n\n\n3D object with shape [samples x features x steps] containing the time series we need to split\n\n\nn_splits\nint\n1\n# of splits\n\n\ntrain_size\nNoneType\nNone\noptional: training set size as an int or a float. None when using and anchored strategy.\n\n\nvalid_size\nfloat\n0.2\nvalidation set size as an int or a float\n\n\ntest_size\nfloat\n0.0\ntest set size as an int or a float\n\n\nanchored\nbool\nFalse\nstarting point for train set remains the same for all splits\n\n\ngap\nfloat\n0.0\n# of samples to exclude from the end of each train set before the validation set. Entered as an int or a float\n\n\ntest_after_valid\nbool\nTrue\nflag to indicate if validation and test will be samples randomly or sequentially\n\n\nrandom_state\nNoneType\nNone\ninteger that can be used to generate reproducible results\n\n\nshow_plot\nbool\nTrue\nplots the splits created\n\n\n\n\no = np.random.rand(10_000, 3,  50) # shape: [samples x features x steps]\n\nsplits = get_walk_forward_splits(\n    o, \n    n_splits=4, \n    train_size=.6,\n    valid_size=0.1, \n    test_size=0.1, \n    anchored = True,\n    gap = 100,\n    test_after_valid = True,\n    random_state = None,\n    show_plot=True,\n)\n\nsplits = get_walk_forward_splits(\n    o, \n    n_splits=3, \n    train_size=0.3,\n    valid_size=0.1, \n    test_size=0.1, \n    anchored = False,\n    gap = 0.,\n    test_after_valid = False,\n    random_state = None,\n    show_plot=True,\n)\n\n\n\n\n\n\n\n\nsource\n\n\nTSSplitter\n\n TSSplitter (valid_size=0.2, test_size=0.0, fcst_horizon=0,\n             show_plot=True)\n\nCreate function that splits items between train/val with valid_size without shuffling data.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nvalid_size\nfloat\n0.2\nint or float indicating the validation set size\n\n\ntest_size\nfloat\n0.0\nint or float indicating the test set size\n\n\nfcst_horizon\nint\n0\nint that indicates the number of time steps removed at the end of train (and validation)\n\n\nshow_plot\nbool\nTrue\nflag that indicates if a plot showing the splits will be created\n\n\n\n\ny = np.arange(1000) + 100\ntest_eq(TimeSplitter(valid_size=0.2)(y)[1], L(np.arange(800, 1000).tolist()))\ntest_eq(TimeSplitter(valid_size=0.2)(y)[0], TimeSplitter(valid_size=200)(y)[0])\nTimeSplitter(valid_size=0.2, show_plot=True)(y)\n\n\n\n\n\n\n\n\n\n\n\n\n\n((#800) [0,1,2,3,4,5,6,7,8,9...],\n (#200) [800,801,802,803,804,805,806,807,808,809...])\n\n\n\nn_splits                = 5\nvalid_size              = 0.2  \ntest_size               = 0\ntrain_only              = False  # set to True for debugging (valid = train)\ntrain_size              = None\nstratify                = True\nbalance                 = True\nshuffle                 = True\npredefined_splits       = None\nshow_plot               = True \n\n\ncheck_splits = True\nrandom_state = 23\n\nsplits = get_splits(y, n_splits=n_splits, valid_size=valid_size, test_size=test_size, shuffle=shuffle, balance=balance, stratify=stratify,\n                    train_only=train_only, train_size=train_size, check_splits=check_splits, random_state=random_state, show_plot=show_plot, verbose=True)\nsplit = splits[0] if n_splits == 1 else splits[0][0]\ny[split].mean(), split\n\nstratify set to False as n_splits=5 cannot be greater than the min number of members in each class (1).\n\n\n\n\n\n(601.11, (#800) [314,194,782,789,502,917,137,415,904,181...])\n\n\n\nlist([splits[0], splits[1], splits[2], splits[3], splits[4]])\n\n[((#800) [314,194,782,789,502,917,137,415,904,181...],\n  (#200) [362,151,934,378,95,597,500,117,980,844...]),\n ((#800) [312,198,777,788,515,910,145,413,898,186...],\n  (#200) [352,133,955,396,64,596,442,79,991,882...]),\n ((#800) [311,197,783,791,507,922,145,416,908,184...],\n  (#200) [338,125,912,361,54,594,486,88,994,859...]),\n ((#800) [296,181,782,789,493,917,130,401,905,165...],\n  (#200) [405,199,953,444,113,610,515,137,997,881...]),\n ((#800) [320,190,782,788,506,906,141,412,893,178...],\n  (#200) [336,149,942,358,49,582,472,70,990,907...])]\n\n\n\nn_splits = 5\nvalid_size = 0.\ntest_size = 0.\nshuffle = True\nstratify = True\ntrain_only = True\ntrain_size = None\ncheck_splits = True\nrandom_state = 1\nshow_plot = True \n\nsplits = get_splits(y, n_splits=n_splits, valid_size=valid_size, test_size=test_size, shuffle=shuffle, stratify=stratify,\n                    train_only=train_only, train_size=train_size, check_splits=check_splits, random_state=random_state, show_plot=show_plot, verbose=True)\nfor split in splits: \n    test_eq(len(split[0]), len(y))\n    test_eq(np.sort(split[0]), np.arange(len(y)))\n\nstratify set to False as n_splits=5 cannot be greater than the min number of members in each class (1).\nvalid == train\n\n\n\n\n\n\nn_splits = 5\ny = np.random.randint(0, 2, 1000)\n\nsplits = get_splits(y, n_splits=n_splits, shuffle=False, check_splits=True)\ntest_eq(np.concatenate((L(zip(*splits))[1])), np.arange(len(y)))\n\nsplits = get_splits(y, n_splits=n_splits, shuffle=True, check_splits=True)\ntest_eq(np.sort(np.concatenate((L(zip(*splits))[1]))), np.arange(len(y)))\n\n\n\n\n\n\n\n\nn_splits = 2\ny = np.random.randint(0, 2, 1000)\n\nsplits = get_splits(y, n_splits=n_splits, test_size=0.2, shuffle=False)\nfor i in range(n_splits): leakage_finder(*splits[i])\ntest_eq(len(splits), n_splits)\ntest_eq(len(splits[0]), 3)\ns = []\n[s.extend(split) for split in splits[0]]\ntest_eq(np.sort(s), np.arange(len(y)))\ns = []\n[s.extend(split) for split in splits[1]]\ntest_eq(np.sort(s), np.arange(len(y)))\n\n\n\n\n\ny = np.random.randint(0, 2, 1000)\nsplits1 = get_splits(y, valid_size=.25, test_size=0, random_state=23, stratify=True, shuffle=True)\nsplits2 = get_splits(y, valid_size=.25, test_size=0, random_state=23, stratify=True, shuffle=True)\nsplits3 = get_splits(y, valid_size=.25, test_size=0, random_state=None, stratify=True, shuffle=True)\nsplits4 = get_splits(y, valid_size=.25, test_size=0, random_state=None, stratify=True, shuffle=True)\ntest_eq(splits1[0], splits2[0])\ntest_ne(splits3[0], splits4[0])\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ny = np.random.randint(0, 2, 100)\nsplits = get_splits(y, valid_size=.25, test_size=0, random_state=23, stratify=True, shuffle=True)\ntest_eq(len(splits), 2)\n\n\n\n\n\ny = np.random.randint(0, 2, 100)\nsplits = get_splits(y, valid_size=.25, test_size=0, random_state=23, stratify=True)\ntest_eq(len(splits), 2)\n\n\n\n\n\ny = np.random.randint(0, 2, 100)\nsplits = get_splits(y, valid_size=.25, test_size=20, random_state=23, stratify=True)\ntest_eq(len(splits), 3)\nleakage_finder(*splits)\n\n\n\n\n\nsplits = TrainValidTestSplitter(valid_size=.25, test_size=20, random_state=23, stratify=True)(np.random.randint(0, 2, 100))\ntest_eq(len(splits[1]), 25)\ntest_eq(len(splits[2]), 20)\n\n\no = np.random.randint(0, 2, 1000)\nfor p in [1, .75, .5, .25, .125]:\n    splits = get_splits(o, train_size=p)\n    test_eq(len(splits[0]), len(o) * .8 * p)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ny = L([0] * 50 + [1] * 25 + [2] * 15 + [3] * 10)\nsplits = get_splits(y, valid_size=.2, test_size=.2)\ntest_eq(np.mean(y[splits[0]])==np.mean(y[splits[1]])==np.mean(y[splits[2]]), True)\nsplits\n\n\n\n\n((#60) [49,87,71,98,84,36,66,57,24,30...],\n (#20) [46,79,45,78,42,65,35,97,11,50...],\n (#20) [63,48,6,88,4,81,1,58,64,28...])\n\n\n\ny = L([0] * 50 + [1] * 25 + [2] * 15 + [3] * 10)\nsplits = get_splits(y, n_splits=1, valid_size=.2, test_size=.2, shuffle=False)\n# test_eq(splits[0] + splits[1] + splits[2], np.arange(100))\nsplits\n\n\n\n\n((#60) [0,1,2,3,4,5,6,7,8,9...],\n (#20) [60,61,62,63,64,65,66,67,68,69...],\n (#20) [80,81,82,83,84,85,86,87,88,89...])\n\n\n\nsplits = get_splits(np.random.randint(0,5,100), valid_size=0.213, test_size=17)\ntest_eq(len(splits[1]), 21)\ntest_eq(len(splits[2]), 17)\n\n\n\n\n\nsplits = get_splits(np.random.randint(0,5,100), valid_size=0.213, test_size=17, train_size=.2)\nsplits\n\n\n\n\n((#12) [58,22,7,60,82,14,67,69,59,47...],\n (#21) [18,34,46,65,8,30,55,42,97,71...],\n (#17) [41,57,43,95,86,88,35,24,5,81...])\n\n\n\nsource\n\n\ncombine_split_data\n\n combine_split_data (xs, ys=None)\n\nxs is a list with X_train, X_valid, …. ys is None or a list with y_train, y_valid, ….\n\nsource\n\n\nget_predefined_splits\n\n get_predefined_splits (*xs)\n\nxs is a list with X_train, X_valid, …\n\nsource\n\n\nget_splits_len\n\n get_splits_len (splits)\n\n\nX_train, y_train, X_valid, y_valid = np.random.rand(3,3,4), np.random.randint(0,2,3), np.random.rand(2,3,4), np.random.randint(0,2,2)\nX, y, splits = combine_split_data([X_train, X_valid], [y_train, y_valid])\ntest_eq(X_train, X[splits[0]])\ntest_eq(X_valid, X[splits[1]])\ntest_type(X_train, X)\ntest_type(y_train, y)\n\n\nX_train, y_train, X_valid, y_valid = np.random.rand(3,4), np.random.randint(0,2,3), np.random.rand(2,4), np.random.randint(0,2,2)\nX, y, splits = combine_split_data([X_train, X_valid], [y_train, y_valid])\ntest_eq(X_train[:, None], X[splits[0]])\ntest_eq(X_valid[:, None], X[splits[1]])\ntest_type(X_train, X)\ntest_type(y_train, y)\n\n\n\nForecasting\n\nsource\n\nget_df_usable_idxs\n\n get_df_usable_idxs (df, fcst_history, fcst_horizon, stride=1,\n                     unique_id_cols=None)\n\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ndf\n\n\ndataframe containing a sorted time series\n\n\nfcst_history\n\n\n# historical steps used as input (size of the sliding window for the input)\n\n\nfcst_horizon\n\n\n# steps forecasted into the future (size of the sliding window for the target)\n\n\nstride\nint\n1\nint or tuple of 2 int containing the strides of the sliding windows (input and target)\n\n\nunique_id_cols\nNoneType\nNone\nstr indicating the column/s with the unique identifier/s for each entity\n\n\n\n\nsource\n\n\nget_usable_idxs\n\n get_usable_idxs (df, fcst_history, fcst_horizon, stride=1)\n\n\nsource\n\n\ncalculate_fcst_stats\n\n calculate_fcst_stats (df, fcst_history, fcst_horizon, splits,\n                       x_vars=None, y_vars=None, subset_size=None)\n\nCalculates the training stats required in a forecasting task\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ndf\n\n\ndataframe containing a sorted time series for a single entity or subject\n\n\nfcst_history\n\n\n# historical steps used as input.\n\n\nfcst_horizon\n\n\n# steps forecasted into the future.\n\n\nsplits\n\n\nsplits that will be used to train the model. splits[0] is the train split:\n\n\nx_vars\nNoneType\nNone\nfeatures used as input\n\n\ny_vars\nNoneType\nNone\nfeatures used as output\n\n\nsubset_size\nNoneType\nNone\nint or float to determne the number of train samples used to calculate the mean and std\n\n\n\n\nsource\n\n\nget_forecasting_splits\n\n get_forecasting_splits (df, fcst_history, fcst_horizon, stride=1,\n                         valid_size=0.0, test_size=0.2,\n                         valid_cutoff_datetime=None,\n                         test_cutoff_datetime=None, datetime_col=None,\n                         use_index=False, unique_id_cols=None,\n                         show_plot=True)\n\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ndf\n\n\ndataframe containing a sorted time series\n\n\nfcst_history\n\n\n# historical steps used as input (size of the sliding window for the input)\n\n\nfcst_horizon\n\n\n# steps forecasted into the future (size of the sliding window for the target)\n\n\nstride\nint\n1\nint or tuple of 2 int containing the strides of the sliding windows (input and target)\n\n\nvalid_size\nfloat\n0.0\nint or float indicating the size of the training set (based on datetimes)\n\n\ntest_size\nfloat\n0.2\nint or float indicating the size of the test set (based on datetimes)\n\n\nvalid_cutoff_datetime\nNoneType\nNone\nfirst prediction datetime of validation dataset\n\n\ntest_cutoff_datetime\nNoneType\nNone\nfirst prediction datetime of test dataset\n\n\ndatetime_col\nNoneType\nNone\nstr indicating the column with the datetime values\n\n\nuse_index\nbool\nFalse\nflag to indicate if the datetime is in the index\n\n\nunique_id_cols\nNoneType\nNone\nstr indicating the column/s with the unique identifier/s for each entity\n\n\nshow_plot\nbool\nTrue\nflag to indicate if splits should be plotted\n\n\n\n\ndf1_len = 100\ndf2_len = 80\n\ndatetime_col = 'datetime' \ndf1 = pd.DataFrame(np.arange(df1_len), columns=['value'])\ndf1['datetime'] = pd.date_range(pd.to_datetime('1749-03-31'), periods=df1_len, freq='1D')\ndf1['type'] = 1\n\ndf = df1\ndisplay(df)\n\n# settings\nfcst_history          = 10\nfcst_horizon          = 1\nstride                = 1\nunique_id_cols        = 'type'\ndatetime_col          = 'datetime' \nuse_index             = False\nvalid_size            = 0.1  # a percent (float) or a number of samples (int) - .1 means 10% of the dates\ntest_size             = 0.2  # a percent (float) or a number of samples (int) - .1 means 10% of the dates\nvalid_cutoff_datetime = '1749-08-21' # first prediction datetime of validation dataset\ntest_cutoff_datetime  = '1749-12-24' # first prediction datetime of test dataset\nvalid_cutoff_datetime = None # datetime compatible with the datetime_col containing the starting date for the validation dataset\ntest_cutoff_datetime  = None # datetime compatible with the datetime_col containing the starting date for the validation dataset\n\n\nsplits = get_forecasting_splits(df, fcst_history=fcst_history, fcst_horizon=fcst_horizon, stride=stride, \n                                unique_id_cols=unique_id_cols, datetime_col=datetime_col, use_index=use_index,\n                                valid_size=valid_size, test_size=test_size, \n                                valid_cutoff_datetime=valid_cutoff_datetime, test_cutoff_datetime=test_cutoff_datetime)\n\nprint(f\"splits size   : {[len(s) for s in splits]} ({sum([len(s) for s in splits])}: {[round(len(s)/sum([len(s) for s in splits]), 2) for s in splits]})\")\n\n# settings\nfcst_history          = 10\nfcst_horizon          = 5\nstride                = 5\nunique_id_cols        = 'type'\ndatetime_col          = 'datetime' \nuse_index             = False\nvalid_size            = 0.1  # a percent (float) or a number of samples (int) - .1 means 10% of the dates\ntest_size             = 0.2  # a percent (float) or a number of samples (int) - .1 means 10% of the dates\nvalid_cutoff_datetime = '1749-08-21' # first prediction datetime of validation dataset\ntest_cutoff_datetime  = '1749-12-24' # first prediction datetime of test dataset\nvalid_cutoff_datetime = None # datetime compatible with the datetime_col containing the starting date for the validation dataset\ntest_cutoff_datetime  = None # datetime compatible with the datetime_col containing the starting date for the validation dataset\n\n\nsplits = get_forecasting_splits(df, fcst_history=fcst_history, fcst_horizon=fcst_horizon, stride=stride, \n                                unique_id_cols=unique_id_cols, datetime_col=datetime_col, use_index=use_index,\n                                valid_size=valid_size, test_size=test_size, \n                                valid_cutoff_datetime=valid_cutoff_datetime, test_cutoff_datetime=test_cutoff_datetime)\n\nprint(f\"splits size   : {[len(s) for s in splits]} ({sum([len(s) for s in splits])}: {[round(len(s)/sum([len(s) for s in splits]), 2) for s in splits]})\")\n\n\n\n\n\n  \n    \n      \n      value\n      datetime\n      type\n    \n  \n  \n    \n      0\n      0\n      1749-03-31\n      1\n    \n    \n      1\n      1\n      1749-04-01\n      1\n    \n    \n      2\n      2\n      1749-04-02\n      1\n    \n    \n      3\n      3\n      1749-04-03\n      1\n    \n    \n      4\n      4\n      1749-04-04\n      1\n    \n    \n      ...\n      ...\n      ...\n      ...\n    \n    \n      95\n      95\n      1749-07-04\n      1\n    \n    \n      96\n      96\n      1749-07-05\n      1\n    \n    \n      97\n      97\n      1749-07-06\n      1\n    \n    \n      98\n      98\n      1749-07-07\n      1\n    \n    \n      99\n      99\n      1749-07-08\n      1\n    \n  \n\n100 rows × 3 columns\n\n\n\n\n\n\nsplits size   : [63, 9, 18] (90: [0.7, 0.1, 0.2])\n\n\n\n\n\nsplits size   : [12, 2, 4] (18: [0.67, 0.11, 0.22])\n\n\n\ndf1_len = 100\ndf2_len = 80\n\ndatetime_col = 'datetime' \ndf1 = pd.DataFrame(np.arange(df1_len), columns=['value'])\ndf1['datetime'] = pd.date_range(pd.to_datetime('1749-03-31'), periods=df1_len, freq='1D')\ndf1['type'] = 1\ndf1_index = df1.set_index(\"datetime\")\n\ndf = df1_index\ndisplay(df)\n\n# settings\nfcst_history          = 10\nfcst_horizon          = 1\nstride                = 1\nunique_id_cols        = 'type'\ndatetime_col          = 'datetime' \nuse_index             = True\nvalid_size            = 0.1  # a percent (float) or a number of samples (int) - .1 means 10% of the dates\ntest_size             = 0.2  # a percent (float) or a number of samples (int) - .1 means 10% of the dates\nvalid_cutoff_datetime = '1749-08-21' # first prediction datetime of validation dataset\ntest_cutoff_datetime  = '1749-12-24' # first prediction datetime of test dataset\nvalid_cutoff_datetime = None # datetime compatible with the datetime_col containing the starting date for the validation dataset\ntest_cutoff_datetime  = None # datetime compatible with the datetime_col containing the starting date for the validation dataset\n\n\nsplits = get_forecasting_splits(df, fcst_history=fcst_history, fcst_horizon=fcst_horizon, stride=stride, \n                                unique_id_cols=unique_id_cols, datetime_col=datetime_col, use_index=use_index,\n                                valid_size=valid_size, test_size=test_size, \n                                valid_cutoff_datetime=valid_cutoff_datetime, test_cutoff_datetime=test_cutoff_datetime)\n\nprint(f\"splits size   : {[len(s) for s in splits]} ({sum([len(s) for s in splits])}: {[round(len(s)/sum([len(s) for s in splits]), 2) for s in splits]})\")\n\n# settings\nfcst_history          = 10\nfcst_horizon          = 5\nstride                = 5\nunique_id_cols        = 'type'\ndatetime_col          = 'datetime' \nuse_index             = True\nvalid_size            = 0.1  # a percent (float) or a number of samples (int) - .1 means 10% of the dates\ntest_size             = 0.2  # a percent (float) or a number of samples (int) - .1 means 10% of the dates\nvalid_cutoff_datetime = '1749-08-21' # first prediction datetime of validation dataset\ntest_cutoff_datetime  = '1749-12-24' # first prediction datetime of test dataset\nvalid_cutoff_datetime = None # datetime compatible with the datetime_col containing the starting date for the validation dataset\ntest_cutoff_datetime  = None # datetime compatible with the datetime_col containing the starting date for the validation dataset\n\n\nsplits = get_forecasting_splits(df, fcst_history=fcst_history, fcst_horizon=fcst_horizon, stride=stride, \n                                unique_id_cols=unique_id_cols, datetime_col=datetime_col, use_index=use_index,\n                                valid_size=valid_size, test_size=test_size, \n                                valid_cutoff_datetime=valid_cutoff_datetime, test_cutoff_datetime=test_cutoff_datetime)\n\nprint(f\"splits size   : {[len(s) for s in splits]} ({sum([len(s) for s in splits])}: {[round(len(s)/sum([len(s) for s in splits]), 2) for s in splits]})\")\n\n\n\n\n\n  \n    \n      \n      value\n      type\n    \n    \n      datetime\n      \n      \n    \n  \n  \n    \n      1749-03-31\n      0\n      1\n    \n    \n      1749-04-01\n      1\n      1\n    \n    \n      1749-04-02\n      2\n      1\n    \n    \n      1749-04-03\n      3\n      1\n    \n    \n      1749-04-04\n      4\n      1\n    \n    \n      ...\n      ...\n      ...\n    \n    \n      1749-07-04\n      95\n      1\n    \n    \n      1749-07-05\n      96\n      1\n    \n    \n      1749-07-06\n      97\n      1\n    \n    \n      1749-07-07\n      98\n      1\n    \n    \n      1749-07-08\n      99\n      1\n    \n  \n\n100 rows × 2 columns\n\n\n\n\n\n\nsplits size   : [63, 9, 18] (90: [0.7, 0.1, 0.2])\n\n\n\n\n\nsplits size   : [12, 2, 4] (18: [0.67, 0.11, 0.22])\n\n\n\ndf1_len = 100\ndf2_len = 80\n\ndatetime_col = 'datetime' \ndf1 = pd.DataFrame(np.arange(df1_len), columns=['value'])\ndf1['datetime'] = pd.date_range(pd.to_datetime('1749-03-31'), periods=df1_len, freq='1D')\ndf1['type'] = 1\ndf1_index = df1.set_index(\"datetime\")\ndf2 = pd.DataFrame(np.arange(df2_len) * 10, columns=['value'])\ndf2['datetime'] = pd.date_range(pd.to_datetime('1749-04-15'), periods=df2_len, freq='1D')\ndf2['type'] = 2\ndf_comb = pd.concat([df1, df2]).reset_index(drop=True).reset_index(drop=True)\n\n\ndf = df_comb\ndisplay(df)\n\n# settings\nfcst_history          = 10\nfcst_horizon          = 3\nstride                = 1\nunique_id_cols        = 'type'\ndatetime_col          = 'datetime' \nuse_index             = False\nvalid_size            = 0.1  # a percent (float) or a number of samples (int) - .1 means 10% of the dates\ntest_size             = 0.2  # a percent (float) or a number of samples (int) - .1 means 10% of the dates\nvalid_cutoff_datetime = '1749-08-21' # first prediction datetime of validation dataset\ntest_cutoff_datetime  = '1749-12-24' # first prediction datetime of test dataset\nvalid_cutoff_datetime = None # datetime compatible with the datetime_col containing the starting date for the validation dataset\ntest_cutoff_datetime  = None # datetime compatible with the datetime_col containing the starting date for the validation dataset\n\n\nsplits = get_forecasting_splits(df, fcst_history=fcst_history, fcst_horizon=fcst_horizon, stride=stride, \n                                unique_id_cols=unique_id_cols, datetime_col=datetime_col, use_index=use_index,\n                                valid_size=valid_size, test_size=test_size, \n                                valid_cutoff_datetime=valid_cutoff_datetime, test_cutoff_datetime=test_cutoff_datetime)\n\nprint(f\"splits size   : {[len(s) for s in splits]} ({sum([len(s) for s in splits])}: {[round(len(s)/sum([len(s) for s in splits]), 2) for s in splits]})\")\n\n\n\n\n\n  \n    \n      \n      value\n      datetime\n      type\n    \n  \n  \n    \n      0\n      0\n      1749-03-31\n      1\n    \n    \n      1\n      1\n      1749-04-01\n      1\n    \n    \n      2\n      2\n      1749-04-02\n      1\n    \n    \n      3\n      3\n      1749-04-03\n      1\n    \n    \n      4\n      4\n      1749-04-04\n      1\n    \n    \n      ...\n      ...\n      ...\n      ...\n    \n    \n      175\n      750\n      1749-06-29\n      2\n    \n    \n      176\n      760\n      1749-06-30\n      2\n    \n    \n      177\n      770\n      1749-07-01\n      2\n    \n    \n      178\n      780\n      1749-07-02\n      2\n    \n    \n      179\n      790\n      1749-07-03\n      2\n    \n  \n\n180 rows × 3 columns\n\n\n\n\n\n\nsplits size   : [101, 16, 31] (148: [0.68, 0.11, 0.21])\n\n\n\ndf1_len = 100\ndf2_len = 80\n\ndatetime_col = 'datetime' \ndf1 = pd.DataFrame(np.arange(df1_len), columns=['value'])\ndf1['datetime'] = pd.date_range(pd.to_datetime('1749-03-31'), periods=df1_len, freq='1D')\ndf1['type'] = 1\ndf1_index = df1.set_index(\"datetime\")\ndf2 = pd.DataFrame(np.arange(df2_len) * 10, columns=['value'])\ndf2['datetime'] = pd.date_range(pd.to_datetime('1749-04-15'), periods=df2_len, freq='1D')\ndf2['type'] = 2\ndf_comb = pd.concat([df1, df2]).reset_index(drop=True).reset_index(drop=True)\ndf_comb_index = df_comb.set_index(\"datetime\")\ndf_comb_index.index.name = None\n\n\ndf = df_comb_index\ndisplay(df)\n\n# settings\nfcst_history          = 15\nfcst_horizon          = 5\nstride                = 1\nunique_id_cols        = 'type'\ndatetime_col          = 'datetime' \nuse_index             = True\nvalid_size            = 0.1  # a percent (float) or a number of samples (int) - .1 means 10% of the dates\ntest_size             = 0.2  # a percent (float) or a number of samples (int) - .1 means 10% of the dates\nvalid_cutoff_datetime = '1749-08-21' # first prediction datetime of validation dataset\ntest_cutoff_datetime  = '1749-12-24' # first prediction datetime of test dataset\nvalid_cutoff_datetime = None # datetime compatible with the datetime_col containing the starting date for the validation dataset\ntest_cutoff_datetime  = None # datetime compatible with the datetime_col containing the starting date for the validation dataset\n\n\nsplits = get_forecasting_splits(df, fcst_history=fcst_history, fcst_horizon=fcst_horizon, stride=stride, \n                                unique_id_cols=unique_id_cols, datetime_col=datetime_col, use_index=use_index,\n                                valid_size=valid_size, test_size=test_size, \n                                valid_cutoff_datetime=valid_cutoff_datetime, test_cutoff_datetime=test_cutoff_datetime)\n\nprint(f\"splits size   : {[len(s) for s in splits]} ({sum([len(s) for s in splits])}: {[round(len(s)/sum([len(s) for s in splits]), 2) for s in splits]})\")\n\n\n\n\n\n  \n    \n      \n      value\n      type\n    \n  \n  \n    \n      1749-03-31\n      0\n      1\n    \n    \n      1749-04-01\n      1\n      1\n    \n    \n      1749-04-02\n      2\n      1\n    \n    \n      1749-04-03\n      3\n      1\n    \n    \n      1749-04-04\n      4\n      1\n    \n    \n      ...\n      ...\n      ...\n    \n    \n      1749-06-29\n      750\n      2\n    \n    \n      1749-06-30\n      760\n      2\n    \n    \n      1749-07-01\n      770\n      2\n    \n    \n      1749-07-02\n      780\n      2\n    \n    \n      1749-07-03\n      790\n      2\n    \n  \n\n180 rows × 2 columns\n\n\n\n\n\n\nsplits size   : [83, 14, 29] (126: [0.66, 0.11, 0.23])\n\n\n\nsource\n\n\nget_long_term_forecasting_splits\n\n get_long_term_forecasting_splits (df, fcst_history, fcst_horizon,\n                                   dsid=None, show_plot=True)\n\nReturns the train, valid and test splits for long-range time series datasets\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ndf\n\n\ndataframe containing a sorted time series for a single entity or subject\n\n\nfcst_history\n\n\n# historical steps used as input.\n\n\nfcst_horizon\n\n\n# steps forecasted into the future.\n\n\ndsid\nNoneType\nNone\ndataset name\n\n\nshow_plot\nbool\nTrue\nplot the splits"
  },
  {
    "objectID": "data.mixed.html",
    "href": "data.mixed.html",
    "title": "Mixed data",
    "section": "",
    "text": "DataLoader than can take data from multiple dataloaders with different types of data\n\n\nsource\n\nMixedDataLoaders\n\n MixedDataLoaders (*loaders, path:str|Path='.', device=None)\n\nBasic wrapper around several DataLoaders.\n\nsource\n\n\nMixedDataLoader\n\n MixedDataLoader (*loaders, path='.', shuffle=False, device=None, bs=None)\n\nAccepts any number of DataLoader and a device\n\nsource\n\n\nget_mixed_dls\n\n get_mixed_dls (*dls, device=None, shuffle_train=None, shuffle_valid=None,\n                **kwargs)\n\n\nfrom tsai.data.tabular import *\n\n\npath = untar_data(URLs.ADULT_SAMPLE)\ndf = pd.read_csv(path/'adult.csv')\n# df['salary'] = np.random.rand(len(df)) # uncomment to simulate a cont dependent variable\ntarget = 'salary'\nsplits = RandomSplitter()(range_of(df))\n\ncat_names = ['workclass', 'education', 'marital-status']\ncont_names = ['age', 'fnlwgt']\ndls1 = get_tabular_dls(df, cat_names=cat_names, cont_names=cont_names, y_names=target, splits=splits, bs=512)\ndls1.show_batch()\n\ncat_names = None #['occupation', 'relationship', 'race']\ncont_names = ['education-num']\ndls2 = get_tabular_dls(df, cat_names=cat_names, cont_names=cont_names, y_names=target, splits=splits, bs=128)\ndls2.show_batch()\n\n\n\n  \n    \n      \n      workclass\n      education\n      marital-status\n      age\n      fnlwgt\n      salary\n    \n  \n  \n    \n      0\n      Private\n      Bachelors\n      Married-civ-spouse\n      59.999999\n      131680.999115\n      >=50k\n    \n    \n      1\n      Private\n      12th\n      Never-married\n      18.000000\n      311795.000052\n      <50k\n    \n    \n      2\n      Private\n      HS-grad\n      Married-civ-spouse\n      45.000000\n      350440.002257\n      >=50k\n    \n    \n      3\n      Local-gov\n      Masters\n      Never-married\n      44.000000\n      101593.001253\n      <50k\n    \n    \n      4\n      ?\n      Some-college\n      Never-married\n      20.999999\n      41355.995576\n      <50k\n    \n    \n      5\n      Private\n      Bachelors\n      Never-married\n      30.000000\n      207668.000292\n      <50k\n    \n    \n      6\n      Federal-gov\n      Bachelors\n      Never-married\n      28.000000\n      281859.998606\n      <50k\n    \n    \n      7\n      ?\n      Some-college\n      Never-married\n      20.999999\n      180338.999810\n      <50k\n    \n    \n      8\n      Private\n      Some-college\n      Never-married\n      20.000000\n      174713.999509\n      <50k\n    \n    \n      9\n      Self-emp-not-inc\n      Bachelors\n      Married-civ-spouse\n      50.000000\n      334273.005863\n      <50k\n    \n  \n\n\n\n\n\n  \n    \n      \n      education-num_na\n      education-num\n      salary\n    \n  \n  \n    \n      0\n      False\n      9.0\n      <50k\n    \n    \n      1\n      False\n      9.0\n      <50k\n    \n    \n      2\n      False\n      13.0\n      >=50k\n    \n    \n      3\n      False\n      9.0\n      <50k\n    \n    \n      4\n      False\n      9.0\n      <50k\n    \n    \n      5\n      False\n      13.0\n      >=50k\n    \n    \n      6\n      False\n      10.0\n      <50k\n    \n    \n      7\n      False\n      10.0\n      <50k\n    \n    \n      8\n      False\n      13.0\n      <50k\n    \n    \n      9\n      False\n      10.0\n      <50k\n    \n  \n\n\n\n\ndls = get_mixed_dls(dls1, dls2, bs=8)\nfirst(dls.train)\nfirst(dls.valid)\ntorch.save(dls,'export/mixed_dls.pth')\ndel dls\ndls = torch.load('export/mixed_dls.pth')\ndls.train.show_batch()\n\n\n\n  \n    \n      \n      workclass\n      education\n      marital-status\n      age\n      fnlwgt\n      salary\n    \n  \n  \n    \n      0\n      State-gov\n      HS-grad\n      Never-married\n      43.000000\n      23156.998049\n      <50k\n    \n    \n      1\n      Private\n      11th\n      Married-civ-spouse\n      32.000000\n      140092.001434\n      <50k\n    \n    \n      2\n      Self-emp-not-inc\n      HS-grad\n      Never-married\n      43.000000\n      48086.995399\n      <50k\n    \n    \n      3\n      Self-emp-not-inc\n      Assoc-acdm\n      Never-married\n      34.000000\n      177638.999728\n      <50k\n    \n    \n      4\n      Local-gov\n      Masters\n      Married-civ-spouse\n      65.000001\n      146453.999176\n      <50k\n    \n    \n      5\n      Private\n      HS-grad\n      Married-civ-spouse\n      33.000000\n      227281.999333\n      <50k\n    \n    \n      6\n      Private\n      HS-grad\n      Never-married\n      33.000000\n      194900.999911\n      <50k\n    \n    \n      7\n      Private\n      HS-grad\n      Divorced\n      23.000000\n      259301.002460\n      <50k\n    \n  \n\n\n\n\n\n  \n    \n      \n      education-num_na\n      education-num\n      salary\n    \n  \n  \n    \n      0\n      False\n      9.0\n      <50k\n    \n    \n      1\n      False\n      7.0\n      <50k\n    \n    \n      2\n      False\n      9.0\n      <50k\n    \n    \n      3\n      False\n      12.0\n      <50k\n    \n    \n      4\n      False\n      14.0\n      <50k\n    \n    \n      5\n      True\n      10.0\n      <50k\n    \n    \n      6\n      False\n      9.0\n      <50k\n    \n    \n      7\n      False\n      9.0\n      <50k\n    \n  \n\n\n\n\nxb, yb = first(dls.train)\nxb\n\n((tensor([[ 8, 12,  5],\n          [ 5,  2,  3],\n          [ 7, 12,  5],\n          [ 7,  8,  5],\n          [ 3, 13,  3],\n          [ 5, 12,  3],\n          [ 5, 12,  5],\n          [ 5, 12,  1]]),\n  tensor([[ 0.3222, -1.5782],\n          [-0.4850, -0.4696],\n          [ 0.3222, -1.3418],\n          [-0.3383, -0.1136],\n          [ 1.9368, -0.4093],\n          [-0.4117,  0.3570],\n          [-0.4117,  0.0500],\n          [-1.1455,  0.6606]])),\n (tensor([[1],\n          [1],\n          [1],\n          [1],\n          [1],\n          [2],\n          [1],\n          [1]]),\n  tensor([[-0.4258],\n          [-1.2097],\n          [-0.4258],\n          [ 0.7502],\n          [ 1.5342],\n          [-0.0338],\n          [-0.4258],\n          [-0.4258]])))\n\n\n\nxs, ys = first(dls.train)\nxs[0][0].shape, xs[0][1].shape, xs[1][0].shape, xs[1][1].shape\n\n(torch.Size([8, 3]),\n torch.Size([8, 2]),\n torch.Size([8, 1]),\n torch.Size([8, 1]))\n\n\n\nfrom tsai.data.validation import TimeSplitter\nfrom tsai.data.core import TSRegression, get_ts_dls\n\n\nX = np.repeat(np.repeat(np.arange(8)[:, None, None], 2, 1), 5, 2).astype(float)\nX = np.concatenate([X, X])\ny = np.concatenate([np.arange(len(X)//2)]*2)\nalphabet = np.array(list(string.ascii_lowercase))\n# y = alphabet[y]\nsplits = TimeSplitter(.5, show_plot=False)(range_of(X))\ntfms = [None, TSRegression()]\ndls1 = get_ts_dls(X, y, splits=splits, tfms=tfms)\ndls1.one_batch()\n\n(TSTensor(samples:8, vars:2, len:5, device=cpu, dtype=torch.float32),\n tensor([7., 0., 2., 1., 5., 4., 3., 6.]))\n\n\n\ndata = np.concatenate([np.repeat(np.arange(8)[:, None], 3, 1)*np.array([1, 10, 100])]*2)\ndf = pd.DataFrame(data, columns=['cat1', 'cat2', 'cont'])\ndf['cont'] = df['cont'].astype(float)\ndf['target'] = y\ncat_names = ['cat1', 'cat2']\ncont_names = ['cont']\ntarget = 'target'\ndls2 = get_tabular_dls(df, procs=[Categorify, FillMissing, #Normalize\n                                 ], cat_names=cat_names, cont_names=cont_names, y_names=target, splits=splits, bs=8)\ndls2.one_batch()\n\n(tensor([[2, 2],\n         [5, 5],\n         [1, 1],\n         [7, 7],\n         [3, 3],\n         [6, 6],\n         [8, 8],\n         [4, 4]]),\n tensor([[100.],\n         [400.],\n         [  0.],\n         [600.],\n         [200.],\n         [500.],\n         [700.],\n         [300.]]),\n tensor([[1],\n         [4],\n         [0],\n         [6],\n         [2],\n         [5],\n         [7],\n         [3]], dtype=torch.int8))\n\n\n\nz = zip(_loaders[dls1.train.fake_l.num_workers == 0](dls1.train.fake_l))\nfor b in z: \n    print(b)\n    break\n\n((TSTensor(samples:8, vars:2, len:5, device=cpu, dtype=torch.float32), tensor([7., 0., 2., 1., 5., 4., 3., 6.])),)\n\n\n\nbs = 8\ndls = get_mixed_dls(dls1, dls2, bs=bs)\ndl = dls.train\nxb, yb = dl.one_batch()\ntest_eq(len(xb), 2)\ntest_eq(len(xb[0]), bs)\ntest_eq(len(xb[1]), 2)\ntest_eq(len(xb[1][0]), bs)\ntest_eq(len(xb[1][1]), bs)\ntest_eq(xb[0].data[:, 0, 0].long(), xb[1][0][:, 0] - 1) # categorical data and ts are in synch\ntest_eq(xb[0].data[:, 0, 0], (xb[1][1]/100).flatten()) # continuous data and ts are in synch\ntest_eq(tensor(dl.input_idxs), yb.long().cpu())\ndl = dls.valid\nxb, yb = dl.one_batch()\ntest_eq(tensor(y[dl.input_idxs]), yb.long().cpu())"
  },
  {
    "objectID": "models.fcn.html",
    "href": "models.fcn.html",
    "title": "FCN",
    "section": "",
    "text": "This is an unofficial PyTorch implementation created by Ignacio Oguiza (oguiza@timeseriesAI.co) based on:\n\nWang, Z., Yan, W., & Oates, T. (2017, May). Time series classification from scratch with deep neural networks: A strong baseline. In 2017 international joint conference on neural networks (IJCNN) (pp. 1578-1585). IEEE.\nFawaz, H. I., Forestier, G., Weber, J., Idoumghar, L., & Muller, P. A. (2019). Deep learning for time series classification: a review. Data Mining and Knowledge Discovery, 33(4), 917-963.\n\nOfficial FCN TensorFlow implementation: https://github.com/hfawaz/dl-4-tsc/blob/master/classifiers/fcn.py.\nNote: kernel filter size 8 has been replaced by 7 (since we believe it’s a bug).\n\nsource\n\nFCN\n\n FCN (c_in, c_out, layers=[128, 256, 128], kss=[7, 5, 3])\n\nSame as nn.Module, but no need for subclasses to call super().__init__\n\nbs = 16\nnvars = 3\nseq_len = 128\nc_out = 2\nxb = torch.rand(bs, nvars, seq_len)\nmodel = FCN(nvars, c_out)\ntest_eq(model(xb).shape, (bs, c_out))\nmodel\n\nFCN(\n  (convblock1): ConvBlock(\n    (0): Conv1d(3, 128, kernel_size=(7,), stride=(1,), padding=(3,), bias=False)\n    (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (2): ReLU()\n  )\n  (convblock2): ConvBlock(\n    (0): Conv1d(128, 256, kernel_size=(5,), stride=(1,), padding=(2,), bias=False)\n    (1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (2): ReLU()\n  )\n  (convblock3): ConvBlock(\n    (0): Conv1d(256, 128, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)\n    (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (2): ReLU()\n  )\n  (gap): GAP1d(\n    (gap): AdaptiveAvgPool1d(output_size=1)\n    (flatten): Flatten(full=False)\n  )\n  (fc): Linear(in_features=128, out_features=2, bias=True)\n)"
  },
  {
    "objectID": "data.external.html",
    "href": "data.external.html",
    "title": "External data",
    "section": "",
    "text": "Helper functions used to download and extract common time series datasets.\n\n\nsource\n\ndecompress_from_url\n\n decompress_from_url (url, target_dir=None, verbose=False)\n\n\nsource\n\n\ndownload_data\n\n download_data (url, fname=None, c_key='archive', force_download=False,\n                timeout=4, verbose=False)\n\nDownload url to fname.\n\nsource\n\n\nget_UCR_univariate_list\n\n get_UCR_univariate_list ()\n\n\nsource\n\n\nget_UCR_multivariate_list\n\n get_UCR_multivariate_list ()\n\n\nsource\n\n\nget_UCR_data\n\n get_UCR_data (dsid, path='.', parent_dir='data/UCR', on_disk=True,\n               mode='c', Xdtype='float32', ydtype=None, return_split=True,\n               split_data=True, force_download=False, verbose=False)\n\n\nfrom fastai.data.transforms import get_files\n\n\nPATH = Path('.')\ndsids = ['ECGFiveDays', 'AtrialFibrillation'] # univariate and multivariate\nfor dsid in dsids:\n    print(dsid)\n    tgt_dir = PATH/f'data/UCR/{dsid}'\n    if os.path.isdir(tgt_dir): shutil.rmtree(tgt_dir)\n    test_eq(len(get_files(tgt_dir)), 0) # no file left\n    X_train, y_train, X_valid, y_valid = get_UCR_data(dsid)\n    test_eq(len(get_files(tgt_dir, '.npy')), 6)\n    test_eq(len(get_files(tgt_dir, '.npy')), len(get_files(tgt_dir))) # test no left file/ dir\n    del X_train, y_train, X_valid, y_valid\n    start = time.time()\n    X_train, y_train, X_valid, y_valid = get_UCR_data(dsid)\n    elapsed = time.time() - start\n    test_eq(elapsed < 1, True)\n    test_eq(X_train.ndim, 3)\n    test_eq(y_train.ndim, 1)\n    test_eq(X_valid.ndim, 3)\n    test_eq(y_valid.ndim, 1)\n    test_eq(len(get_files(tgt_dir, '.npy')), 6)\n    test_eq(len(get_files(tgt_dir, '.npy')), len(get_files(tgt_dir))) # test no left file/ dir\n    test_eq(X_train.ndim, 3)\n    test_eq(y_train.ndim, 1)\n    test_eq(X_valid.ndim, 3)\n    test_eq(y_valid.ndim, 1)\n    test_eq(X_train.dtype, np.float32)\n    test_eq(X_train.__class__.__name__, 'memmap')\n    del X_train, y_train, X_valid, y_valid\n    X_train, y_train, X_valid, y_valid = get_UCR_data(dsid, on_disk=False)\n    test_eq(X_train.__class__.__name__, 'ndarray')\n    del X_train, y_train, X_valid, y_valid\n\nECGFiveDays\nAtrialFibrillation\n\n\n\nX_train, y_train, X_valid, y_valid = get_UCR_data('natops')\n\n\ndsid = 'natops' \nX_train, y_train, X_valid, y_valid = get_UCR_data(dsid, verbose=True)\nX, y, splits = get_UCR_data(dsid, split_data=False)\ntest_eq(X[splits[0]], X_train)\ntest_eq(y[splits[1]], y_valid)\ntest_eq(X[splits[0]], X_train)\ntest_eq(y[splits[1]], y_valid)\ntest_type(X, X_train)\ntest_type(y, y_train)\n\nDataset: NATOPS\nX_train: (180, 24, 51)\ny_train: (180,)\nX_valid: (180, 24, 51)\ny_valid: (180,) \n\n\n\n\nsource\n\n\ncheck_data\n\n check_data (X, y=None, splits=None, show_plot=True)\n\n\ndsid = 'ECGFiveDays'\nX, y, splits = get_UCR_data(dsid, split_data=False, on_disk=False, force_download=False)\ncheck_data(X, y, splits)\ncheck_data(X[:, 0], y, splits)\ny = y.astype(np.float32)\ncheck_data(X, y, splits)\ny[:10] = np.nan\ncheck_data(X[:, 0], y, splits)\nX, y, splits = get_UCR_data(dsid, split_data=False, on_disk=False, force_download=False)\nsplits = get_splits(y, 3)\ncheck_data(X, y, splits)\ncheck_data(X[:, 0], y, splits)\ny[:5]= np.nan\ncheck_data(X[:, 0], y, splits)\nX, y, splits = get_UCR_data(dsid, split_data=False, on_disk=False, force_download=False)\n\nX      - shape: [884 samples x 1 features x 136 timesteps]  type: ndarray  dtype:float32  isnan: 0\ny      - shape: (884,)  type: ndarray  dtype:<U1  n_classes: 2 (442 samples per class) ['1', '2']  isnan: False\nsplits - n_splits: 2 shape: [23, 861]  overlap: False\n\n\n\n\n\nX      - shape: (884, 136)  type: ndarray  dtype:float32  isnan: 0\ny      - shape: (884,)  type: ndarray  dtype:<U1  n_classes: 2 (442 samples per class) ['1', '2']  isnan: False\nsplits - n_splits: 2 shape: [23, 861]  overlap: False\n\n\n\n\n\nX      - shape: [884 samples x 1 features x 136 timesteps]  type: ndarray  dtype:float32  isnan: 0\ny      - shape: (884,)  type: ndarray  dtype:float32  isnan: 0\nsplits - n_splits: 2 shape: [23, 861]  overlap: False\n\n\n\n\n\nX      - shape: (884, 136)  type: ndarray  dtype:float32  isnan: 0\ny      - shape: (884,)  type: ndarray  dtype:float32  isnan: 10\nsplits - n_splits: 2 shape: [23, 861]  overlap: False\n\n\n/Users/nacho/opt/anaconda3/envs/py37torch113/lib/python3.7/site-packages/ipykernel_launcher.py:23: UserWarning: y contains nan values\n\n\n\n\n\n\n\n\nX      - shape: [884 samples x 1 features x 136 timesteps]  type: ndarray  dtype:float32  isnan: 0\ny      - shape: (884,)  type: ndarray  dtype:<U1  n_classes: 2 (442 samples per class) ['1', '2']  isnan: False\nsplits - n_splits: 3 shape: [[589, 295], [589, 295], [590, 294]]  overlap: [False, False, False]\n\n\n\n\n\nX      - shape: (884, 136)  type: ndarray  dtype:float32  isnan: 0\ny      - shape: (884,)  type: ndarray  dtype:<U1  n_classes: 2 (442 samples per class) ['1', '2']  isnan: False\nsplits - n_splits: 3 shape: [[589, 295], [589, 295], [590, 294]]  overlap: [False, False, False]\n\n\n\n\n\nX      - shape: (884, 136)  type: ndarray  dtype:float32  isnan: 0\ny      - shape: (884,)  type: ndarray  dtype:<U1  n_classes: 3 (294 samples per class) ['1', '2', 'n']  isnan: False\nsplits - n_splits: 3 shape: [[589, 295], [589, 295], [590, 294]]  overlap: [False, False, False]\n\n\n\n\n\n\nsource\n\n\nget_Monash_regression_list\n\n get_Monash_regression_list ()\n\n\nsource\n\n\nget_Monash_regression_data\n\n get_Monash_regression_data (dsid, path='./data/Monash', on_disk=True,\n                             mode='c', Xdtype='float32', ydtype=None,\n                             split_data=True, force_download=False,\n                             verbose=False, timeout=4)\n\n\ndsid = \"Covid3Month\"\nX_train, y_train, X_valid, y_valid = get_Monash_regression_data(dsid, on_disk=False, split_data=True, force_download=False)\nX, y, splits = get_Monash_regression_data(dsid, on_disk=True, split_data=False, force_download=False, verbose=True)\nif X_train is not None: \n    test_eq(X_train.shape, (140, 1, 84))\nif X is not None: \n    test_eq(X.shape, (201, 1, 84))\n\nDataset: Covid3Month\nX      : (201, 1, 84)\ny      : (201,)\nsplits : (#140) [0,1,2,3,4,5,6,7,8,9...] (#61) [140,141,142,143,144,145,146,147,148,149...] \n\n\n\n\nsource\n\n\nget_forecasting_list\n\n get_forecasting_list ()\n\n\nsource\n\n\nget_forecasting_time_series\n\n get_forecasting_time_series (dsid, path='./data/forecasting/',\n                              force_download=False, verbose=True,\n                              **kwargs)\n\n\nts = get_forecasting_time_series(\"sunspots\", force_download=False)\ntest_eq(len(ts), 2820)\nts\n\nDataset: Sunspots\ndownloading data...\n...done. Path = data/forecasting/Sunspots.csv\n\n\n\n\n\n\n  \n    \n      \n      Sunspots\n    \n    \n      Month\n      \n    \n  \n  \n    \n      1749-01-31\n      58.0\n    \n    \n      1749-02-28\n      62.6\n    \n    \n      1749-03-31\n      70.0\n    \n    \n      1749-04-30\n      55.7\n    \n    \n      1749-05-31\n      85.0\n    \n    \n      ...\n      ...\n    \n    \n      1983-08-31\n      71.8\n    \n    \n      1983-09-30\n      50.3\n    \n    \n      1983-10-31\n      55.8\n    \n    \n      1983-11-30\n      33.3\n    \n    \n      1983-12-31\n      33.4\n    \n  \n\n2820 rows × 1 columns\n\n\n\n\nts = get_forecasting_time_series(\"weather\", force_download=False)\nif ts is not None: \n    test_eq(len(ts), 70091)\n    display(ts)\n\nDataset: Weather\ndownloading data...\n...done. Path = data/forecasting/Weather.csv.zip\n\n\n\n\n\n\n  \n    \n      \n      p (mbar)\n      T (degC)\n      Tpot (K)\n      Tdew (degC)\n      rh (%)\n      VPmax (mbar)\n      VPact (mbar)\n      VPdef (mbar)\n      sh (g/kg)\n      H2OC (mmol/mol)\n      rho (g/m**3)\n      Wx\n      Wy\n      max Wx\n      max Wy\n      Day sin\n      Day cos\n      Year sin\n      Year cos\n    \n  \n  \n    \n      0\n      996.50\n      -8.05\n      265.38\n      -8.78\n      94.40\n      3.33\n      3.14\n      0.19\n      1.96\n      3.15\n      1307.86\n      -0.204862\n      -0.046168\n      -0.614587\n      -0.138503\n      -1.776611e-12\n      1.000000\n      0.009332\n      0.999956\n    \n    \n      1\n      996.62\n      -8.88\n      264.54\n      -9.77\n      93.20\n      3.12\n      2.90\n      0.21\n      1.81\n      2.91\n      1312.25\n      -0.245971\n      -0.044701\n      -0.619848\n      -0.112645\n      2.588190e-01\n      0.965926\n      0.010049\n      0.999950\n    \n    \n      2\n      996.84\n      -8.81\n      264.59\n      -9.66\n      93.50\n      3.13\n      2.93\n      0.20\n      1.83\n      2.94\n      1312.18\n      -0.175527\n      0.039879\n      -0.614344\n      0.139576\n      5.000000e-01\n      0.866025\n      0.010766\n      0.999942\n    \n    \n      3\n      996.99\n      -9.05\n      264.34\n      -10.02\n      92.60\n      3.07\n      2.85\n      0.23\n      1.78\n      2.85\n      1313.61\n      -0.050000\n      -0.086603\n      -0.190000\n      -0.329090\n      7.071068e-01\n      0.707107\n      0.011483\n      0.999934\n    \n    \n      4\n      997.46\n      -9.63\n      263.72\n      -10.65\n      92.20\n      2.94\n      2.71\n      0.23\n      1.69\n      2.71\n      1317.19\n      -0.368202\n      0.156292\n      -0.810044\n      0.343843\n      8.660254e-01\n      0.500000\n      0.012199\n      0.999926\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      70086\n      1002.18\n      -0.98\n      272.01\n      -5.36\n      72.00\n      5.69\n      4.09\n      1.59\n      2.54\n      4.08\n      1280.70\n      -0.855154\n      -0.160038\n      -1.336792\n      -0.250174\n      -9.990482e-01\n      0.043619\n      0.006183\n      0.999981\n    \n    \n      70087\n      1001.40\n      -1.40\n      271.66\n      -6.84\n      66.29\n      5.51\n      3.65\n      1.86\n      2.27\n      3.65\n      1281.87\n      -0.716196\n      -0.726267\n      -1.348134\n      -1.367090\n      -9.537170e-01\n      0.300706\n      0.006900\n      0.999976\n    \n    \n      70088\n      1001.19\n      -2.75\n      270.32\n      -6.90\n      72.90\n      4.99\n      3.64\n      1.35\n      2.26\n      3.63\n      1288.02\n      -0.661501\n      0.257908\n      -1.453438\n      0.566672\n      -8.433914e-01\n      0.537300\n      0.007617\n      0.999971\n    \n    \n      70089\n      1000.65\n      -2.89\n      270.22\n      -7.15\n      72.30\n      4.93\n      3.57\n      1.37\n      2.22\n      3.57\n      1288.03\n      -0.280621\n      -0.209169\n      -0.545207\n      -0.406385\n      -6.755902e-01\n      0.737277\n      0.008334\n      0.999965\n    \n    \n      70090\n      1000.11\n      -3.93\n      269.23\n      -8.09\n      72.60\n      4.56\n      3.31\n      1.25\n      2.06\n      3.31\n      1292.41\n      -0.516998\n      -0.215205\n      -0.923210\n      -0.384295\n      -4.617486e-01\n      0.887011\n      0.009050\n      0.999959\n    \n  \n\n70091 rows × 19 columns\n\n\n\n\nsource\n\n\nconvert_tsf_to_dataframe\n\n convert_tsf_to_dataframe (full_file_path_and_name,\n                           replace_missing_vals_with='NaN',\n                           value_column_name='series_value')\n\n\nsource\n\n\nget_Monash_forecasting_data\n\n get_Monash_forecasting_data (dsid, path='./data/forecasting/',\n                              force_download=False,\n                              remove_from_disk=False, add_timestamp=True,\n                              verbose=True)\n\n\nsource\n\n\nget_fcst_horizon\n\n get_fcst_horizon (frequency, dsid)\n\n\nsource\n\n\npreprocess_Monash_df\n\n preprocess_Monash_df (df, frequency)\n\n\ndsid = 'covid_deaths_dataset'\ndf = get_Monash_forecasting_data(dsid, force_download=False)\nif df is not None: \n    test_eq(df.shape, (56392, 3))\n\nDataset: covid_deaths_dataset\nconverting data to dataframe...\n...done\n\nfreq                   : daily\nforecast_horizon       : 30\ncontain_missing_values : False\ncontain_equal_length   : True\n\nexploding dataframe...\n...done\n\n\ndata.shape: (56392, 3)\n\n\n\nsource\n\n\ndownload_all_long_term_forecasting_data\n\n download_all_long_term_forecasting_data\n                                          (target_dir='./data/long_forecas\n                                          ting/', force_download=False,\n                                          remove_zip=False,\n                                          c_key='archive', timeout=4,\n                                          verbose=True)\n\n\nsource\n\n\nunzip_file\n\n unzip_file (file, target_dir)\n\n\nsource\n\n\nget_long_term_forecasting_data\n\n get_long_term_forecasting_data (dsid,\n                                 target_dir='./data/long_forecasting/',\n                                 task='M', fcst_horizon=None,\n                                 fcst_history=None, preprocess=True,\n                                 force_download=False, remove_zip=False,\n                                 return_df=True, show_plot=True,\n                                 dtype=<class 'numpy.float32'>,\n                                 verbose=True, **kwargs)\n\nDownloads (and preprocess) a pandas dataframe with the requested long-term forecasting dataset\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ndsid\n\n\nID of the dataset to be used for long-term forecasting.\n\n\ntarget_dir\nstr\n./data/long_forecasting/\nDirectory where the long-term forecasting data will be saved.\n\n\ntask\nstr\nM\n‘M’ for multivariate, ‘S’ for univariate and ‘MS’ for multivariate input with univariate output\n\n\nfcst_horizon\nNoneType\nNone\n# historical steps used as input. If None, the default is applied.\n\n\nfcst_history\nNoneType\nNone\n# steps forecasted into the future. If None, the minimum default is applied.\n\n\npreprocess\nbool\nTrue\nFlag that indicates whether if the data is preprocessed before saving.\n\n\nforce_download\nbool\nFalse\nFlag that indicates if the data should be downloaded again even if directory exists.\n\n\nremove_zip\nbool\nFalse\nFlag that indicates if the zip file should be removed after extracting the data.\n\n\nreturn_df\nbool\nTrue\nFlag that indicates whether a dataframe (True) or X and and y arrays (False) are returned.\n\n\nshow_plot\nbool\nTrue\nplot the splits\n\n\ndtype\ntype\nfloat32\n\n\n\nverbose\nbool\nTrue\nFlag tto indicate the verbosity.\n\n\nkwargs\n\n\n\n\n\n\n\ndsid = \"ILI\"\ntry:\n    df = get_long_term_forecasting_data(dsid, target_dir='./data/forecasting/', force_download=False)\n    print(f\"{dsid:15}: {str(df.shape):15}\")\n    del df; gc.collect()\n    remove_dir('./data/forecasting/', False)\nexcept Exception as e:\n    print(f\"{dsid:15}: {str(e):15}\")\n\n\n\n\n\nILI            : (966, 8)       \n\n\n\ndsid = \"ILI\"\ntry:\n    X, y, splits, stats = get_long_term_forecasting_data(dsid, target_dir='./data/forecasting/', force_download=False, return_df=False, show_plot=False)\n    print(f\"{dsid:15} -  X.shape: {str(X.shape):20}  y.shape: {str(y.shape):20}  splits: {str([len(s) for s in splits]):25}  \\\nstats: {str([s.shape for s in stats]):30}\")\n    del X, y, splits, stats\n    gc.collect()\n    remove_dir('./data/forecasting/', False)\nexcept Exception as e:\n    print(f\"{dsid:15}: {str(e):15}\")\n\nILI             -  X.shape: (839, 7, 104)         y.shape: (839, 7, 24)          splits: [549, 74, 170]             stats: [(1, 7, 1), (1, 7, 1)]"
  },
  {
    "objectID": "models.minirocket_pytorch.html",
    "href": "models.minirocket_pytorch.html",
    "title": "MINIROCKET Pytorch",
    "section": "",
    "text": "A Very Fast (Almost) Deterministic Transform for Time Series Classification.\n\nThis is a Pytorch implementation of MiniRocket developed by Malcolm McLean and Ignacio Oguiza based on:\nDempster, A., Schmidt, D. F., & Webb, G. I. (2020). MINIROCKET: A Very Fast (Almost) Deterministic Transform for Time Series Classification. arXiv preprint arXiv:2012.08791.\nOriginal paper: https://arxiv.org/abs/2012.08791\nOriginal code: https://github.com/angus924/minirocket\n\nsource\n\nMiniRocketFeatures\n\n MiniRocketFeatures (c_in, seq_len, num_features=10000,\n                     max_dilations_per_kernel=32, random_state=None)\n\nThis is a Pytorch implementation of MiniRocket developed by Malcolm McLean and Ignacio Oguiza\nMiniRocket paper citation: @article{dempster_etal_2020, author = {Dempster, Angus and Schmidt, Daniel F and Webb, Geoffrey I}, title = {{MINIROCKET}: A Very Fast (Almost) Deterministic Transform for Time Series Classification}, year = {2020}, journal = {arXiv:2012.08791} } Original paper: https://arxiv.org/abs/2012.08791 Original code: https://github.com/angus924/minirocket\n\nsource\n\n\nget_minirocket_features\n\n get_minirocket_features (o, model, chunksize=1024, use_cuda=None,\n                          to_np=True)\n\nFunction used to split a large dataset into chunks, avoiding OOM error.\n\nsource\n\n\nMiniRocketHead\n\n MiniRocketHead (c_in, c_out, seq_len=1, bn=True, fc_dropout=0.0)\n\nA sequential container. Modules will be added to it in the order they are passed in the constructor. Alternatively, an OrderedDict of modules can be passed in. The forward() method of [Sequential](https://timeseriesAI.github.io/models.layers.html#sequential) accepts any input and forwards it to the first module it contains. It then “chains” outputs to inputs sequentially for each subsequent module, finally returning the output of the last module.\nThe value a [Sequential](https://timeseriesAI.github.io/models.layers.html#sequential) provides over manually calling a sequence of modules is that it allows treating the whole container as a single module, such that performing a transformation on the [Sequential](https://timeseriesAI.github.io/models.layers.html#sequential) applies to each of the modules it stores (which are each a registered submodule of the [Sequential](https://timeseriesAI.github.io/models.layers.html#sequential)).\nWhat’s the difference between a [Sequential](https://timeseriesAI.github.io/models.layers.html#sequential) and a :class:torch.nn.ModuleList? A ModuleList is exactly what it sounds like–a list for storing Module s! On the other hand, the layers in a [Sequential](https://timeseriesAI.github.io/models.layers.html#sequential) are connected in a cascading way.\nExample::\n# Using Sequential to create a small model. When `model` is run,\n# input will first be passed to `Conv2d(1,20,5)`. The output of\n# `Conv2d(1,20,5)` will be used as the input to the first\n# `ReLU`; the output of the first `ReLU` will become the input\n# for `Conv2d(20,64,5)`. Finally, the output of\n# `Conv2d(20,64,5)` will be used as input to the second `ReLU`\nmodel = nn.Sequential(\n          nn.Conv2d(1,20,5),\n          nn.ReLU(),\n          nn.Conv2d(20,64,5),\n          nn.ReLU()\n        )\n\n# Using Sequential with OrderedDict. This is functionally the\n# same as the above code\nmodel = nn.Sequential(OrderedDict([\n          ('conv1', nn.Conv2d(1,20,5)),\n          ('relu1', nn.ReLU()),\n          ('conv2', nn.Conv2d(20,64,5)),\n          ('relu2', nn.ReLU())\n        ]))\n\nsource\n\n\nMiniRocket\n\n MiniRocket (c_in, c_out, seq_len, num_features=10000,\n             max_dilations_per_kernel=32, random_state=None, bn=True,\n             fc_dropout=0)\n\nA sequential container. Modules will be added to it in the order they are passed in the constructor. Alternatively, an OrderedDict of modules can be passed in. The forward() method of [Sequential](https://timeseriesAI.github.io/models.layers.html#sequential) accepts any input and forwards it to the first module it contains. It then “chains” outputs to inputs sequentially for each subsequent module, finally returning the output of the last module.\nThe value a [Sequential](https://timeseriesAI.github.io/models.layers.html#sequential) provides over manually calling a sequence of modules is that it allows treating the whole container as a single module, such that performing a transformation on the [Sequential](https://timeseriesAI.github.io/models.layers.html#sequential) applies to each of the modules it stores (which are each a registered submodule of the [Sequential](https://timeseriesAI.github.io/models.layers.html#sequential)).\nWhat’s the difference between a [Sequential](https://timeseriesAI.github.io/models.layers.html#sequential) and a :class:torch.nn.ModuleList? A ModuleList is exactly what it sounds like–a list for storing Module s! On the other hand, the layers in a [Sequential](https://timeseriesAI.github.io/models.layers.html#sequential) are connected in a cascading way.\nExample::\n# Using Sequential to create a small model. When `model` is run,\n# input will first be passed to `Conv2d(1,20,5)`. The output of\n# `Conv2d(1,20,5)` will be used as the input to the first\n# `ReLU`; the output of the first `ReLU` will become the input\n# for `Conv2d(20,64,5)`. Finally, the output of\n# `Conv2d(20,64,5)` will be used as input to the second `ReLU`\nmodel = nn.Sequential(\n          nn.Conv2d(1,20,5),\n          nn.ReLU(),\n          nn.Conv2d(20,64,5),\n          nn.ReLU()\n        )\n\n# Using Sequential with OrderedDict. This is functionally the\n# same as the above code\nmodel = nn.Sequential(OrderedDict([\n          ('conv1', nn.Conv2d(1,20,5)),\n          ('relu1', nn.ReLU()),\n          ('conv2', nn.Conv2d(20,64,5)),\n          ('relu2', nn.ReLU())\n        ]))\n\nfrom tsai.imports import default_device\nfrom fastai.metrics import accuracy\nfrom fastai.callback.tracker import ReduceLROnPlateau\nfrom tsai.data.all import *\nfrom tsai.learner import *\n\n\n# Offline feature calculation\ndsid = 'ECGFiveDays'\nX, y, splits = get_UCR_data(dsid, split_data=False)\nmrf = MiniRocketFeatures(c_in=X.shape[1], seq_len=X.shape[2]).to(default_device())\nX_train = X[splits[0]]  # X_train may either be a np.ndarray or a torch.Tensor\nmrf.fit(X_train)\nX_tfm = get_minirocket_features(X, mrf)\ntfms = [None, TSClassification()]\nbatch_tfms = TSStandardize(by_var=True)\ndls = get_ts_dls(X_tfm, y, splits=splits, tfms=tfms, batch_tfms=batch_tfms, bs=256)\nlearn = ts_learner(dls, MiniRocketHead, metrics=accuracy)\nlearn.fit(1, 1e-4, cbs=ReduceLROnPlateau(factor=0.5, min_lr=1e-8, patience=10))\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      accuracy\n      time\n    \n  \n  \n    \n      0\n      0.693147\n      0.530879\n      0.752613\n      00:00\n    \n  \n\n\n\n\n# Online feature calculation\ndsid = 'ECGFiveDays'\nX, y, splits = get_UCR_data(dsid, split_data=False)\ntfms = [None, TSClassification()]\nbatch_tfms = TSStandardize()\ndls = get_ts_dls(X, y, splits=splits, tfms=tfms, batch_tfms=batch_tfms, bs=256)\nlearn = ts_learner(dls, MiniRocket, metrics=accuracy)\nlearn.fit_one_cycle(1, 1e-2)\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      accuracy\n      time\n    \n  \n  \n    \n      0\n      0.693147\n      0.713297\n      0.502904\n      00:06"
  },
  {
    "objectID": "models.inceptiontime.html",
    "href": "models.inceptiontime.html",
    "title": "InceptionTime",
    "section": "",
    "text": "An ensemble of deep Convolutional Neural Network (CNN) models, inspired by the Inception-v4 architecture\n\nThis is an unofficial PyTorch implementation created by Ignacio Oguiza (oguiza@timeseriesAI.co) based on:\nFawaz, H. I., Lucas, B., Forestier, G., Pelletier, C., Schmidt, D. F., Weber, J. & Petitjean, F. (2019). InceptionTime: Finding AlexNet for Time Series Classification. arXiv preprint arXiv:1909.04939.\nOfficial InceptionTime tensorflow implementation: https://github.com/hfawaz/InceptionTime\n\nsource\n\nInceptionTime\n\n InceptionTime (c_in, c_out, seq_len=None, nf=32, nb_filters=None, ks=40,\n                bottleneck=True)\n\nSame as nn.Module, but no need for subclasses to call super().__init__\n\nsource\n\n\nInceptionBlock\n\n InceptionBlock (ni, nf=32, residual=True, depth=6, ks=40,\n                 bottleneck=True)\n\nSame as nn.Module, but no need for subclasses to call super().__init__\n\nsource\n\n\nInceptionModule\n\n InceptionModule (ni, nf, ks=40, bottleneck=True)\n\nSame as nn.Module, but no need for subclasses to call super().__init__\n\nfrom tsai.models.utils import count_parameters\n\n\nbs = 16\nvars = 1\nseq_len = 12\nc_out = 2\nxb = torch.rand(bs, vars, seq_len)\ntest_eq(InceptionTime(vars,c_out)(xb).shape, [bs, c_out])\ntest_eq(InceptionTime(vars,c_out, bottleneck=False)(xb).shape, [bs, c_out])\ntest_eq(InceptionTime(vars,c_out, residual=False)(xb).shape, [bs, c_out])\ntest_eq(count_parameters(InceptionTime(3, 2)), 455490)\n\n\nInceptionTime(3,2)\n\nInceptionTime(\n  (inceptionblock): InceptionBlock(\n    (inception): ModuleList(\n      (0): InceptionModule(\n        (bottleneck): Conv1d(3, 32, kernel_size=(1,), stride=(1,), bias=False)\n        (convs): ModuleList(\n          (0): Conv1d(32, 32, kernel_size=(39,), stride=(1,), padding=(19,), bias=False)\n          (1): Conv1d(32, 32, kernel_size=(19,), stride=(1,), padding=(9,), bias=False)\n          (2): Conv1d(32, 32, kernel_size=(9,), stride=(1,), padding=(4,), bias=False)\n        )\n        (maxconvpool): Sequential(\n          (0): MaxPool1d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)\n          (1): Conv1d(3, 32, kernel_size=(1,), stride=(1,), bias=False)\n        )\n        (concat): Concat(dim=1)\n        (bn): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (act): ReLU()\n      )\n      (1): InceptionModule(\n        (bottleneck): Conv1d(128, 32, kernel_size=(1,), stride=(1,), bias=False)\n        (convs): ModuleList(\n          (0): Conv1d(32, 32, kernel_size=(39,), stride=(1,), padding=(19,), bias=False)\n          (1): Conv1d(32, 32, kernel_size=(19,), stride=(1,), padding=(9,), bias=False)\n          (2): Conv1d(32, 32, kernel_size=(9,), stride=(1,), padding=(4,), bias=False)\n        )\n        (maxconvpool): Sequential(\n          (0): MaxPool1d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)\n          (1): Conv1d(128, 32, kernel_size=(1,), stride=(1,), bias=False)\n        )\n        (concat): Concat(dim=1)\n        (bn): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (act): ReLU()\n      )\n      (2): InceptionModule(\n        (bottleneck): Conv1d(128, 32, kernel_size=(1,), stride=(1,), bias=False)\n        (convs): ModuleList(\n          (0): Conv1d(32, 32, kernel_size=(39,), stride=(1,), padding=(19,), bias=False)\n          (1): Conv1d(32, 32, kernel_size=(19,), stride=(1,), padding=(9,), bias=False)\n          (2): Conv1d(32, 32, kernel_size=(9,), stride=(1,), padding=(4,), bias=False)\n        )\n        (maxconvpool): Sequential(\n          (0): MaxPool1d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)\n          (1): Conv1d(128, 32, kernel_size=(1,), stride=(1,), bias=False)\n        )\n        (concat): Concat(dim=1)\n        (bn): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (act): ReLU()\n      )\n      (3): InceptionModule(\n        (bottleneck): Conv1d(128, 32, kernel_size=(1,), stride=(1,), bias=False)\n        (convs): ModuleList(\n          (0): Conv1d(32, 32, kernel_size=(39,), stride=(1,), padding=(19,), bias=False)\n          (1): Conv1d(32, 32, kernel_size=(19,), stride=(1,), padding=(9,), bias=False)\n          (2): Conv1d(32, 32, kernel_size=(9,), stride=(1,), padding=(4,), bias=False)\n        )\n        (maxconvpool): Sequential(\n          (0): MaxPool1d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)\n          (1): Conv1d(128, 32, kernel_size=(1,), stride=(1,), bias=False)\n        )\n        (concat): Concat(dim=1)\n        (bn): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (act): ReLU()\n      )\n      (4): InceptionModule(\n        (bottleneck): Conv1d(128, 32, kernel_size=(1,), stride=(1,), bias=False)\n        (convs): ModuleList(\n          (0): Conv1d(32, 32, kernel_size=(39,), stride=(1,), padding=(19,), bias=False)\n          (1): Conv1d(32, 32, kernel_size=(19,), stride=(1,), padding=(9,), bias=False)\n          (2): Conv1d(32, 32, kernel_size=(9,), stride=(1,), padding=(4,), bias=False)\n        )\n        (maxconvpool): Sequential(\n          (0): MaxPool1d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)\n          (1): Conv1d(128, 32, kernel_size=(1,), stride=(1,), bias=False)\n        )\n        (concat): Concat(dim=1)\n        (bn): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (act): ReLU()\n      )\n      (5): InceptionModule(\n        (bottleneck): Conv1d(128, 32, kernel_size=(1,), stride=(1,), bias=False)\n        (convs): ModuleList(\n          (0): Conv1d(32, 32, kernel_size=(39,), stride=(1,), padding=(19,), bias=False)\n          (1): Conv1d(32, 32, kernel_size=(19,), stride=(1,), padding=(9,), bias=False)\n          (2): Conv1d(32, 32, kernel_size=(9,), stride=(1,), padding=(4,), bias=False)\n        )\n        (maxconvpool): Sequential(\n          (0): MaxPool1d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)\n          (1): Conv1d(128, 32, kernel_size=(1,), stride=(1,), bias=False)\n        )\n        (concat): Concat(dim=1)\n        (bn): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (act): ReLU()\n      )\n    )\n    (shortcut): ModuleList(\n      (0): ConvBlock(\n        (0): Conv1d(3, 128, kernel_size=(1,), stride=(1,), bias=False)\n        (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n      (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (add): Add\n    (act): ReLU()\n  )\n  (gap): GAP1d(\n    (gap): AdaptiveAvgPool1d(output_size=1)\n    (flatten): Flatten(full=False)\n  )\n  (fc): Linear(in_features=128, out_features=2, bias=True)\n)"
  },
  {
    "objectID": "data.metadatasets.html",
    "href": "data.metadatasets.html",
    "title": "Metadataset",
    "section": "",
    "text": "A dataset of datasets\n\nThis functionality will allow you to create a dataset from data stores in multiple, smaller datasets.\nI’d like to thank both Thomas Capelle (https://github.com/tcapelle) and Xander Dunn (https://github.com/xanderdunn) for their contributions to make this code possible.\nThis functionality allows you to use multiple numpy arrays instead of a single one, which may be very useful in many practical settings. It’s been tested it with 10k+ datasets and it works well.\n\nsource\n\nTSMetaDatasets\n\n TSMetaDatasets (metadataset, splits)\n\nBase class for lists with subsets\n\nsource\n\n\nTSMetaDataset\n\n TSMetaDataset (dataset_list, **kwargs)\n\nA dataset capable of indexing mutiple datasets at the same time\nLet’s create 3 datasets. In this case they will have different sizes.\n\nvocab = L(['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j'])\ndsets = []\nfor i in range(3):\n    size = np.random.randint(50, 150)\n    X = torch.rand(size, 5, 50)\n    y = vocab[torch.randint(0, 10, (size,))]\n    tfms = [None, TSClassification()]\n    dset = TSDatasets(X, y, tfms=tfms)\n    dsets.append(dset)\ndsets\n\n[(#129) [(tensor([[5.3669e-01, 1.0522e-01, 2.3424e-01, 2.8666e-01, 9.0664e-01, 3.2788e-01,\n          9.1225e-01, 9.1162e-02, 7.5022e-01, 5.9094e-02, 1.6774e-01, 4.0472e-04,\n          6.2178e-01, 3.8152e-01, 5.2377e-01, 8.5151e-01, 7.2163e-01, 4.1720e-01,\n          3.2478e-01, 1.5009e-01, 3.0171e-02, 9.7913e-01, 5.1613e-01, 5.2005e-01,\n          7.7834e-01, 7.3598e-02, 2.0829e-03, 5.6736e-01, 3.2840e-01, 2.4965e-02,\n          1.9927e-01, 7.2145e-01, 8.8769e-01, 1.2570e-01, 2.3218e-01, 7.4932e-01,\n          4.1147e-01, 9.5307e-01, 3.7898e-01, 7.0439e-01, 4.2171e-01, 9.6501e-01,\n          9.4748e-01, 6.0352e-01, 5.5169e-02, 9.5845e-01, 2.3753e-01, 3.6956e-01,\n          3.4563e-01, 1.6815e-01],\n         [2.6036e-01, 4.5708e-02, 5.5474e-01, 1.1786e-02, 4.2807e-01, 8.2726e-01,\n          6.9222e-01, 4.2812e-02, 9.5162e-01, 7.4347e-01, 2.7092e-02, 6.4381e-01,\n          9.8559e-01, 2.7430e-01, 1.4690e-01, 5.5904e-01, 2.8087e-01, 1.7434e-01,\n          6.0433e-01, 2.1684e-01, 2.0410e-01, 4.1278e-01, 3.7491e-01, 1.8963e-01,\n          9.4323e-01, 5.0934e-03, 6.1097e-01, 1.9053e-03, 3.5555e-01, 6.7718e-01,\n          4.5428e-01, 7.2556e-01, 3.6295e-01, 5.1161e-01, 1.0379e-01, 9.4708e-01,\n          5.5298e-01, 1.2386e-02, 4.3137e-01, 1.2166e-01, 9.2162e-01, 5.5694e-01,\n          1.2802e-01, 5.8418e-01, 3.2311e-01, 7.0058e-01, 1.1091e-01, 2.0442e-01,\n          8.0807e-01, 9.3576e-01],\n         [6.6859e-01, 8.1102e-01, 1.4716e-01, 4.9197e-01, 7.6329e-01, 2.0206e-01,\n          7.8409e-01, 6.5934e-01, 8.0484e-01, 4.8772e-01, 4.9931e-01, 2.7149e-01,\n          3.5654e-01, 1.8816e-01, 4.7005e-02, 3.5958e-01, 2.6489e-01, 4.3182e-01,\n          4.9941e-01, 8.9695e-01, 2.3790e-01, 1.8101e-01, 8.9604e-01, 7.5587e-01,\n          6.2590e-01, 9.4712e-01, 4.1853e-01, 7.9081e-01, 6.7329e-01, 5.3824e-02,\n          2.7264e-01, 4.9127e-02, 4.6076e-01, 1.1538e-01, 1.9280e-01, 9.9060e-01,\n          2.2843e-01, 5.8155e-01, 6.9064e-01, 4.6487e-01, 9.8144e-01, 7.7350e-01,\n          1.4695e-01, 3.0052e-01, 5.5623e-01, 7.8291e-01, 4.1759e-01, 1.8181e-01,\n          6.2212e-01, 6.0173e-01],\n         [5.3279e-01, 7.4861e-02, 1.4173e-01, 5.1657e-01, 4.1626e-01, 4.2824e-01,\n          4.2244e-01, 4.0111e-02, 4.8833e-01, 2.4868e-01, 2.6360e-01, 7.6799e-02,\n          5.1504e-01, 2.8002e-01, 7.2894e-01, 9.0459e-01, 5.5897e-02, 6.8786e-01,\n          7.8586e-01, 3.4009e-01, 1.6494e-01, 7.8797e-01, 9.4162e-01, 9.8920e-01,\n          4.7574e-01, 3.8370e-01, 1.0169e-01, 5.6337e-01, 6.4677e-01, 6.6762e-01,\n          6.6937e-01, 6.9632e-01, 2.6747e-01, 6.4317e-02, 3.6788e-01, 8.3139e-01,\n          5.8124e-01, 2.0493e-01, 4.5675e-01, 9.7881e-01, 4.5096e-01, 5.4965e-01,\n          6.0515e-01, 9.9260e-01, 1.0823e-01, 6.0786e-01, 3.8485e-02, 2.4682e-01,\n          6.2507e-01, 7.4985e-01],\n         [2.1396e-01, 7.7012e-01, 4.2857e-02, 2.3004e-02, 6.2476e-01, 9.0339e-01,\n          2.3276e-01, 5.3264e-01, 1.8161e-01, 2.3229e-01, 6.3197e-01, 1.0900e-01,\n          6.4241e-02, 2.6869e-01, 9.1943e-01, 1.8767e-02, 7.0595e-01, 9.3483e-01,\n          8.2962e-01, 4.3135e-01, 9.5682e-01, 3.1314e-01, 1.4003e-01, 5.6419e-02,\n          5.1554e-01, 7.4020e-01, 5.1055e-01, 5.3088e-01, 5.0654e-01, 7.2600e-01,\n          4.8987e-01, 8.0443e-02, 3.2597e-01, 1.9611e-01, 8.0565e-01, 4.8723e-01,\n          8.3932e-01, 6.2803e-01, 7.9229e-01, 2.2802e-01, 6.5720e-01, 6.5338e-01,\n          2.4883e-01, 9.8642e-01, 1.9997e-01, 3.5434e-01, 7.6342e-01, 1.6060e-01,\n          2.4060e-01, 1.4820e-01]]), TensorCategory(1)), (tensor([[0.7827, 0.0024, 0.8999, 0.9005, 0.5539, 0.0380, 0.8148, 0.9231, 0.8770,\n          0.7892, 0.8088, 0.6730, 0.0937, 0.7113, 0.8490, 0.6794, 0.5632, 0.1759,\n          0.5723, 0.0116, 0.4811, 0.1231, 0.8304, 0.6547, 0.8646, 0.2010, 0.3336,\n          0.0784, 0.7042, 0.0595, 0.6948, 0.6407, 0.7066, 0.1918, 0.2178, 0.0986,\n          0.5695, 0.5035, 0.9758, 0.7405, 0.5121, 0.4725, 0.7798, 0.4227, 0.3542,\n          0.0985, 0.1156, 0.6263, 0.7524, 0.7897],\n         [0.5544, 0.8828, 0.8645, 0.5230, 0.7209, 0.9081, 0.9851, 0.0752, 0.2691,\n          0.3525, 0.2189, 0.9243, 0.4460, 0.5331, 0.3956, 0.5897, 0.9545, 0.6982,\n          0.3649, 0.4463, 0.3193, 0.7124, 0.3302, 0.4342, 0.5516, 0.0220, 0.2500,\n          0.4784, 0.1613, 0.0741, 0.3182, 0.2748, 0.7779, 0.1063, 0.3410, 0.4966,\n          0.1808, 0.9698, 0.9763, 0.5184, 0.1266, 0.2594, 0.0252, 0.1007, 0.0213,\n          0.5332, 0.8761, 0.8890, 0.5513, 0.9329],\n         [0.1505, 0.5972, 0.9965, 0.1049, 0.5502, 0.8051, 0.5908, 0.2697, 0.9226,\n          0.3837, 0.4059, 0.5507, 0.0548, 0.2844, 0.1184, 0.4607, 0.7579, 0.0988,\n          0.9316, 0.4398, 0.8954, 0.6691, 0.0270, 0.9882, 0.6812, 0.8488, 0.6916,\n          0.9190, 0.1903, 0.3671, 0.9992, 0.0119, 0.4547, 0.3194, 0.8242, 0.3490,\n          0.0273, 0.0742, 0.3157, 0.6916, 0.3559, 0.6533, 0.7469, 0.6196, 0.0078,\n          0.5988, 0.1526, 0.8124, 0.4061, 0.5550],\n         [0.6128, 0.7140, 0.6423, 0.1185, 0.7639, 0.2958, 0.3867, 0.3055, 0.8081,\n          0.8891, 0.9515, 0.7979, 0.5777, 0.7666, 0.6200, 0.2612, 0.4429, 0.0309,\n          0.7439, 0.5629, 0.5879, 0.5247, 0.7890, 0.1337, 0.1633, 0.9730, 0.6402,\n          0.0925, 0.0358, 0.4770, 0.2315, 0.5852, 0.7593, 0.7591, 0.5489, 0.0580,\n          0.0612, 0.2128, 0.3203, 0.0932, 0.3437, 0.9807, 0.2434, 0.5635, 0.4558,\n          0.3342, 0.0592, 0.1858, 0.9326, 0.4844],\n         [0.8151, 0.8906, 0.4550, 0.2889, 0.0202, 0.8205, 0.1715, 0.2561, 0.8065,\n          0.9192, 0.9509, 0.4047, 0.0279, 0.5206, 0.4418, 0.4314, 0.6152, 0.8398,\n          0.0267, 0.5397, 0.8217, 0.2933, 0.7070, 0.1256, 0.8931, 0.6134, 0.9153,\n          0.0700, 0.2866, 0.5582, 0.4263, 0.1494, 0.2773, 0.8439, 0.3597, 0.6780,\n          0.1127, 0.2304, 0.4367, 0.9240, 0.7549, 0.3168, 0.6296, 0.7417, 0.8806,\n          0.1809, 0.7364, 0.1907, 0.9264, 0.9209]]), TensorCategory(5)), (tensor([[0.8752, 0.5482, 0.1431, 0.6910, 0.0039, 0.4878, 0.3893, 0.6573, 0.5597,\n          0.2872, 0.5268, 0.0814, 0.1003, 0.5626, 0.1198, 0.9564, 0.8438, 0.8658,\n          0.0698, 0.6182, 0.9516, 0.6702, 0.7390, 0.7891, 0.4058, 0.0913, 0.4961,\n          0.4336, 0.6099, 0.9343, 0.6073, 0.5594, 0.3765, 0.5425, 0.8510, 0.0970,\n          0.9765, 0.3601, 0.6617, 0.0434, 0.4985, 0.9945, 0.0041, 0.1822, 0.6748,\n          0.7063, 0.5022, 0.7127, 0.4197, 0.9664],\n         [0.0559, 0.8831, 0.1578, 0.6038, 0.8672, 0.6463, 0.8380, 0.8988, 0.7689,\n          0.5145, 0.9826, 0.6660, 0.1529, 0.7667, 0.7558, 0.6969, 0.8336, 0.9438,\n          0.2202, 0.2834, 0.8424, 0.2133, 0.5675, 0.3007, 0.1058, 0.0284, 0.1676,\n          0.1066, 0.0784, 0.2523, 0.8549, 0.6211, 0.6824, 0.5414, 0.4323, 0.9712,\n          0.2996, 0.3540, 0.8251, 0.6446, 0.2031, 0.4788, 0.1052, 0.8206, 0.9175,\n          0.1109, 0.9297, 0.0591, 0.5617, 0.3838],\n         [0.5392, 0.7855, 0.7521, 0.9857, 0.5307, 0.0025, 0.1486, 0.6003, 0.2706,\n          0.1555, 0.1472, 0.5087, 0.5636, 0.0313, 0.0702, 0.2046, 0.5226, 0.2795,\n          0.2392, 0.3121, 0.6624, 0.5482, 0.2905, 0.3392, 0.6096, 0.2194, 0.8769,\n          0.3614, 0.2817, 0.8003, 0.1373, 0.4885, 0.9449, 0.8164, 0.1287, 0.0814,\n          0.2393, 0.3406, 0.8338, 0.0610, 0.4958, 0.7026, 0.8299, 0.6089, 0.1227,\n          0.1502, 0.8059, 0.5047, 0.8506, 0.1313],\n         [0.4440, 0.9960, 0.6786, 0.9904, 0.0039, 0.1782, 0.5922, 0.6082, 0.2061,\n          0.6687, 0.8001, 0.3106, 0.4316, 0.0707, 0.2971, 0.7829, 0.1634, 0.1257,\n          0.1483, 0.4900, 0.7291, 0.8074, 0.6301, 0.5053, 0.4350, 0.5038, 0.1996,\n          0.7555, 0.4741, 0.9309, 0.4560, 0.5645, 0.2888, 0.0419, 0.6222, 0.0074,\n          0.1472, 0.3005, 0.8626, 0.8394, 0.3078, 0.1641, 0.4233, 0.7413, 0.9118,\n          0.4020, 0.1738, 0.6453, 0.9042, 0.2527],\n         [0.0823, 0.9433, 0.5384, 0.7717, 0.9612, 0.7242, 0.9584, 0.5309, 0.0963,\n          0.1868, 0.0370, 0.0923, 0.2360, 0.1297, 0.2216, 0.8217, 0.9032, 0.9669,\n          0.6874, 0.6170, 0.5829, 0.1275, 0.7131, 0.6717, 0.6368, 0.6601, 0.7746,\n          0.7536, 0.3112, 0.4229, 0.3367, 0.0933, 0.8793, 0.7734, 0.3128, 0.7063,\n          0.2164, 0.3295, 0.2255, 0.7682, 0.9760, 0.4779, 0.6306, 0.5044, 0.4181,\n          0.8868, 0.9367, 0.6447, 0.9944, 0.5354]]), TensorCategory(3)), (tensor([[0.9423, 0.0957, 0.6043, 0.0758, 0.1774, 0.9564, 0.1341, 0.9028, 0.3587,\n          0.9565, 0.7819, 0.2125, 0.4916, 0.5396, 0.2894, 0.9539, 0.5755, 0.1592,\n          0.3127, 0.5602, 0.0465, 0.8953, 0.6942, 0.1122, 0.8744, 0.3147, 0.7451,\n          0.2300, 0.3610, 0.8490, 0.5958, 0.2657, 0.5499, 0.2641, 0.1005, 0.5276,\n          0.1150, 0.3247, 0.3323, 0.4637, 0.1782, 0.0704, 0.9190, 0.9652, 0.9221,\n          0.7380, 0.2482, 0.2301, 0.7721, 0.3733],\n         [0.4036, 0.3990, 0.9985, 0.8602, 0.2556, 0.1543, 0.4712, 0.0940, 0.3094,\n          0.5160, 0.6344, 0.2001, 0.5048, 0.0790, 0.2546, 0.3180, 0.9646, 0.8566,\n          0.0454, 0.6314, 0.3584, 0.9846, 0.5528, 0.8012, 0.8498, 0.7233, 0.1548,\n          0.3217, 0.3828, 0.4257, 0.0913, 0.0491, 0.9399, 0.9978, 0.8406, 0.3710,\n          0.1346, 0.8386, 0.6200, 0.4351, 0.9482, 0.2390, 0.7343, 0.8552, 0.9636,\n          0.7107, 0.7811, 0.0644, 0.8712, 0.4141],\n         [0.4664, 0.3568, 0.7814, 0.7397, 0.3294, 0.8006, 0.2155, 0.4141, 0.2706,\n          0.5881, 0.5416, 0.5120, 0.4080, 0.4068, 0.0901, 0.5861, 0.6091, 0.6147,\n          0.5171, 0.1183, 0.7330, 0.1439, 0.7751, 0.2559, 0.7912, 0.6727, 0.3155,\n          0.0765, 0.5970, 0.0428, 0.9724, 0.4617, 0.2157, 0.3108, 0.6836, 0.0290,\n          0.4427, 0.5479, 0.6297, 0.9311, 0.3002, 0.8964, 0.4344, 0.1576, 0.1074,\n          0.4828, 0.8473, 0.6141, 0.6978, 0.8755],\n         [0.5375, 0.1834, 0.9696, 0.5604, 0.9326, 0.1727, 0.6755, 0.9556, 0.4358,\n          0.6103, 0.0665, 0.0999, 0.0962, 0.9501, 0.2704, 0.4945, 0.8150, 0.6485,\n          0.3474, 0.6221, 0.6068, 0.2778, 0.1543, 0.6089, 0.3790, 0.7520, 0.1574,\n          0.1874, 0.3352, 0.2109, 0.0175, 0.6632, 0.4510, 0.1729, 0.1445, 0.0328,\n          0.1274, 0.2018, 0.2171, 0.5637, 0.5224, 0.9714, 0.9491, 0.1470, 0.1673,\n          0.4926, 0.0525, 0.7447, 0.4841, 0.2837],\n         [0.3544, 0.4497, 0.7505, 0.8566, 0.8192, 0.0821, 0.4791, 0.7657, 0.7195,\n          0.9282, 0.1067, 0.5789, 0.1791, 0.3459, 0.9824, 0.1457, 0.0882, 0.0601,\n          0.1288, 0.5409, 0.2303, 0.2873, 0.8269, 0.7544, 0.6329, 0.7686, 0.9693,\n          0.5069, 0.0021, 0.5578, 0.8883, 0.2383, 0.5150, 0.2098, 0.5773, 0.4666,\n          0.3221, 0.0266, 0.3037, 0.0080, 0.1136, 0.6878, 0.7047, 0.1353, 0.4565,\n          0.7737, 0.7786, 0.1630, 0.7676, 0.1451]]), TensorCategory(4)), (tensor([[0.2062, 0.2859, 0.7248, 0.8904, 0.1289, 0.8140, 0.8924, 0.7553, 0.2563,\n          0.1081, 0.9861, 0.8408, 0.4587, 0.0788, 0.0925, 0.4235, 0.8804, 0.0284,\n          0.4915, 0.8364, 0.0169, 0.0673, 0.2040, 0.3733, 0.7010, 0.1176, 0.4290,\n          0.0957, 0.8262, 0.0701, 0.4239, 0.2631, 0.5336, 0.1137, 0.1730, 0.4032,\n          0.8351, 0.2773, 0.7368, 0.5207, 0.4658, 0.9001, 0.6537, 0.1146, 0.5574,\n          0.7402, 0.6191, 0.0153, 0.4057, 0.4440],\n         [0.0141, 0.7877, 0.7618, 0.7990, 0.2024, 0.0831, 0.1699, 0.8442, 0.2582,\n          0.6770, 0.2835, 0.9647, 0.8901, 0.6499, 0.8632, 0.0852, 0.2771, 0.3085,\n          0.6764, 0.1792, 0.5581, 0.3164, 0.2935, 0.8785, 0.8210, 0.3713, 0.4170,\n          0.6867, 0.1042, 0.8845, 0.4668, 0.8309, 0.4131, 0.1544, 0.2023, 0.2653,\n          0.5299, 0.4288, 0.7837, 0.3593, 0.1080, 0.1661, 0.4063, 0.5897, 0.7898,\n          0.4245, 0.0670, 0.6423, 0.0283, 0.7128],\n         [0.7395, 0.3050, 0.3107, 0.6712, 0.7452, 0.1649, 0.8423, 0.8901, 0.2789,\n          0.1425, 0.4621, 0.7021, 0.8914, 0.2484, 0.8657, 0.1967, 0.8330, 0.2031,\n          0.0181, 0.8736, 0.5316, 0.7638, 0.8004, 0.9204, 0.6534, 0.5769, 0.6221,\n          0.3726, 0.3213, 0.4822, 0.8692, 0.5674, 0.7953, 0.7943, 0.6967, 0.3095,\n          0.7238, 0.0416, 0.0657, 0.5072, 0.2680, 0.0691, 0.7750, 0.2837, 0.9345,\n          0.0991, 0.0349, 0.1103, 0.1569, 0.9695],\n         [0.6804, 0.6490, 0.4391, 0.8077, 0.4067, 0.4792, 0.2499, 0.4346, 0.0620,\n          0.1821, 0.8521, 0.9119, 0.2354, 0.5640, 0.4007, 0.5259, 0.7139, 0.4220,\n          0.9703, 0.4285, 0.7965, 0.5332, 0.0741, 0.8655, 0.9418, 0.9422, 0.7721,\n          0.3591, 0.6173, 0.2758, 0.4946, 0.8974, 0.4397, 0.9757, 0.9768, 0.7306,\n          0.6302, 0.4381, 0.2038, 0.4853, 0.8299, 0.9686, 0.5076, 0.8667, 0.5727,\n          0.2741, 0.7438, 0.9081, 0.8011, 0.7129],\n         [0.5705, 0.7798, 0.7157, 0.3613, 0.2133, 0.4024, 0.6230, 0.0210, 0.5773,\n          0.3010, 0.2232, 0.4136, 0.2083, 0.5040, 0.5319, 0.6522, 0.8221, 0.9070,\n          0.0128, 0.9654, 0.6763, 0.2087, 0.2626, 0.9377, 0.3642, 0.9259, 0.4050,\n          0.5338, 0.5972, 0.1409, 0.3650, 0.7800, 0.0093, 0.7347, 0.7208, 0.1220,\n          0.2791, 0.8931, 0.9147, 0.7367, 0.9074, 0.1298, 0.5057, 0.1735, 0.1231,\n          0.7259, 0.0436, 0.2038, 0.2860, 0.4938]]), TensorCategory(0)), (tensor([[9.8426e-01, 4.0896e-02, 4.2489e-01, 5.5480e-01, 7.9970e-01, 1.7536e-01,\n          3.6647e-01, 1.6799e-01, 5.4765e-02, 7.6352e-01, 1.0350e-01, 6.5167e-01,\n          2.2324e-01, 7.8269e-01, 4.9927e-01, 8.7751e-01, 5.1827e-02, 6.0372e-01,\n          7.4215e-01, 6.9607e-01, 4.1980e-02, 1.4755e-01, 7.7154e-01, 4.5243e-01,\n          8.1157e-01, 9.2817e-01, 5.4651e-01, 1.1075e-01, 6.1413e-02, 4.3866e-01,\n          6.8967e-01, 9.8644e-01, 8.7735e-01, 5.9756e-01, 4.9182e-01, 4.1799e-01,\n          4.4504e-01, 8.3909e-01, 8.1771e-01, 5.7236e-01, 8.1933e-01, 1.4244e-01,\n          8.8749e-01, 4.8428e-01, 6.6693e-01, 6.5428e-01, 2.2453e-01, 2.1881e-01,\n          5.8674e-01, 3.2855e-01],\n         [1.5082e-01, 8.7071e-02, 2.9216e-01, 8.9446e-01, 9.8835e-01, 3.0215e-01,\n          7.5631e-01, 8.0538e-04, 6.9137e-01, 6.5176e-01, 6.5240e-01, 7.8566e-01,\n          1.6496e-01, 9.8343e-01, 5.7149e-01, 9.3864e-02, 6.6251e-01, 7.3830e-01,\n          1.5431e-02, 8.3004e-01, 6.9070e-01, 8.4142e-02, 4.9070e-01, 7.7335e-01,\n          7.2788e-01, 3.8151e-02, 4.3146e-01, 7.5496e-01, 2.4311e-01, 9.2434e-01,\n          1.2407e-01, 1.8758e-01, 6.2804e-01, 7.3331e-02, 6.8652e-01, 7.4185e-01,\n          1.5750e-01, 6.8361e-01, 7.7493e-01, 4.0742e-01, 2.5363e-01, 6.8577e-01,\n          2.5725e-01, 5.5336e-01, 9.0071e-01, 9.5327e-01, 5.1443e-01, 3.7593e-01,\n          9.4973e-01, 1.5504e-01],\n         [7.2840e-01, 9.0539e-01, 2.5714e-01, 7.6755e-01, 1.5358e-01, 8.9368e-01,\n          6.0310e-01, 6.2938e-01, 3.2100e-01, 3.4044e-01, 7.0073e-01, 9.7341e-02,\n          4.9001e-01, 8.0973e-01, 3.9423e-01, 8.8390e-01, 8.9078e-01, 7.4117e-01,\n          1.2791e-01, 4.1650e-02, 2.4217e-01, 1.5018e-02, 9.6620e-01, 2.1216e-01,\n          1.5094e-01, 9.9024e-01, 8.2652e-01, 5.7903e-01, 2.9047e-01, 6.8799e-01,\n          4.9359e-01, 9.7867e-01, 1.2382e-01, 3.5207e-01, 3.2511e-01, 8.6518e-01,\n          8.2106e-01, 3.7364e-01, 1.1405e-01, 2.7247e-01, 2.1474e-01, 6.9869e-03,\n          1.4337e-01, 6.1940e-01, 1.9140e-01, 4.0980e-01, 8.1098e-02, 5.3118e-01,\n          6.8099e-02, 4.6354e-01],\n         [7.3786e-01, 1.1147e-01, 9.8793e-01, 4.9060e-02, 8.3370e-01, 4.2734e-01,\n          4.0550e-01, 8.4980e-02, 2.7922e-01, 7.1742e-01, 6.4203e-01, 8.1815e-01,\n          3.9954e-01, 6.5924e-01, 9.0273e-01, 9.1816e-01, 8.0630e-01, 1.5670e-01,\n          6.9099e-01, 1.7742e-01, 3.8604e-01, 8.4728e-01, 8.2444e-01, 2.1348e-01,\n          6.1390e-01, 2.4343e-01, 8.9388e-01, 6.8131e-01, 6.3700e-01, 8.4972e-01,\n          5.9960e-01, 7.9697e-01, 3.3596e-01, 4.9152e-01, 2.6614e-01, 4.8823e-01,\n          8.9387e-01, 1.3711e-01, 7.4661e-01, 2.4883e-01, 4.1860e-01, 9.9569e-01,\n          6.4248e-01, 8.8646e-01, 7.7250e-01, 8.9770e-01, 7.3839e-01, 9.9637e-01,\n          4.5245e-01, 1.1688e-01],\n         [5.0327e-01, 5.3937e-01, 2.4789e-01, 1.0449e-01, 8.7169e-01, 1.9951e-01,\n          4.1896e-01, 2.2139e-01, 1.9181e-01, 4.0825e-01, 3.8316e-01, 1.7797e-01,\n          8.2273e-01, 6.3823e-02, 7.4156e-01, 7.5078e-01, 2.1177e-01, 5.2285e-01,\n          2.4382e-01, 9.2772e-01, 1.4412e-01, 7.3080e-01, 6.9379e-02, 8.9140e-01,\n          2.7038e-01, 6.9310e-01, 5.7022e-01, 7.2221e-02, 6.7027e-01, 1.4056e-01,\n          3.2253e-01, 5.7898e-01, 9.4653e-01, 7.4235e-01, 8.6222e-01, 2.6138e-01,\n          2.9371e-01, 5.8739e-01, 7.9431e-01, 5.0600e-01, 2.4492e-01, 6.9908e-01,\n          8.8289e-01, 3.4148e-01, 1.9476e-01, 4.1950e-01, 4.7843e-01, 1.4356e-01,\n          8.6368e-01, 5.5124e-02]]), TensorCategory(2)), (tensor([[4.2078e-01, 8.4436e-01, 8.4991e-01, 7.2039e-01, 8.6587e-01, 9.7409e-01,\n          1.0021e-01, 5.3885e-01, 1.2742e-01, 5.3783e-01, 1.7354e-01, 1.9545e-01,\n          7.1860e-01, 6.5367e-01, 6.6074e-01, 6.3119e-01, 8.0885e-01, 7.6008e-01,\n          9.4299e-01, 4.2447e-01, 2.2728e-02, 9.6375e-01, 6.3255e-01, 2.7618e-01,\n          7.3783e-01, 3.6072e-02, 2.5500e-01, 4.4271e-01, 1.5621e-01, 4.4304e-01,\n          6.9071e-01, 1.9256e-01, 5.9220e-02, 3.7904e-01, 6.2749e-01, 3.5100e-01,\n          3.9677e-01, 2.2779e-01, 9.9571e-01, 8.8942e-01, 2.6929e-01, 4.4418e-01,\n          5.2563e-02, 1.5822e-01, 3.9494e-01, 8.4724e-01, 3.2942e-01, 6.7909e-01,\n          1.3318e-01, 4.3287e-01],\n         [4.0960e-01, 9.1125e-02, 8.9804e-01, 9.6697e-01, 7.2394e-01, 3.1447e-01,\n          8.1913e-01, 1.4620e-01, 5.6588e-01, 8.8585e-01, 3.4570e-01, 1.4660e-01,\n          7.3615e-01, 4.9046e-01, 8.4840e-01, 2.0072e-01, 6.7407e-01, 3.7839e-01,\n          4.0673e-02, 5.1385e-01, 5.8222e-01, 6.5698e-02, 7.7558e-01, 7.4556e-01,\n          1.9802e-01, 3.6410e-01, 9.4787e-01, 3.1971e-01, 3.9221e-01, 3.2399e-01,\n          4.7725e-01, 6.2164e-01, 8.6453e-01, 8.7272e-01, 9.1239e-01, 1.1813e-01,\n          8.7800e-01, 9.6777e-01, 1.2170e-01, 3.2150e-01, 2.5000e-01, 9.1324e-01,\n          3.9465e-01, 8.0230e-01, 3.1587e-01, 1.0746e-01, 7.5120e-01, 6.4084e-03,\n          2.6814e-01, 1.4244e-01],\n         [9.9278e-01, 7.1657e-02, 5.0520e-01, 8.2372e-01, 1.7661e-01, 7.4843e-01,\n          3.6771e-01, 6.7044e-01, 9.6574e-01, 2.7518e-01, 7.5149e-01, 5.3474e-02,\n          2.8493e-01, 7.7026e-01, 4.1017e-01, 9.6670e-01, 5.3121e-01, 5.9768e-01,\n          8.0887e-01, 1.0537e-01, 4.8575e-01, 1.5730e-01, 6.9927e-01, 8.3855e-01,\n          8.6684e-01, 7.6952e-01, 3.2255e-01, 7.7619e-01, 2.2758e-01, 4.4439e-01,\n          2.4610e-01, 1.4773e-01, 8.4931e-01, 5.7131e-01, 2.8747e-01, 3.3525e-01,\n          7.4394e-01, 6.5714e-01, 5.6648e-01, 5.6678e-01, 5.4872e-01, 1.3900e-02,\n          4.7282e-01, 4.6857e-01, 7.7865e-01, 7.3721e-01, 5.4963e-01, 3.4266e-01,\n          9.5875e-01, 7.3994e-01],\n         [3.0082e-01, 7.0359e-01, 7.5008e-01, 8.8958e-01, 8.4634e-01, 4.1567e-01,\n          6.6332e-02, 8.4631e-01, 5.6930e-01, 9.8128e-01, 3.2888e-01, 6.7737e-01,\n          2.6256e-01, 5.7858e-01, 9.2948e-01, 6.0111e-01, 5.1518e-01, 3.1074e-01,\n          8.2345e-01, 8.7298e-01, 6.4504e-01, 8.6913e-01, 9.2305e-01, 4.8730e-01,\n          2.8695e-01, 3.8859e-01, 4.2686e-01, 4.8101e-02, 4.0669e-01, 5.7959e-01,\n          2.9208e-01, 2.3364e-01, 1.2999e-01, 5.3149e-01, 6.2650e-01, 2.7876e-01,\n          7.9536e-01, 5.4541e-01, 8.5689e-01, 1.7156e-01, 5.2116e-01, 9.4766e-01,\n          5.9086e-04, 1.8357e-01, 9.2545e-01, 7.5974e-02, 7.9920e-01, 4.5913e-01,\n          7.6306e-01, 7.3167e-01],\n         [9.9185e-01, 4.4407e-01, 4.9074e-01, 5.8138e-01, 6.5747e-01, 1.1315e-01,\n          2.3877e-01, 6.4501e-01, 2.4695e-01, 1.7235e-01, 5.5576e-01, 4.7317e-01,\n          8.2310e-01, 1.0897e-02, 8.8816e-01, 6.2369e-01, 1.2462e-01, 6.1794e-01,\n          6.0169e-01, 5.5152e-01, 1.3688e-01, 9.5918e-01, 2.6375e-01, 3.2087e-01,\n          4.1310e-01, 6.4538e-01, 1.3556e-01, 5.2370e-01, 6.0717e-01, 4.0038e-01,\n          8.5478e-01, 5.6591e-03, 3.2538e-01, 6.7265e-01, 2.7452e-01, 8.5312e-01,\n          2.8062e-01, 3.4536e-01, 5.1029e-01, 4.2153e-01, 8.8004e-01, 9.6691e-01,\n          8.3738e-01, 3.2344e-01, 2.2505e-01, 1.8572e-01, 6.9133e-01, 4.2443e-01,\n          3.1948e-01, 7.9635e-01]]), TensorCategory(4)), (tensor([[0.2265, 0.4248, 0.7110, 0.3193, 0.2307, 0.5198, 0.5970, 0.2375, 0.8905,\n          0.5967, 0.1521, 0.9409, 0.2003, 0.0133, 0.6474, 0.1062, 0.3441, 0.5410,\n          0.8147, 0.1696, 0.1058, 0.2725, 0.5173, 0.7727, 0.5221, 0.7437, 0.4250,\n          0.3869, 0.8261, 0.8587, 0.8120, 0.6603, 0.4011, 0.0563, 0.5108, 0.5277,\n          0.0028, 0.5473, 0.7958, 0.5369, 0.9140, 0.8109, 0.9714, 0.3906, 0.4917,\n          0.3164, 0.1932, 0.9072, 0.2566, 0.7359],\n         [0.0103, 0.7055, 0.1808, 0.6364, 0.6366, 0.2505, 0.7886, 0.6557, 0.8534,\n          0.6704, 0.7149, 0.0784, 0.6339, 0.0519, 0.1176, 0.1727, 0.4654, 0.3470,\n          0.3697, 0.2629, 0.2735, 0.0588, 0.5605, 0.9872, 0.6911, 0.1168, 0.3647,\n          0.0273, 0.9599, 0.1538, 0.9561, 0.6866, 0.7611, 0.5524, 0.1269, 0.2248,\n          0.2285, 0.5688, 0.6153, 0.8001, 0.5370, 0.9917, 0.5871, 0.2340, 0.5893,\n          0.2948, 0.1315, 0.0423, 0.1561, 0.6229],\n         [0.8709, 0.8638, 0.5282, 0.0550, 0.3636, 0.7535, 0.1174, 0.2532, 0.1756,\n          0.0094, 0.7941, 0.7930, 0.8191, 0.8423, 0.0871, 0.2595, 0.5008, 0.9461,\n          0.4349, 0.5367, 0.8483, 0.1497, 0.6762, 0.4957, 0.3889, 0.2302, 0.5298,\n          0.9723, 0.3148, 0.1297, 0.8713, 0.3997, 0.9925, 0.0534, 0.8286, 0.2954,\n          0.6363, 0.1220, 0.9102, 0.6186, 0.0539, 0.9123, 0.6347, 0.2950, 0.1865,\n          0.3012, 0.1662, 0.4119, 0.3660, 0.9006],\n         [0.3757, 0.4561, 0.4771, 0.6974, 0.0830, 0.9552, 0.9880, 0.9583, 0.8376,\n          0.3887, 0.0081, 0.6206, 0.5082, 0.3718, 0.1337, 0.8832, 0.3727, 0.6288,\n          0.8382, 0.8066, 0.1814, 0.5107, 0.7658, 0.2622, 0.6517, 0.4187, 0.3373,\n          0.6298, 0.2209, 0.9400, 0.1119, 0.9461, 0.4089, 0.1129, 0.4118, 0.4837,\n          0.1827, 0.1999, 0.2944, 0.2539, 0.5541, 0.1082, 0.1964, 0.7899, 0.9932,\n          0.2500, 0.6462, 0.5945, 0.6859, 0.8691],\n         [0.1090, 0.9351, 0.0946, 0.9678, 0.9555, 0.4231, 0.6023, 0.0852, 0.8535,\n          0.6180, 0.0593, 0.3677, 0.8289, 0.2859, 0.6160, 0.3548, 0.6299, 0.2336,\n          0.9607, 0.0133, 0.0380, 0.5226, 0.6587, 0.8530, 0.8501, 0.0601, 0.7147,\n          0.7773, 0.3480, 0.4861, 0.7832, 0.4533, 0.1104, 0.8838, 0.4946, 0.0787,\n          0.8907, 0.0259, 0.4345, 0.7296, 0.3085, 0.4922, 0.9329, 0.3613, 0.6430,\n          0.9739, 0.1033, 0.9102, 0.9796, 0.8910]]), TensorCategory(8)), (tensor([[0.2371, 0.3636, 0.0402, 0.9681, 0.9791, 0.8475, 0.4615, 0.5108, 0.9595,\n          0.1887, 0.3682, 0.8904, 0.0922, 0.8683, 0.1548, 0.9607, 0.5221, 0.8056,\n          0.2025, 0.9564, 0.6235, 0.8525, 0.3870, 0.5998, 0.8299, 0.9250, 0.8368,\n          0.7207, 0.0532, 0.7226, 0.8612, 0.9216, 0.0232, 0.4970, 0.4289, 0.8106,\n          0.5910, 0.0512, 0.8960, 0.4261, 0.2133, 0.7405, 0.5299, 0.2340, 0.0308,\n          0.8983, 0.4524, 0.4526, 0.3724, 0.5238],\n         [0.5217, 0.7805, 0.7431, 0.6640, 0.3657, 0.9050, 0.4203, 0.5270, 0.9395,\n          0.8307, 0.1349, 0.8943, 0.3538, 0.6144, 0.2371, 0.1381, 0.5620, 0.8469,\n          0.5131, 0.6271, 0.9838, 0.0335, 0.1512, 0.1820, 0.2436, 0.4968, 0.7796,\n          0.9760, 0.1586, 0.2172, 0.9921, 0.1440, 0.1048, 0.9808, 0.4925, 0.7870,\n          0.2611, 0.1207, 0.9967, 0.6187, 0.9708, 0.7294, 0.7270, 0.8610, 0.0875,\n          0.1237, 0.3295, 0.0522, 0.6174, 0.1946],\n         [0.7966, 0.1712, 0.5011, 0.4736, 0.4528, 0.9669, 0.4330, 0.6834, 0.7183,\n          0.1126, 0.4030, 0.6463, 0.1698, 0.5998, 0.8802, 0.2209, 0.8896, 0.5803,\n          0.0384, 0.2115, 0.9887, 0.3436, 0.7290, 0.3841, 0.1040, 0.1560, 0.8901,\n          0.8038, 0.8121, 0.8327, 0.7534, 0.4223, 0.8089, 0.6855, 0.7402, 0.4592,\n          0.0428, 0.4838, 0.4728, 0.3560, 0.6380, 0.0953, 0.5756, 0.4680, 0.3418,\n          0.0480, 0.8442, 0.9452, 0.2936, 0.3882],\n         [0.7399, 0.3757, 0.7562, 0.7415, 0.5638, 0.9223, 0.5944, 0.2582, 0.6109,\n          0.6928, 0.9447, 0.9611, 0.4648, 0.8583, 0.3612, 0.5056, 0.3114, 0.4287,\n          0.7528, 0.9433, 0.1601, 0.0724, 0.4238, 0.5075, 0.9781, 0.4482, 0.6578,\n          0.6514, 0.1168, 0.9843, 0.8645, 0.1636, 0.7445, 0.6859, 0.5068, 0.2200,\n          0.5456, 0.6502, 0.5533, 0.6378, 0.1779, 0.3810, 0.6023, 0.9752, 0.5013,\n          0.5314, 0.3465, 0.7253, 0.8686, 0.1814],\n         [0.0340, 0.8011, 0.4340, 0.0556, 0.3900, 0.0275, 0.7786, 0.1684, 0.4750,\n          0.8527, 0.3493, 0.9580, 0.2624, 0.6101, 0.5715, 0.6892, 0.1642, 0.4493,\n          0.3062, 0.9666, 0.5664, 0.4194, 0.9208, 0.2355, 0.2528, 0.9140, 0.4468,\n          0.6311, 0.8632, 0.6884, 0.6439, 0.3639, 0.2144, 0.7654, 0.8653, 0.2316,\n          0.2980, 0.4245, 0.7624, 0.6712, 0.3097, 0.8275, 0.9040, 0.4959, 0.6690,\n          0.7900, 0.1562, 0.5021, 0.2895, 0.9765]]), TensorCategory(3)), (tensor([[0.8792, 0.1875, 0.4487, 0.9693, 0.8220, 0.8805, 0.7937, 0.5490, 0.0745,\n          0.4061, 0.3997, 0.0916, 0.6876, 0.7548, 0.4887, 0.6923, 0.8841, 0.2597,\n          0.2477, 0.3484, 0.0468, 0.5894, 0.2499, 0.3015, 0.1757, 0.1985, 0.3026,\n          0.6823, 0.5740, 0.3610, 0.4120, 0.7138, 0.0781, 0.9778, 0.7793, 0.8566,\n          0.1194, 0.2584, 0.2724, 0.7233, 0.6995, 0.3210, 0.1985, 0.9746, 0.0281,\n          0.6531, 0.7612, 0.7210, 0.3550, 0.0446],\n         [0.7105, 0.6620, 0.0101, 0.0255, 0.1623, 0.7465, 0.5502, 0.9461, 0.2871,\n          0.6468, 0.3740, 0.7240, 0.9834, 0.9675, 0.3926, 0.3558, 0.4895, 0.9865,\n          0.4696, 0.9649, 0.0384, 0.3839, 0.0520, 0.0971, 0.9739, 0.9436, 0.4117,\n          0.2064, 0.0355, 0.8651, 0.2985, 0.9178, 0.0505, 0.2701, 0.0247, 0.6179,\n          0.8134, 0.6347, 0.6672, 0.4559, 0.3241, 0.2927, 0.8304, 0.0369, 0.3124,\n          0.4150, 0.7525, 0.7622, 0.2466, 0.6722],\n         [0.1952, 0.3496, 0.5455, 0.0427, 0.4036, 0.1756, 0.3774, 0.3185, 0.0520,\n          0.5026, 0.0250, 0.1950, 0.8073, 0.4565, 0.9853, 0.5268, 0.7640, 0.9028,\n          0.1016, 0.2128, 0.2273, 0.6836, 0.8640, 0.5657, 0.0126, 0.7399, 0.4293,\n          0.6642, 0.2315, 0.1580, 0.9292, 0.0731, 0.5680, 0.1417, 0.5876, 0.7765,\n          0.7192, 0.1485, 0.9838, 0.5020, 0.6674, 0.7763, 0.4640, 0.7567, 0.9149,\n          0.9155, 0.5239, 0.6105, 0.0707, 0.0752],\n         [0.6227, 0.6693, 0.7152, 0.8664, 0.1351, 0.0443, 0.6093, 0.8949, 0.8644,\n          0.6432, 0.5276, 0.0724, 0.2787, 0.9495, 0.3806, 0.9284, 0.8291, 0.2150,\n          0.1586, 0.0279, 0.7219, 0.1189, 0.9986, 0.3755, 0.6956, 0.5243, 0.9482,\n          0.1985, 0.3706, 0.8963, 0.1286, 0.6261, 0.4836, 0.2516, 0.0018, 0.0209,\n          0.9252, 0.0758, 0.3638, 0.5548, 0.8610, 0.4928, 0.1520, 0.1310, 0.1612,\n          0.5005, 0.4025, 0.7710, 0.8311, 0.1993],\n         [0.3328, 0.6690, 0.7398, 0.6022, 0.6430, 0.8689, 0.7918, 0.8240, 0.5286,\n          0.5378, 0.4990, 0.0660, 0.7776, 0.1836, 0.3559, 0.7229, 0.5229, 0.6048,\n          0.9464, 0.2019, 0.1255, 0.0021, 0.6118, 0.3043, 0.9934, 0.4835, 0.7896,\n          0.4999, 0.0936, 0.4175, 0.9001, 0.8133, 0.2174, 0.9222, 0.6310, 0.6132,\n          0.0979, 0.4145, 0.0811, 0.0444, 0.6401, 0.0961, 0.6327, 0.3032, 0.7825,\n          0.4131, 0.2534, 0.9429, 0.9294, 0.4339]]), TensorCategory(8))] ...],\n (#66) [(tensor([[0.7768, 0.2279, 0.5594, 0.1785, 0.4371, 0.7003, 0.4575, 0.1157, 0.4229,\n          0.7766, 0.9173, 0.4212, 0.5825, 0.4227, 0.2635, 0.1216, 0.0919, 0.7151,\n          0.3067, 0.8822, 0.1740, 0.6729, 0.3104, 0.2668, 0.9748, 0.2923, 0.6550,\n          0.4904, 0.0718, 0.7364, 0.6322, 0.6216, 0.3371, 0.1959, 0.7373, 0.8462,\n          0.7345, 0.4772, 0.9590, 0.4495, 0.1218, 0.8243, 0.1451, 0.6942, 0.3564,\n          0.3740, 0.5998, 0.0574, 0.7415, 0.5104],\n         [0.7114, 0.9495, 0.1967, 0.5401, 0.1970, 0.2663, 0.5293, 0.5642, 0.5045,\n          0.5948, 0.4801, 0.5387, 0.8902, 0.5276, 0.8672, 0.0270, 0.7144, 0.4578,\n          0.7255, 0.7186, 0.6959, 0.6580, 0.4733, 0.7130, 0.8160, 0.0296, 0.8453,\n          0.5556, 0.5425, 0.9379, 0.2613, 0.8521, 0.0092, 0.9430, 0.5450, 0.4405,\n          0.1918, 0.4453, 0.8192, 0.5094, 0.7053, 0.5314, 0.3520, 0.9840, 0.7620,\n          0.6328, 0.9469, 0.8175, 0.1023, 0.5672],\n         [0.7368, 0.1682, 0.8502, 0.2359, 0.6837, 0.5321, 0.4106, 0.7876, 0.2249,\n          0.5776, 0.3282, 0.8666, 0.6871, 0.6207, 0.4704, 0.4386, 0.1872, 0.4950,\n          0.6997, 0.9210, 0.7414, 0.9441, 0.6070, 0.8254, 0.9545, 0.0659, 0.9459,\n          0.1876, 0.6565, 0.3903, 0.1209, 0.0803, 0.7535, 0.1401, 0.2110, 0.8130,\n          0.0489, 0.3871, 0.3314, 0.0743, 0.0036, 0.5289, 0.1847, 0.3871, 0.9052,\n          0.9872, 0.5879, 0.4872, 0.2588, 0.6939],\n         [0.2456, 0.7926, 0.2280, 0.5083, 0.6654, 0.9044, 0.1709, 0.5007, 0.3275,\n          0.2382, 0.0856, 0.7837, 0.9523, 0.7500, 0.6403, 0.9048, 0.7490, 0.7531,\n          0.4266, 0.3874, 0.8518, 0.5143, 0.7542, 0.8803, 0.7732, 0.3686, 0.3767,\n          0.7478, 0.3263, 0.1148, 0.3031, 0.2404, 0.5633, 0.1642, 0.1816, 0.5026,\n          0.9603, 0.0318, 0.9937, 0.3024, 0.6656, 0.2578, 0.9552, 0.1053, 0.6377,\n          0.2183, 0.8518, 0.9437, 0.7990, 0.0604],\n         [0.7305, 0.2273, 0.7661, 0.9651, 0.5277, 0.4209, 0.2112, 0.7378, 0.6966,\n          0.9434, 0.9811, 0.4531, 0.9434, 0.7187, 0.7186, 0.4744, 0.7828, 0.4990,\n          0.6426, 0.7540, 0.3982, 0.7331, 0.9339, 0.5100, 0.8633, 0.7440, 0.4020,\n          0.9121, 0.9178, 0.9332, 0.7262, 0.7779, 0.2857, 0.8000, 0.4253, 0.0728,\n          0.1784, 0.2613, 0.4471, 0.9359, 0.4382, 0.9324, 0.0834, 0.9751, 0.3562,\n          0.2600, 0.4564, 0.5136, 0.1841, 0.4093]]), TensorCategory(8)), (tensor([[0.1302, 0.0387, 0.6271, 0.3295, 0.6051, 0.0955, 0.0026, 0.0230, 0.2420,\n          0.0062, 0.5515, 0.5129, 0.0980, 0.8743, 0.4197, 0.8433, 0.7032, 0.5171,\n          0.3198, 0.2698, 0.1615, 0.7707, 0.6391, 0.3351, 0.0901, 0.8346, 0.0158,\n          0.9558, 0.6018, 0.9834, 0.8674, 0.8136, 0.5606, 0.2143, 0.6374, 0.4424,\n          0.0828, 0.9718, 0.1281, 0.8941, 0.2091, 0.7500, 0.8509, 0.7891, 0.2631,\n          0.6493, 0.4769, 0.7866, 0.0414, 0.9913],\n         [0.4699, 0.7552, 0.2072, 0.7340, 0.9084, 0.1414, 0.7469, 0.7076, 0.2324,\n          0.9210, 0.5565, 0.5952, 0.7875, 0.1877, 0.1563, 0.0966, 0.6287, 0.8719,\n          0.9486, 0.6714, 0.7792, 0.2345, 0.1232, 0.1220, 0.9678, 0.5015, 0.4807,\n          0.4616, 0.0281, 0.5410, 0.6189, 0.7862, 0.4632, 0.0272, 0.6964, 0.8928,\n          0.0047, 0.5856, 0.9336, 0.4054, 0.4585, 0.4792, 0.8312, 0.9028, 0.1815,\n          0.3685, 0.2249, 0.9165, 0.1974, 0.3223],\n         [0.8664, 0.6392, 0.2796, 0.9301, 0.3415, 0.2811, 0.9252, 0.7420, 0.9986,\n          0.6483, 0.2393, 0.8723, 0.0869, 0.6091, 0.6089, 0.8863, 0.8193, 0.7471,\n          0.5906, 0.6937, 0.0835, 0.4221, 0.0588, 0.9941, 0.8721, 0.5023, 0.4750,\n          0.7112, 0.6332, 0.9713, 0.1381, 0.8244, 0.2748, 0.3747, 0.3059, 0.3925,\n          0.8424, 0.5635, 0.2296, 0.8658, 0.4726, 0.0766, 0.5114, 0.1064, 0.8951,\n          0.4077, 0.0758, 0.4465, 0.1271, 0.8758],\n         [0.4149, 0.7585, 0.0075, 0.0073, 0.6633, 0.8435, 0.9852, 0.8903, 0.4830,\n          0.4133, 0.9027, 0.4680, 0.9097, 0.1325, 0.8626, 0.3964, 0.1869, 0.3941,\n          0.7687, 0.4853, 0.3529, 0.0656, 0.8551, 0.6186, 0.9955, 0.6731, 0.6312,\n          0.4192, 0.9227, 0.4260, 0.2660, 0.5365, 0.6254, 0.5063, 0.6048, 0.4646,\n          0.7762, 0.2100, 0.9775, 0.9900, 0.5278, 0.6398, 0.5259, 0.0240, 0.1528,\n          0.7049, 0.2487, 0.3254, 0.0172, 0.4121],\n         [0.2020, 0.4063, 0.6866, 0.8839, 0.9851, 0.7142, 0.0521, 0.4995, 0.9219,\n          0.9258, 0.4862, 0.6702, 0.6086, 0.2904, 0.3080, 0.1529, 0.5377, 0.2107,\n          0.7143, 0.1243, 0.4364, 0.1507, 0.8943, 0.9548, 0.3953, 0.9190, 0.8217,\n          0.4436, 0.0757, 0.2809, 0.3996, 0.6834, 0.1432, 0.9187, 0.9915, 0.8230,\n          0.1703, 0.3230, 0.0293, 0.5006, 0.6898, 0.5452, 0.7523, 0.8370, 0.1813,\n          0.8176, 0.1070, 0.2723, 0.2964, 0.0020]]), TensorCategory(7)), (tensor([[9.9442e-01, 9.1258e-01, 8.3024e-01, 6.8195e-01, 4.7409e-01, 4.6938e-01,\n          6.9558e-01, 4.9876e-01, 4.6420e-01, 9.9869e-01, 5.5829e-01, 8.1567e-01,\n          4.0929e-01, 7.9767e-01, 4.4508e-02, 1.1521e-02, 4.1273e-01, 2.6357e-01,\n          5.8077e-01, 4.6785e-01, 2.5241e-01, 9.6777e-01, 6.0388e-01, 2.2550e-01,\n          8.0490e-01, 5.8652e-01, 3.8634e-01, 5.1162e-01, 1.4030e-01, 1.4279e-01,\n          4.2432e-01, 1.9535e-01, 4.8812e-02, 3.1104e-01, 9.5810e-02, 6.7128e-01,\n          7.8896e-01, 8.5005e-01, 6.3695e-01, 6.9442e-01, 1.9241e-01, 5.8412e-01,\n          8.2674e-01, 6.3511e-01, 3.9449e-02, 3.4566e-01, 8.5613e-01, 1.2923e-01,\n          7.9645e-01, 7.6753e-01],\n         [9.0722e-01, 1.0241e-01, 7.0997e-01, 8.0305e-02, 1.7449e-01, 4.5107e-01,\n          6.7719e-01, 9.1135e-01, 2.3451e-01, 1.4753e-01, 1.6267e-01, 2.9498e-01,\n          9.2896e-01, 1.5877e-01, 4.3167e-01, 1.8937e-02, 5.9137e-01, 7.4454e-01,\n          1.4183e-01, 1.0941e-01, 6.2625e-01, 2.2055e-01, 5.9980e-02, 9.5824e-01,\n          9.6820e-01, 3.1308e-01, 5.9503e-01, 3.8329e-01, 9.7925e-01, 5.5779e-01,\n          3.0477e-01, 3.5647e-01, 9.9601e-01, 1.7288e-01, 9.6479e-01, 4.5904e-01,\n          5.1864e-01, 8.6790e-01, 8.2837e-01, 1.6044e-03, 9.1944e-01, 9.2135e-01,\n          7.5002e-01, 7.4545e-01, 5.4445e-01, 4.7801e-01, 9.4346e-01, 6.0687e-01,\n          7.4860e-01, 4.1634e-01],\n         [9.4250e-01, 9.2646e-01, 7.8792e-01, 1.6274e-01, 7.9544e-01, 4.6171e-01,\n          9.1439e-01, 9.5531e-01, 8.8066e-01, 9.8166e-01, 4.8074e-03, 1.0858e-01,\n          7.2315e-01, 1.1455e-01, 2.1595e-01, 6.2940e-01, 8.8644e-01, 3.6637e-01,\n          6.0885e-01, 1.6072e-02, 9.0757e-01, 7.7364e-01, 9.7723e-01, 4.0353e-01,\n          2.5067e-01, 4.0406e-01, 4.4859e-01, 3.4106e-01, 6.0164e-01, 2.8940e-01,\n          1.0944e-01, 2.5679e-03, 5.2659e-01, 9.7649e-01, 8.4601e-02, 7.2423e-01,\n          2.2125e-01, 4.3714e-01, 7.6835e-01, 3.1717e-01, 8.5392e-02, 7.7014e-01,\n          8.4650e-01, 7.2226e-01, 4.0769e-01, 2.0124e-01, 9.4502e-01, 2.7276e-01,\n          6.9635e-01, 7.4651e-01],\n         [5.1004e-01, 1.2604e-01, 6.8343e-01, 2.7992e-01, 2.4013e-01, 8.4783e-01,\n          7.3954e-01, 4.0698e-01, 2.8815e-01, 3.7156e-01, 2.1754e-01, 1.9378e-01,\n          7.0695e-01, 8.5664e-01, 7.2082e-01, 8.6994e-01, 1.4807e-01, 2.4569e-02,\n          3.3851e-02, 1.6478e-01, 7.0039e-01, 7.4465e-01, 4.5747e-01, 6.3411e-01,\n          8.3893e-01, 7.3165e-01, 6.7083e-01, 1.5298e-01, 1.0667e-01, 4.1298e-01,\n          7.2788e-01, 8.1516e-01, 4.1057e-01, 9.4760e-01, 3.3291e-01, 3.2571e-01,\n          4.2830e-03, 9.7724e-02, 4.4674e-01, 1.7263e-01, 8.2825e-01, 3.4730e-01,\n          2.2376e-03, 4.8066e-01, 6.2925e-01, 9.9233e-01, 8.8468e-01, 6.5160e-01,\n          4.1173e-01, 4.5424e-01],\n         [4.8451e-02, 7.2296e-02, 9.1564e-01, 4.4140e-01, 4.2789e-01, 4.9949e-01,\n          3.0273e-02, 8.7547e-02, 7.8750e-01, 9.8292e-01, 4.1856e-01, 9.0620e-02,\n          7.6029e-01, 4.9792e-01, 6.4694e-01, 5.3054e-01, 1.0303e-03, 3.8633e-01,\n          3.7787e-01, 6.1128e-02, 3.0378e-01, 4.6351e-01, 9.2286e-01, 2.9694e-01,\n          3.8030e-02, 2.1616e-01, 7.6175e-04, 6.5592e-01, 5.9989e-02, 1.7436e-01,\n          8.5242e-01, 1.5391e-01, 2.0494e-01, 5.7161e-01, 8.7865e-01, 9.9827e-01,\n          5.4983e-01, 3.3971e-01, 7.4838e-01, 3.3370e-01, 4.3125e-01, 9.1244e-01,\n          5.9100e-01, 8.0456e-01, 3.5734e-01, 9.0047e-01, 5.8696e-01, 1.6422e-01,\n          8.3527e-01, 9.7004e-01]]), TensorCategory(1)), (tensor([[0.5188, 0.4444, 0.8719, 0.8861, 0.3034, 0.3080, 0.4400, 0.8333, 0.6596,\n          0.0250, 0.3337, 0.8235, 0.2522, 0.9502, 0.0235, 0.8493, 0.4729, 0.0281,\n          0.1803, 0.1476, 0.8424, 0.3391, 0.4515, 0.9699, 0.5327, 0.4501, 0.9205,\n          0.3209, 0.6414, 0.6983, 0.6459, 0.5042, 0.2304, 0.4194, 0.7844, 0.5986,\n          0.5509, 0.9923, 0.3691, 0.2368, 0.6284, 0.8892, 0.5269, 1.0000, 0.0809,\n          0.7210, 0.2947, 0.5151, 0.5171, 0.6574],\n         [0.7851, 0.6592, 0.4373, 0.1130, 0.8619, 0.4585, 0.0278, 0.9226, 0.4467,\n          0.3625, 0.8110, 0.6043, 0.2478, 0.3539, 0.0537, 0.0875, 0.6473, 0.8665,\n          0.9318, 0.6326, 0.1183, 0.0984, 0.6438, 0.7714, 0.1842, 0.1808, 0.7275,\n          0.2192, 0.0656, 0.9632, 0.8257, 0.5843, 0.7946, 0.7420, 0.4614, 0.4101,\n          0.1701, 0.0171, 0.4355, 0.0405, 0.1982, 0.6024, 0.7983, 0.2878, 0.3475,\n          0.6526, 0.1767, 0.7391, 0.6861, 0.1257],\n         [0.7953, 0.8167, 0.3037, 0.7436, 0.2562, 0.2598, 0.6133, 0.4701, 0.8991,\n          0.4100, 0.0317, 0.6201, 0.2291, 0.5295, 0.2848, 0.2099, 0.4492, 0.3405,\n          0.3826, 0.7965, 0.8717, 0.7568, 0.6121, 0.8442, 0.5673, 0.9826, 0.0060,\n          0.1787, 0.1745, 0.9564, 0.9151, 0.4832, 0.5001, 0.2834, 0.5427, 0.1579,\n          0.1349, 0.6087, 0.5576, 0.7423, 0.0901, 0.1875, 0.2407, 0.7222, 0.8230,\n          0.3966, 0.5988, 0.8855, 0.3517, 0.4172],\n         [0.4080, 0.0589, 0.9247, 0.7007, 0.9152, 0.3523, 0.2054, 0.4164, 0.8248,\n          0.9046, 0.5302, 0.4537, 0.5423, 0.8636, 0.3103, 0.0501, 0.1842, 0.9362,\n          0.0998, 0.0126, 0.1876, 0.5144, 0.8172, 0.9726, 0.0439, 0.2023, 0.1607,\n          0.9596, 0.1274, 0.9931, 0.1748, 0.3633, 0.6077, 0.8620, 0.2720, 0.0839,\n          0.6261, 0.9665, 0.0471, 0.8320, 0.5497, 0.1166, 0.7195, 0.4596, 0.7323,\n          0.1316, 0.9447, 0.4625, 0.7126, 0.5178],\n         [0.1744, 0.2934, 0.4534, 0.6393, 0.5381, 0.8729, 0.4580, 0.4879, 0.5421,\n          0.8081, 0.0942, 0.6334, 0.1068, 0.6410, 0.9788, 0.0760, 0.5535, 0.0710,\n          0.2003, 0.3920, 0.7180, 0.1655, 0.3644, 0.0471, 0.3040, 0.8595, 0.5661,\n          0.1195, 0.9805, 0.4264, 0.4866, 0.4692, 0.5269, 0.3466, 0.0785, 0.0720,\n          0.7785, 0.1139, 0.3497, 0.8492, 0.1016, 0.9520, 0.8942, 0.7828, 0.4662,\n          0.2189, 0.4547, 0.0556, 0.3140, 0.2558]]), TensorCategory(6)), (tensor([[0.4375, 0.5060, 0.9436, 0.7944, 0.2098, 0.1587, 0.0389, 0.3171, 0.5269,\n          0.0057, 0.6801, 0.1144, 0.5808, 0.6486, 0.3270, 0.4303, 0.3634, 0.2566,\n          0.9380, 0.7953, 0.5519, 0.9155, 0.4611, 0.8095, 0.6973, 0.7711, 0.5466,\n          0.5567, 0.0238, 0.9549, 0.1872, 0.7874, 0.6807, 0.0794, 0.9095, 0.9629,\n          0.8104, 0.0351, 0.6015, 0.8351, 0.1375, 0.4231, 0.1932, 0.7721, 0.2899,\n          0.4468, 0.8989, 0.5748, 0.4413, 0.3238],\n         [0.3945, 0.1032, 0.4038, 0.6674, 0.7351, 0.9474, 0.1558, 0.4643, 0.1422,\n          0.5336, 0.7809, 0.0304, 0.5297, 0.4328, 0.1599, 0.4604, 0.2971, 0.2421,\n          0.3786, 0.4424, 0.0306, 0.5918, 0.8792, 0.9238, 0.3091, 0.1268, 0.5286,\n          0.1973, 0.7985, 0.1159, 0.5947, 0.4173, 0.6007, 0.3515, 0.4304, 0.5235,\n          0.3815, 0.8057, 0.1187, 0.4661, 0.6513, 0.8364, 0.7564, 0.1577, 0.3690,\n          0.2563, 0.3468, 0.1409, 0.3053, 0.2611],\n         [0.3532, 0.6543, 0.4735, 0.8039, 0.0869, 0.3028, 0.6686, 0.8318, 0.7469,\n          0.9620, 0.3614, 0.5726, 0.1459, 0.6466, 0.9229, 0.4959, 0.3884, 0.1144,\n          0.2002, 0.5316, 0.1789, 0.2430, 0.6436, 0.3191, 0.6242, 0.6608, 0.4448,\n          0.2819, 0.8275, 0.2010, 0.5356, 0.5038, 0.0760, 0.0154, 0.4819, 0.7628,\n          0.2663, 0.0478, 0.6180, 0.1053, 0.8622, 0.4936, 0.4398, 0.0319, 0.9116,\n          0.2856, 0.7227, 0.8125, 0.7715, 0.6184],\n         [0.9736, 0.4726, 0.9244, 0.0089, 0.9198, 0.1085, 0.2871, 0.4254, 0.6270,\n          0.7155, 0.0111, 0.2134, 0.4274, 0.1487, 0.4726, 0.9203, 0.3333, 0.2990,\n          0.4971, 0.0776, 0.6872, 0.6345, 0.7223, 0.8905, 0.9566, 0.0913, 0.6549,\n          0.6814, 0.9479, 0.9388, 0.4262, 0.3037, 0.2156, 0.3087, 0.1561, 0.8704,\n          0.9635, 0.2281, 0.2750, 0.0723, 0.4762, 0.9118, 0.9787, 0.0240, 0.6124,\n          0.2401, 0.2139, 0.0046, 0.0256, 0.5147],\n         [0.9663, 0.1789, 0.0232, 0.1816, 0.5102, 0.0859, 0.6194, 0.7151, 0.7790,\n          0.3718, 0.4207, 0.7175, 0.8940, 0.2450, 0.9146, 0.6051, 0.0694, 0.9602,\n          0.7315, 0.9499, 0.9824, 0.3165, 0.3838, 0.0134, 0.4139, 0.7291, 0.0101,\n          0.9135, 0.6058, 0.9813, 0.3413, 0.2258, 0.3742, 0.4919, 0.2875, 0.5869,\n          0.8780, 0.0027, 0.0766, 0.9945, 0.4522, 0.1929, 0.3893, 0.1577, 0.3835,\n          0.9951, 0.3454, 0.0631, 0.6555, 0.3168]]), TensorCategory(4)), (tensor([[7.0753e-01, 6.6565e-02, 2.7259e-01, 8.6959e-01, 9.4964e-01, 1.7832e-01,\n          7.9345e-02, 4.2268e-01, 1.4248e-01, 5.5139e-01, 6.8478e-02, 4.2986e-01,\n          5.4080e-01, 2.6450e-01, 7.0387e-02, 6.1478e-01, 5.9135e-01, 1.4186e-01,\n          2.7062e-02, 9.1523e-01, 8.1440e-01, 7.3290e-01, 4.3423e-01, 1.0432e-01,\n          7.9550e-01, 8.5419e-01, 8.9932e-02, 3.0207e-01, 6.9025e-01, 2.4932e-02,\n          6.4050e-01, 6.5681e-01, 1.9405e-01, 9.0854e-01, 1.6348e-01, 4.4901e-01,\n          2.7706e-01, 6.1457e-01, 8.3136e-01, 5.0581e-01, 7.4131e-01, 2.3957e-01,\n          8.8159e-03, 8.6094e-01, 3.5500e-02, 4.7322e-01, 8.1346e-01, 5.4368e-01,\n          5.8818e-01, 8.5551e-02],\n         [2.0299e-01, 8.1295e-01, 2.8343e-01, 5.5966e-01, 8.6553e-01, 9.3441e-02,\n          8.6703e-01, 6.9616e-01, 8.5767e-01, 6.0401e-01, 7.4096e-02, 9.9627e-01,\n          7.5757e-01, 4.2571e-01, 4.7017e-01, 6.6609e-01, 3.1445e-01, 5.8626e-01,\n          8.6522e-01, 1.8728e-01, 7.8887e-01, 4.0116e-01, 2.5752e-01, 3.8022e-01,\n          4.7817e-02, 6.4245e-01, 9.3492e-01, 6.5443e-01, 1.9870e-02, 6.2006e-01,\n          1.7892e-01, 4.0642e-01, 2.7051e-01, 4.6651e-01, 1.2596e-01, 5.5537e-01,\n          4.2475e-01, 9.4261e-01, 8.6800e-01, 4.8562e-01, 8.0815e-01, 9.1041e-01,\n          3.8306e-01, 2.3306e-01, 7.0091e-01, 1.8354e-01, 3.4668e-01, 2.0875e-02,\n          4.7099e-01, 1.7786e-01],\n         [9.9065e-01, 9.2325e-01, 4.2584e-01, 3.1720e-01, 6.9935e-01, 2.6444e-02,\n          3.2390e-01, 3.7830e-01, 9.3029e-01, 9.5525e-01, 9.1555e-01, 8.3328e-02,\n          3.7359e-01, 9.9300e-01, 2.4976e-01, 5.5294e-01, 5.1942e-01, 7.4493e-01,\n          8.1771e-01, 8.3651e-01, 3.2912e-01, 4.0206e-01, 6.8331e-02, 2.1775e-01,\n          4.7317e-01, 2.7593e-01, 9.3016e-01, 4.7225e-01, 8.0455e-01, 1.7876e-01,\n          5.6091e-02, 4.7908e-01, 1.2424e-01, 7.0438e-01, 4.7920e-01, 3.6558e-01,\n          2.5927e-01, 5.0331e-01, 2.8272e-01, 6.3105e-02, 2.2981e-01, 1.4153e-01,\n          5.1028e-01, 1.7042e-01, 6.8343e-01, 8.6077e-01, 7.0666e-01, 6.7065e-02,\n          3.3586e-01, 9.7373e-01],\n         [8.2938e-01, 4.6810e-02, 8.8545e-01, 8.0712e-01, 8.9491e-01, 4.6834e-01,\n          9.1967e-01, 2.2492e-01, 6.7759e-01, 2.6604e-01, 5.1421e-01, 8.9890e-01,\n          5.0639e-01, 1.5240e-01, 9.0376e-01, 8.2392e-01, 1.2581e-01, 3.5875e-01,\n          5.2758e-01, 7.2088e-01, 4.5223e-01, 5.8184e-01, 7.1885e-01, 4.9444e-01,\n          4.6045e-01, 9.0012e-01, 5.3574e-01, 7.5545e-01, 6.1264e-01, 4.9443e-01,\n          2.1135e-03, 7.9895e-01, 3.1357e-01, 5.8489e-01, 1.6636e-01, 9.2935e-01,\n          1.1125e-01, 4.6904e-01, 3.8298e-01, 5.4630e-03, 2.0540e-03, 3.8999e-01,\n          1.6872e-02, 7.4637e-01, 4.3518e-01, 7.5879e-01, 1.1524e-01, 4.5804e-01,\n          2.0980e-01, 6.8740e-01],\n         [8.3015e-01, 9.9285e-01, 7.9146e-01, 4.3164e-01, 6.3529e-02, 8.6971e-01,\n          8.3664e-01, 3.3314e-01, 6.2463e-01, 6.3765e-01, 7.8048e-01, 3.9163e-01,\n          8.2156e-02, 2.3214e-01, 1.9170e-02, 1.6751e-01, 4.5365e-01, 7.5906e-01,\n          2.1486e-01, 9.8973e-01, 4.3374e-01, 1.3558e-01, 3.3187e-01, 5.8279e-01,\n          8.2734e-01, 7.5967e-01, 2.3067e-04, 2.3331e-01, 7.5008e-01, 9.4165e-01,\n          5.5677e-01, 4.1397e-01, 8.5770e-02, 4.9837e-01, 9.7930e-01, 5.0206e-01,\n          1.2979e-01, 1.1137e-01, 6.0309e-01, 5.2018e-01, 5.7332e-01, 2.9102e-01,\n          3.0237e-01, 9.0012e-01, 4.7570e-01, 2.6911e-01, 9.3638e-01, 6.4194e-01,\n          4.4381e-01, 9.2375e-01]]), TensorCategory(7)), (tensor([[0.7477, 0.4004, 0.8609, 0.4392, 0.3598, 0.0576, 0.7863, 0.0586, 0.8082,\n          0.2536, 0.9831, 0.2928, 0.3872, 0.7640, 0.0209, 0.9541, 0.3964, 0.5822,\n          0.2627, 0.6099, 0.2683, 0.3089, 0.3350, 0.6941, 0.5091, 0.5873, 0.4684,\n          0.9493, 0.6013, 0.5288, 0.2333, 0.8531, 0.7482, 0.8399, 0.9041, 0.6545,\n          0.6021, 0.7887, 0.3152, 0.1509, 0.2536, 0.8216, 0.3395, 0.4228, 0.6137,\n          0.9481, 0.0229, 0.0011, 0.3828, 0.5495],\n         [0.2318, 0.2032, 0.9671, 0.7083, 0.1138, 0.4040, 0.0285, 0.7815, 0.8582,\n          0.8766, 0.3090, 0.7016, 0.0656, 0.0761, 0.3759, 0.5951, 0.5780, 0.9155,\n          0.2107, 0.1888, 0.3178, 0.5505, 0.3996, 0.0748, 0.2645, 0.6684, 0.5034,\n          0.2384, 0.2465, 0.8461, 0.6279, 0.5542, 0.6636, 0.1386, 0.4724, 0.6755,\n          0.5996, 0.5990, 0.2840, 0.3836, 0.1673, 0.9277, 0.2892, 0.3058, 0.9377,\n          0.4187, 0.9885, 0.0945, 0.7865, 0.9616],\n         [0.4466, 0.9118, 0.6225, 0.0851, 0.3657, 0.8139, 0.0322, 0.7437, 0.7151,\n          0.1330, 0.8266, 0.6103, 0.1468, 0.7245, 0.2918, 0.8115, 0.7454, 0.5910,\n          0.8102, 0.5960, 0.1207, 0.6009, 0.7926, 0.5317, 0.4450, 0.2082, 0.4175,\n          0.9625, 0.6382, 0.6154, 0.0524, 0.0716, 0.8611, 0.8990, 0.2779, 0.7082,\n          0.8615, 0.1181, 0.2648, 0.9881, 0.1237, 0.2290, 0.6101, 0.5672, 0.9868,\n          0.3464, 0.3704, 0.0664, 0.0488, 0.8869],\n         [0.4934, 0.5554, 0.4346, 0.4410, 0.7155, 0.9238, 0.9682, 0.3024, 0.2411,\n          0.0236, 0.6659, 0.2216, 0.3277, 0.9491, 0.3830, 0.8760, 0.1489, 0.4104,\n          0.5038, 0.8337, 0.9482, 0.9711, 0.5544, 0.6948, 0.2323, 0.4196, 0.6169,\n          0.6157, 0.2252, 0.4264, 0.0556, 0.4911, 0.3942, 0.1701, 0.5911, 0.3092,\n          0.2039, 0.0352, 0.3542, 0.4728, 0.7213, 0.0650, 0.2031, 0.3912, 0.7305,\n          0.9514, 0.3295, 0.4806, 0.6578, 0.6533],\n         [0.4462, 0.6818, 0.2536, 0.3853, 0.4010, 0.4069, 0.2515, 0.3174, 0.3565,\n          0.5253, 0.5204, 0.4491, 0.8907, 0.5105, 0.8851, 0.2064, 0.4915, 0.7612,\n          0.7275, 0.7062, 0.9728, 0.8903, 0.5066, 0.3407, 0.8499, 0.7035, 0.2514,\n          0.4626, 0.1697, 0.8821, 0.6669, 0.5451, 0.5662, 0.4207, 0.7049, 0.7643,\n          0.7265, 0.8747, 0.8690, 0.7089, 0.1586, 0.0510, 0.6681, 0.3320, 0.1990,\n          0.7276, 0.9130, 0.8022, 0.0785, 0.9618]]), TensorCategory(9)), (tensor([[0.0598, 0.7410, 0.5064, 0.1300, 0.5934, 0.0709, 0.3837, 0.1643, 0.2885,\n          0.4480, 0.6921, 0.0108, 0.8173, 0.2114, 0.3325, 0.7798, 0.6734, 0.1982,\n          0.9869, 0.6599, 0.4987, 0.4471, 0.3129, 0.3072, 0.4146, 0.4200, 0.6010,\n          0.7824, 0.8844, 0.8967, 0.4692, 0.9206, 0.1567, 0.0611, 0.9173, 0.6161,\n          0.3305, 0.8571, 0.7803, 0.0162, 0.4468, 0.5039, 0.6928, 0.4382, 0.1437,\n          0.9621, 0.3592, 0.4344, 0.2697, 0.7557],\n         [0.2687, 0.4605, 0.8157, 0.4893, 0.5088, 0.3227, 0.6686, 0.1095, 0.2554,\n          0.0863, 0.5447, 0.7081, 0.3035, 0.2867, 0.8639, 0.9084, 0.6710, 0.8717,\n          0.0158, 0.0426, 0.5422, 0.8078, 0.4884, 0.2117, 0.1106, 0.7727, 0.2222,\n          0.3349, 0.1101, 0.5680, 0.9149, 0.6216, 0.6357, 0.0026, 0.9153, 0.9470,\n          0.4183, 0.1231, 0.4665, 0.5824, 0.7369, 0.7231, 0.1040, 0.7567, 0.6604,\n          0.9455, 0.1185, 0.8491, 0.4729, 0.6965],\n         [0.9448, 0.6097, 0.9414, 0.2225, 0.7391, 0.4297, 0.1077, 0.5199, 0.3061,\n          0.7554, 0.2684, 0.5053, 0.4567, 0.3925, 0.7728, 0.4551, 0.7607, 0.7749,\n          0.7514, 0.0838, 0.5026, 0.4506, 0.7306, 0.0456, 0.8578, 0.8027, 0.8394,\n          0.3496, 0.2281, 0.7597, 0.3843, 0.4185, 0.3326, 0.3193, 0.6542, 0.7737,\n          0.1385, 0.1541, 0.9987, 0.1605, 0.9107, 0.6483, 0.5432, 0.9961, 0.7308,\n          0.5123, 0.8050, 0.9284, 0.7941, 0.3998],\n         [0.7610, 0.0403, 0.1646, 0.4794, 0.5820, 0.7712, 0.4357, 0.9220, 0.2591,\n          0.2099, 0.0920, 0.1134, 0.6894, 0.4423, 0.4006, 0.6947, 0.3861, 0.6095,\n          0.7780, 0.7642, 0.5723, 0.8779, 0.7281, 0.0045, 0.8008, 0.5222, 0.6603,\n          0.0884, 0.9700, 0.9243, 0.4575, 0.6268, 0.3528, 0.3056, 0.3101, 0.2788,\n          0.3636, 0.5441, 0.9874, 0.4612, 0.6875, 0.0141, 0.5647, 0.5955, 0.5060,\n          0.9393, 0.8465, 0.1157, 0.3945, 0.2420],\n         [0.8699, 0.8466, 0.6678, 0.4210, 0.9773, 0.1493, 0.0111, 0.4955, 0.7893,\n          0.0633, 0.8221, 0.4285, 0.0328, 0.8092, 0.1947, 0.6572, 0.7573, 0.4966,\n          0.5992, 0.9811, 0.9143, 0.9246, 0.9577, 0.3935, 0.9814, 0.7080, 0.8245,\n          0.5285, 0.0265, 0.4377, 0.0192, 0.1558, 0.9858, 0.2427, 0.1008, 0.0709,\n          0.7007, 0.5403, 0.9657, 0.5094, 0.6465, 0.0479, 0.1355, 0.7531, 0.3977,\n          0.2916, 0.8208, 0.7312, 0.0685, 0.0046]]), TensorCategory(4)), (tensor([[6.8914e-01, 9.3908e-01, 4.3177e-01, 8.7119e-01, 8.6794e-01, 9.3316e-01,\n          4.3979e-01, 7.6185e-01, 1.8001e-01, 2.8617e-01, 1.0436e-01, 9.3229e-01,\n          2.5468e-01, 6.7232e-01, 3.9074e-01, 6.3900e-01, 7.8602e-01, 5.0705e-01,\n          3.4943e-01, 6.8712e-01, 1.1207e-02, 4.0225e-02, 6.0117e-01, 9.4724e-01,\n          8.9822e-01, 1.2923e-01, 9.5612e-01, 7.4602e-01, 5.2172e-01, 2.7987e-01,\n          7.8924e-01, 8.8464e-01, 2.1968e-01, 6.6187e-01, 5.4810e-01, 7.8493e-01,\n          4.5489e-01, 8.0200e-02, 2.0796e-01, 3.5177e-01, 1.6560e-01, 6.4677e-01,\n          4.7704e-01, 9.3187e-01, 5.7241e-02, 8.8113e-01, 4.6961e-01, 6.3332e-01,\n          8.3603e-01, 6.6678e-01],\n         [4.5456e-01, 3.8277e-01, 6.8924e-02, 7.6988e-01, 6.8215e-01, 3.5773e-01,\n          9.3774e-01, 5.9611e-01, 7.2010e-01, 2.8038e-01, 8.0996e-01, 2.4495e-01,\n          1.6509e-02, 3.2715e-01, 9.5279e-01, 4.9217e-01, 3.3442e-01, 2.7155e-01,\n          5.7741e-01, 5.5255e-02, 7.5059e-01, 9.1093e-03, 9.9785e-01, 9.9284e-01,\n          3.3402e-01, 7.8176e-01, 7.2541e-01, 2.3536e-01, 8.9746e-01, 5.3296e-01,\n          7.2685e-01, 4.5166e-02, 6.0518e-01, 7.8464e-01, 5.6951e-01, 3.4313e-01,\n          3.9659e-01, 1.6538e-01, 4.0351e-03, 4.4843e-01, 7.4794e-01, 7.4248e-01,\n          5.7223e-01, 2.6065e-01, 4.3749e-01, 7.7158e-01, 5.3328e-01, 7.4197e-01,\n          2.4791e-01, 8.8968e-01],\n         [5.7514e-02, 4.5529e-01, 7.4933e-01, 8.9464e-01, 7.0830e-01, 6.0761e-01,\n          7.9463e-01, 2.3982e-01, 8.9373e-01, 5.7977e-01, 1.0409e-01, 4.4510e-01,\n          4.2887e-01, 6.2240e-01, 6.8246e-01, 2.8884e-01, 1.0002e-01, 1.0052e-01,\n          9.6103e-01, 4.3763e-01, 4.1671e-01, 4.9054e-02, 9.6944e-01, 3.8387e-01,\n          3.5557e-01, 9.1661e-02, 6.1842e-01, 2.8250e-01, 4.6791e-01, 2.8267e-01,\n          4.8354e-01, 9.9319e-01, 7.2814e-01, 6.0772e-01, 1.1541e-02, 1.9566e-01,\n          9.1189e-01, 4.9684e-01, 5.3415e-01, 5.0093e-01, 5.8309e-01, 9.1034e-01,\n          8.8308e-01, 9.6700e-02, 4.4481e-01, 4.2991e-02, 7.1649e-01, 1.8348e-01,\n          2.9351e-01, 9.4324e-01],\n         [1.7713e-02, 8.9282e-02, 9.0759e-01, 4.4133e-01, 8.7401e-01, 4.0555e-01,\n          5.9744e-01, 5.4717e-01, 7.0500e-01, 4.2696e-01, 9.5083e-01, 2.2531e-01,\n          3.4940e-01, 1.5037e-01, 1.2379e-01, 1.7817e-01, 2.5634e-01, 8.7882e-01,\n          6.1340e-02, 2.4427e-01, 5.6082e-01, 1.6368e-01, 9.2194e-01, 7.5594e-01,\n          8.5591e-01, 7.4876e-01, 6.2569e-01, 4.5841e-01, 3.3794e-01, 7.8336e-01,\n          9.2912e-01, 2.7995e-01, 5.6449e-01, 3.5487e-01, 4.6438e-01, 6.4531e-01,\n          7.1875e-01, 9.0203e-02, 2.1241e-01, 8.4647e-01, 9.5077e-01, 1.5485e-01,\n          4.6092e-04, 5.6498e-01, 3.7296e-01, 3.3258e-01, 6.2398e-01, 5.5868e-01,\n          9.7326e-01, 7.3763e-01],\n         [5.5308e-03, 4.8996e-01, 6.9957e-01, 1.1917e-01, 6.2608e-01, 8.8162e-01,\n          1.5298e-02, 8.1911e-01, 2.7489e-01, 4.8136e-01, 9.2052e-01, 1.7518e-01,\n          1.7687e-01, 5.0438e-01, 8.6543e-02, 2.9031e-01, 3.9158e-01, 2.3139e-01,\n          8.0558e-01, 6.1831e-01, 3.0039e-01, 3.5082e-01, 8.3378e-01, 9.6990e-01,\n          3.1970e-02, 3.8800e-01, 1.2437e-01, 6.8312e-01, 7.4947e-01, 9.5989e-01,\n          8.3323e-01, 7.8453e-01, 8.1001e-01, 7.1522e-01, 2.4395e-01, 1.5638e-01,\n          4.5210e-01, 7.7045e-01, 3.8386e-02, 5.6710e-01, 1.3838e-01, 9.4611e-01,\n          2.9190e-01, 7.0691e-01, 1.2699e-01, 1.3473e-01, 8.4679e-01, 4.3744e-01,\n          5.3880e-01, 6.9893e-01]]), TensorCategory(4)), (tensor([[0.0689, 0.7865, 0.8723, 0.8481, 0.7105, 0.7555, 0.8038, 0.3632, 0.0824,\n          0.1686, 0.4474, 0.7086, 0.8580, 0.6149, 0.3288, 0.9720, 0.0336, 0.5516,\n          0.3200, 0.5084, 0.7491, 0.5423, 0.8919, 0.2135, 0.6551, 0.4944, 0.5279,\n          0.9662, 0.3669, 0.4526, 0.5048, 0.5190, 0.7309, 0.1230, 0.5035, 0.6960,\n          0.5245, 0.1151, 0.9398, 0.9709, 0.6267, 0.3021, 0.1123, 0.5050, 0.8992,\n          0.8220, 0.5573, 0.0340, 0.9674, 0.7205],\n         [0.9439, 0.3214, 0.2222, 0.4896, 0.3049, 0.9547, 0.0312, 0.4896, 0.6329,\n          0.0038, 0.5303, 0.7187, 0.3564, 0.4235, 0.3545, 0.5962, 0.9494, 0.0513,\n          0.0681, 0.8599, 0.4391, 0.4559, 0.2378, 0.9307, 0.7793, 0.6415, 0.8662,\n          0.1586, 0.9223, 0.6322, 0.4574, 0.8958, 0.7227, 0.2083, 0.3468, 0.0912,\n          0.8580, 0.4854, 0.8397, 0.4823, 0.4877, 0.9681, 0.6177, 0.8451, 0.4069,\n          0.8944, 0.8109, 0.7975, 0.5739, 0.7144],\n         [0.5060, 0.1108, 0.2544, 0.3323, 0.0725, 0.6910, 0.1855, 0.5761, 0.0317,\n          0.6479, 0.4791, 0.5826, 0.5812, 0.3061, 0.0645, 0.3390, 0.4950, 0.8611,\n          0.2704, 0.9455, 0.6395, 0.5918, 0.4467, 0.4676, 0.0344, 0.9224, 0.2277,\n          0.4436, 0.0129, 0.1464, 0.5388, 0.7648, 0.4117, 0.4080, 0.3300, 0.1198,\n          0.0270, 0.2132, 0.4182, 0.9566, 0.7364, 0.0107, 0.4383, 0.8693, 0.4277,\n          0.4181, 0.6876, 0.1175, 0.7217, 0.2553],\n         [0.8272, 0.8982, 0.6648, 0.6774, 0.3301, 0.2315, 0.1384, 0.9214, 0.5411,\n          0.8221, 0.2542, 0.4669, 0.3425, 0.9409, 0.4858, 0.2148, 0.2919, 0.7022,\n          0.3758, 0.5943, 0.4973, 0.7483, 0.1452, 0.6764, 0.0327, 0.8770, 0.3286,\n          0.7095, 0.7957, 0.5310, 0.3517, 0.2536, 0.6424, 0.4444, 0.5686, 0.4231,\n          0.5241, 0.7575, 0.6042, 0.6529, 0.3739, 0.7939, 0.4592, 0.9803, 0.6383,\n          0.1278, 0.7054, 0.6528, 0.4225, 0.0770],\n         [0.6727, 0.8275, 0.7885, 0.9180, 0.7005, 0.6412, 0.1451, 0.3753, 0.1706,\n          0.3417, 0.7977, 0.6465, 0.9709, 0.5246, 0.3197, 0.4673, 0.6487, 0.3984,\n          0.7699, 0.6388, 0.3237, 0.9940, 0.4388, 0.4222, 0.8981, 0.8255, 0.6851,\n          0.8143, 0.9794, 0.7385, 0.3605, 0.7780, 0.2999, 0.8979, 0.6516, 0.8986,\n          0.6111, 0.6445, 0.5031, 0.8913, 0.2987, 0.9518, 0.5587, 0.2907, 0.4271,\n          0.5714, 0.3158, 0.2171, 0.7769, 0.5430]]), TensorCategory(8))] ...],\n (#91) [(tensor([[0.0754, 0.7640, 0.7540, 0.0229, 0.3635, 0.1676, 0.5890, 0.9957, 0.0919,\n          0.6538, 0.3232, 0.8642, 0.8775, 0.5533, 0.1906, 0.9864, 0.0168, 0.6951,\n          0.0803, 0.7611, 0.0513, 0.1517, 0.8344, 0.5021, 0.2703, 0.2689, 0.1803,\n          0.1845, 0.8899, 0.5256, 0.9537, 0.3950, 0.1013, 0.0108, 0.5284, 0.9402,\n          0.4903, 0.5958, 0.9239, 0.0255, 0.1412, 0.0917, 0.9903, 0.4342, 0.6034,\n          0.5003, 0.0449, 0.6039, 0.9616, 0.7653],\n         [0.5156, 0.7232, 0.4657, 0.6163, 0.6496, 0.8620, 0.6929, 0.8216, 0.3712,\n          0.0793, 0.7685, 0.8076, 0.8353, 0.8270, 0.9149, 0.1012, 0.8171, 0.8450,\n          0.9298, 0.7525, 0.1495, 0.5862, 0.3035, 0.9208, 0.3748, 0.0630, 0.4567,\n          0.7046, 0.9279, 0.5044, 0.0182, 0.9581, 0.9424, 0.4771, 0.7296, 0.2040,\n          0.2392, 0.5721, 0.4585, 0.1138, 0.9145, 0.6558, 0.6557, 0.4117, 0.2753,\n          0.3853, 0.7683, 0.7059, 0.8687, 0.8124],\n         [0.0605, 0.7707, 0.4363, 0.2529, 0.2869, 0.7825, 0.9705, 0.1448, 0.2206,\n          0.6399, 0.1907, 0.0185, 0.2341, 0.7908, 0.7541, 0.4877, 0.7668, 0.8515,\n          0.1164, 0.0410, 0.6502, 0.4767, 0.9900, 0.4322, 0.6592, 0.8519, 0.5310,\n          0.6188, 0.6833, 0.4470, 0.6208, 0.5867, 0.7193, 0.2033, 0.3691, 0.4981,\n          0.1556, 0.6187, 0.3883, 0.9065, 0.3055, 0.0854, 0.8025, 0.8349, 0.0930,\n          0.0236, 0.5030, 0.8306, 0.2183, 0.0208],\n         [0.4083, 0.7280, 0.1620, 0.8902, 0.7846, 0.9734, 0.9061, 0.5264, 0.8960,\n          0.1391, 0.4306, 0.4791, 0.7430, 0.1748, 0.0896, 0.7077, 0.8876, 0.9346,\n          0.6205, 0.0146, 0.1670, 0.1261, 0.0038, 0.2750, 0.1516, 0.8925, 0.4384,\n          0.8358, 0.1928, 0.7552, 0.4555, 0.0180, 0.5130, 0.6321, 0.1754, 0.3598,\n          0.4412, 0.9056, 0.0128, 0.2585, 0.2140, 0.6774, 0.1993, 0.3098, 0.3090,\n          0.5705, 0.8381, 0.4459, 0.5237, 0.0064],\n         [0.6927, 0.1066, 0.0820, 0.6425, 0.2995, 0.5597, 0.5210, 0.2240, 0.2782,\n          0.8256, 0.9923, 0.7860, 0.3501, 0.7260, 0.6577, 0.1949, 0.3274, 0.4140,\n          0.2911, 0.3458, 0.5822, 0.0132, 0.4202, 0.3940, 0.3010, 0.9059, 0.0965,\n          0.6957, 0.1748, 0.4780, 0.1714, 0.4711, 0.7799, 0.9038, 0.3035, 0.0515,\n          0.6464, 0.4390, 0.6665, 0.1438, 0.8661, 0.1677, 0.6909, 0.6596, 0.4735,\n          0.5157, 0.9292, 0.7969, 0.9181, 0.2009]]), TensorCategory(5)), (tensor([[0.5749, 0.0676, 0.2870, 0.7596, 0.6924, 0.1214, 0.9383, 0.8976, 0.5824,\n          0.1053, 0.5177, 0.9816, 0.9532, 0.3309, 0.6265, 0.5134, 0.8465, 0.6129,\n          0.6974, 0.3888, 0.7937, 0.8413, 0.6725, 0.9868, 0.4335, 0.4220, 0.9812,\n          0.9789, 0.6422, 0.4346, 0.4138, 0.9759, 0.7418, 0.1828, 0.1262, 0.9360,\n          0.4578, 0.1341, 0.1092, 0.5232, 0.2734, 0.3160, 0.5098, 0.3831, 0.5745,\n          0.1630, 0.7304, 0.4031, 0.1880, 0.2395],\n         [0.5397, 0.1265, 0.7367, 0.1416, 0.9051, 0.5495, 0.3089, 0.1388, 0.3242,\n          0.8234, 0.2216, 0.4923, 0.5801, 0.5124, 0.5966, 0.6809, 0.0104, 0.3422,\n          0.7067, 0.3878, 0.2914, 0.6360, 0.9195, 0.8812, 0.6658, 0.6324, 0.3810,\n          0.8294, 0.8466, 0.2232, 0.3877, 0.7746, 0.9723, 0.9498, 0.5390, 0.5798,\n          0.4407, 0.3906, 0.3150, 0.0775, 0.6311, 0.4040, 0.3995, 0.5905, 0.4577,\n          0.2913, 0.2707, 0.5923, 0.5946, 0.4445],\n         [0.1254, 0.5529, 0.3445, 0.3198, 0.3047, 0.3908, 0.5945, 0.5048, 0.2876,\n          0.0468, 0.8157, 0.9285, 0.1182, 0.2846, 0.6896, 0.9336, 0.5141, 0.9059,\n          0.1723, 0.3985, 0.2683, 0.8417, 0.0055, 0.2613, 0.5941, 0.6746, 0.2360,\n          0.4065, 0.2932, 0.5554, 0.5260, 0.5614, 0.6119, 0.1953, 0.5655, 0.6731,\n          0.8414, 0.2238, 0.0540, 0.3660, 0.4780, 0.4117, 0.7255, 0.2708, 0.2859,\n          0.1485, 0.2852, 0.9839, 0.6291, 0.2102],\n         [0.2746, 0.5636, 0.6689, 0.7527, 0.4822, 0.5085, 0.6553, 0.0848, 0.8277,\n          0.7828, 0.6715, 0.8111, 0.6262, 0.0156, 0.3057, 0.8091, 0.2882, 0.3992,\n          0.6706, 0.5189, 0.8783, 0.9855, 0.1066, 0.8761, 0.8643, 0.1032, 0.2617,\n          0.4602, 0.3747, 0.6093, 0.3767, 0.6704, 0.4815, 0.0919, 0.1862, 0.9853,\n          0.6527, 0.8077, 0.9020, 0.1102, 0.8340, 0.1066, 0.1793, 0.9714, 0.6420,\n          0.5555, 0.0633, 0.8378, 0.8029, 0.4368],\n         [0.2812, 0.7131, 0.0833, 0.5796, 0.5071, 0.4008, 0.3492, 0.8932, 0.7502,\n          0.6994, 0.5548, 0.8843, 0.3303, 0.5338, 0.4032, 0.6421, 0.5483, 0.4076,\n          0.8462, 0.2366, 0.4333, 0.1211, 0.2006, 0.9585, 0.5917, 0.0800, 0.3457,\n          0.6818, 0.3232, 0.5202, 0.6502, 0.8149, 0.2441, 0.1314, 0.6230, 0.4024,\n          0.6532, 0.6731, 0.7966, 0.0541, 0.1290, 0.6253, 0.5117, 0.8866, 0.9301,\n          0.5898, 0.2739, 0.1381, 0.4392, 0.1253]]), TensorCategory(8)), (tensor([[0.4524, 0.0690, 0.8418, 0.3803, 0.8346, 0.9162, 0.5926, 0.0724, 0.1166,\n          0.2780, 0.8495, 0.6946, 0.2605, 0.1426, 0.4075, 0.9882, 0.0581, 0.9858,\n          0.8584, 0.8494, 0.5314, 0.2059, 0.4862, 0.4677, 0.7278, 0.0704, 0.0569,\n          0.8042, 0.4609, 0.8249, 0.0864, 0.6572, 0.3743, 0.9648, 0.1147, 0.4709,\n          0.3585, 0.1029, 0.7161, 0.7205, 0.9198, 0.0916, 0.4649, 0.8699, 0.5751,\n          0.0867, 0.4679, 0.9641, 0.2418, 0.1620],\n         [0.8177, 0.4861, 0.7706, 0.5925, 0.9017, 0.1663, 0.7050, 0.0234, 0.7036,\n          0.1230, 0.2484, 0.5279, 0.9920, 0.0350, 0.7795, 0.0403, 0.7154, 0.4942,\n          0.0207, 0.6519, 0.0092, 0.8881, 0.2563, 0.0661, 0.2393, 0.6700, 0.4811,\n          0.3776, 0.2718, 0.0682, 0.9584, 0.4197, 0.0419, 0.0762, 0.2024, 0.9805,\n          0.3611, 0.3075, 0.7730, 0.4494, 0.3361, 0.6418, 0.0977, 0.7404, 0.6354,\n          0.8239, 0.5157, 0.2773, 0.3367, 0.7804],\n         [0.0521, 0.8514, 0.6955, 0.6328, 0.7104, 0.5556, 0.8293, 0.8511, 0.8690,\n          0.8066, 0.6868, 0.0192, 0.2884, 0.5743, 0.7122, 0.2806, 0.1696, 0.4540,\n          0.4612, 0.7351, 0.4602, 0.0748, 0.2464, 0.1118, 0.0293, 0.1366, 0.2906,\n          0.6226, 0.9188, 0.4740, 0.1238, 0.7768, 0.5795, 0.3380, 0.4300, 0.4827,\n          0.9395, 0.2608, 0.1795, 0.8620, 0.0535, 0.0271, 0.2882, 0.6105, 0.1012,\n          0.8579, 0.6109, 0.8255, 0.3161, 0.2275],\n         [0.4140, 0.5829, 0.3843, 0.8255, 0.2507, 0.4739, 0.7920, 0.5091, 0.3341,\n          0.6966, 0.4745, 0.1101, 0.1449, 0.7334, 0.0083, 0.4009, 0.0528, 0.0788,\n          0.3154, 0.2935, 0.3721, 0.9257, 0.7224, 0.9099, 0.7440, 0.6272, 0.6623,\n          0.4829, 0.0130, 0.4757, 0.8843, 0.9215, 0.4775, 0.7455, 0.2335, 0.6269,\n          0.7219, 0.0496, 0.0184, 0.5806, 0.8029, 0.2436, 0.9533, 0.8838, 0.9416,\n          0.2344, 0.1986, 0.8769, 0.8907, 0.1895],\n         [0.5903, 0.2509, 0.9972, 0.5203, 0.0072, 0.2779, 0.8664, 0.2244, 0.1725,\n          0.0390, 0.8248, 0.0986, 0.2529, 0.5061, 0.4975, 0.8533, 0.1479, 0.7489,\n          0.4362, 0.7900, 0.9714, 0.4652, 0.4277, 0.8465, 0.3441, 0.2362, 0.5640,\n          0.5117, 0.9068, 0.3529, 0.3449, 0.3369, 0.5777, 0.5931, 0.9980, 0.7687,\n          0.0061, 0.6044, 0.4866, 0.1783, 0.0691, 0.4085, 0.3903, 0.5629, 0.9981,\n          0.1664, 0.5467, 0.7487, 0.5558, 0.9969]]), TensorCategory(2)), (tensor([[0.9183, 0.1485, 0.3292, 0.3916, 0.8026, 0.6659, 0.6822, 0.7891, 0.4115,\n          0.1431, 0.7676, 0.7790, 0.4453, 0.9530, 0.5824, 0.7150, 0.7707, 0.5478,\n          0.5490, 0.7015, 0.0228, 0.1130, 0.3695, 0.6657, 0.0096, 0.5396, 0.7051,\n          0.5068, 0.0203, 0.0773, 0.9854, 0.9678, 0.2638, 0.2916, 0.5089, 0.4799,\n          0.0196, 0.3322, 0.5581, 0.6527, 0.1521, 0.6115, 0.9763, 0.5694, 0.9638,\n          0.7486, 0.9295, 0.2324, 0.4573, 0.6193],\n         [0.3857, 0.8403, 0.0396, 0.6787, 0.8459, 0.7492, 0.7312, 0.6715, 0.0389,\n          0.6281, 0.1908, 0.5125, 0.3419, 0.5309, 0.6423, 0.2144, 0.4054, 0.8816,\n          0.6115, 0.2838, 0.5580, 0.9902, 0.9369, 0.2837, 0.2638, 0.8216, 0.7228,\n          0.8084, 0.9095, 0.9670, 0.0758, 0.9129, 0.2283, 0.8316, 0.8924, 0.0355,\n          0.0230, 0.3245, 0.5862, 0.8208, 0.3688, 0.7579, 0.0507, 0.8273, 0.9188,\n          0.9796, 0.8449, 0.0634, 0.4584, 0.0906],\n         [0.0919, 0.8241, 0.9515, 0.6766, 0.9881, 0.6011, 0.7977, 0.7650, 0.6764,\n          0.3932, 0.0972, 0.8417, 0.7609, 0.7568, 0.1907, 0.0498, 0.2430, 0.4749,\n          0.2350, 0.3890, 0.1241, 0.2300, 0.7256, 0.4468, 0.2403, 0.5951, 0.3026,\n          0.2825, 0.1236, 0.3742, 0.1245, 0.1168, 0.5159, 0.5827, 0.6321, 0.0935,\n          0.7805, 0.8597, 0.9487, 0.8613, 0.5737, 0.0457, 0.7524, 0.3876, 0.1881,\n          0.6010, 0.5817, 0.6556, 0.6679, 0.2133],\n         [0.3292, 0.0627, 0.3008, 0.6641, 0.9428, 0.9853, 0.3939, 0.4862, 0.0327,\n          0.7852, 0.6322, 0.6541, 0.1386, 0.2281, 0.0666, 0.0246, 0.5484, 0.7859,\n          0.0208, 0.2072, 0.8700, 0.1906, 0.3968, 0.6501, 0.0808, 0.0049, 0.1393,\n          0.8728, 0.2788, 0.3736, 0.5112, 0.9156, 0.4482, 0.0133, 0.4045, 0.9611,\n          0.1023, 0.6130, 0.2601, 0.0796, 0.0961, 0.9863, 0.2096, 0.9805, 0.9476,\n          0.1255, 0.8764, 0.1451, 0.0552, 0.8688],\n         [0.5032, 0.2307, 0.7737, 0.6040, 0.6325, 0.4034, 0.9390, 0.1976, 0.6939,\n          0.2856, 0.8841, 0.8005, 0.6652, 0.2401, 0.5155, 0.2553, 0.1279, 0.0412,\n          0.9720, 0.8660, 0.5232, 0.4534, 0.8946, 0.0067, 0.1205, 0.6934, 0.6232,\n          0.3787, 0.9334, 0.3988, 0.6288, 0.1400, 0.7522, 0.2105, 0.6789, 0.5876,\n          0.3780, 0.6225, 0.9952, 0.7033, 0.7150, 0.1375, 0.9836, 0.2466, 0.5331,\n          0.6476, 0.9900, 0.5381, 0.4111, 0.8314]]), TensorCategory(7)), (tensor([[0.3484, 0.8380, 0.2164, 0.6643, 0.8279, 0.5547, 0.5646, 0.0106, 0.7051,\n          0.2712, 0.6697, 0.4816, 0.2403, 0.3839, 0.9424, 0.4436, 0.7841, 0.0878,\n          0.8360, 0.4465, 0.7603, 0.1925, 0.6197, 0.3679, 0.8134, 0.1199, 0.3312,\n          0.4595, 0.0476, 0.9298, 0.8553, 0.5829, 0.0021, 0.3784, 0.5885, 0.5334,\n          0.5892, 0.4477, 0.6955, 0.8353, 0.7520, 0.0166, 0.0532, 0.4422, 0.4730,\n          0.6734, 0.4874, 0.3902, 0.8161, 0.4276],\n         [0.2070, 0.1861, 0.7212, 0.7798, 0.5112, 0.0886, 0.0218, 0.0950, 0.2667,\n          0.7716, 0.0342, 0.0480, 0.6842, 0.3024, 0.9367, 0.3662, 0.2869, 0.2789,\n          0.4102, 0.8594, 0.6152, 0.8715, 0.5312, 0.6503, 0.8042, 0.6395, 0.7671,\n          0.3223, 0.8417, 0.8243, 0.1807, 0.3580, 0.4109, 0.3823, 0.6650, 0.4225,\n          0.5277, 0.6120, 0.4835, 0.5141, 0.2274, 0.2088, 0.4338, 0.1559, 0.1120,\n          0.4329, 0.9680, 0.9616, 0.5343, 0.8512],\n         [0.5006, 0.1432, 0.3733, 0.9982, 0.1383, 0.3187, 0.1626, 0.1609, 0.0523,\n          0.8701, 0.9593, 0.5568, 0.7599, 0.3565, 0.5454, 0.8616, 0.5509, 0.6169,\n          0.4327, 0.7244, 0.7514, 0.2308, 0.3990, 0.0450, 0.1469, 0.1849, 0.3483,\n          0.6511, 0.6585, 0.8310, 0.1842, 0.5914, 0.0258, 0.7064, 0.5853, 0.7910,\n          0.7423, 0.5511, 0.7287, 0.1850, 0.5412, 0.8507, 0.4172, 0.8308, 0.7105,\n          0.3531, 0.2849, 0.0978, 0.2769, 0.9707],\n         [0.4279, 0.5987, 0.5623, 0.9285, 0.2959, 0.3033, 0.3583, 0.6532, 0.2486,\n          0.5199, 0.3110, 0.4176, 0.1561, 0.1398, 0.7356, 0.3203, 0.9123, 0.9854,\n          0.8956, 0.9800, 0.0608, 0.8633, 0.9254, 0.2688, 0.7820, 0.3992, 0.3374,\n          0.2299, 0.2046, 0.1009, 0.5198, 0.3177, 0.3073, 0.6064, 0.8700, 0.1630,\n          0.9706, 0.6576, 0.2832, 0.9317, 0.2828, 0.2402, 0.8267, 0.0620, 0.3403,\n          0.7253, 0.4141, 0.9895, 0.0587, 0.5373],\n         [0.1053, 0.5261, 0.2117, 0.6872, 0.0589, 0.4204, 0.6577, 0.3861, 0.0919,\n          0.9271, 0.2818, 0.9134, 0.8141, 0.1986, 0.0376, 0.9505, 0.4375, 0.2849,\n          0.2046, 0.7433, 0.4342, 0.5330, 0.1883, 0.9227, 0.0806, 0.0090, 0.8697,\n          0.8394, 0.4406, 0.9895, 0.3353, 0.6157, 0.2904, 0.2816, 0.0053, 0.5636,\n          0.4578, 0.0412, 0.7670, 0.2344, 0.6462, 0.1155, 0.7988, 0.7545, 0.7188,\n          0.9688, 0.2562, 0.1500, 0.3170, 0.4288]]), TensorCategory(6)), (tensor([[0.5448, 0.6730, 0.3384, 0.2668, 0.1988, 0.9335, 0.2716, 0.4963, 0.6257,\n          0.6851, 0.0892, 0.3336, 0.4193, 0.4476, 0.3045, 0.9737, 0.4211, 0.8548,\n          0.7980, 0.4249, 0.1227, 0.8133, 0.7330, 0.1296, 0.0648, 0.6886, 0.7078,\n          0.0791, 0.8216, 0.3900, 0.9150, 0.5376, 0.1884, 0.5402, 0.5686, 0.5029,\n          0.3401, 0.3092, 0.9889, 0.6742, 0.2015, 0.3741, 0.8013, 0.3956, 0.4698,\n          0.1012, 0.6173, 0.6136, 0.0549, 0.9011],\n         [0.1115, 0.8845, 0.4413, 0.9646, 0.6278, 0.1114, 0.6631, 0.7547, 0.2289,\n          0.5041, 0.7091, 0.6663, 0.7337, 0.1117, 0.5028, 0.3797, 0.0595, 0.0290,\n          0.2607, 0.1085, 0.2138, 0.4307, 0.6313, 0.3833, 0.8262, 0.3758, 0.1260,\n          0.5609, 0.4324, 0.1481, 0.0510, 0.2682, 0.9864, 0.7399, 0.1046, 0.8834,\n          0.7315, 0.6921, 0.0899, 0.4372, 0.0986, 0.4170, 0.5972, 0.9560, 0.4612,\n          0.0469, 0.4334, 0.2580, 0.2174, 0.2507],\n         [0.9636, 0.8036, 0.9999, 0.0284, 0.5694, 0.2628, 0.7998, 0.4361, 0.9394,\n          0.7717, 0.5970, 0.8935, 0.4107, 0.4290, 0.5976, 0.3433, 0.9804, 0.3016,\n          0.4421, 0.2126, 0.0493, 0.5106, 0.5332, 0.7954, 0.4745, 0.2894, 0.3362,\n          0.5440, 0.4378, 0.5687, 0.2472, 0.7646, 0.1878, 0.5679, 0.4690, 0.6584,\n          0.8852, 0.4593, 0.0498, 0.8667, 0.4672, 0.1546, 0.8620, 0.7422, 0.9676,\n          0.7956, 0.3098, 0.6093, 0.9484, 0.9190],\n         [0.5747, 0.0018, 0.6743, 0.7265, 0.0653, 0.4302, 0.1287, 0.6109, 0.4373,\n          0.4012, 0.2521, 0.3329, 0.2829, 0.3411, 0.4142, 0.5330, 0.2091, 0.9086,\n          0.0745, 0.3533, 0.5554, 0.8417, 0.3981, 0.5565, 0.5853, 0.9139, 0.7913,\n          0.4816, 0.4369, 0.1306, 0.4130, 0.7494, 0.6830, 0.2476, 0.1699, 0.3945,\n          0.9386, 0.8135, 0.5139, 0.3556, 0.0716, 0.6012, 0.7177, 0.0300, 0.8757,\n          0.2280, 0.7456, 0.9959, 0.3372, 0.4791],\n         [0.5183, 0.8129, 0.1916, 0.1637, 0.3528, 0.2430, 0.5261, 0.4434, 0.7176,\n          0.8667, 0.5140, 0.4519, 0.8156, 0.6610, 0.4676, 0.0580, 0.9967, 0.3183,\n          0.2581, 0.4872, 0.5450, 0.5584, 0.4705, 0.8692, 0.2188, 0.7518, 0.1512,\n          0.4299, 0.5577, 0.7135, 0.9122, 0.1720, 0.3777, 0.7705, 0.6510, 0.6010,\n          0.1980, 0.0220, 0.7881, 0.0669, 0.9008, 0.2397, 0.9662, 0.2193, 0.5222,\n          0.0184, 0.3187, 0.2445, 0.6027, 0.1691]]), TensorCategory(2)), (tensor([[0.1305, 0.3410, 0.9336, 0.6019, 0.3414, 0.9195, 0.6156, 0.7602, 0.5451,\n          0.1441, 0.5034, 0.7481, 0.8862, 0.5275, 0.8213, 0.2818, 0.9666, 0.3679,\n          0.9791, 0.6941, 0.7225, 0.8236, 0.5213, 0.1095, 0.2833, 0.6395, 0.3007,\n          0.9967, 0.5163, 0.5144, 0.0599, 0.1679, 0.2309, 0.3186, 0.6675, 0.6761,\n          0.6357, 0.7734, 0.5877, 0.3058, 0.3024, 0.0105, 0.1684, 0.3547, 0.2787,\n          0.4609, 0.0205, 0.5164, 0.1784, 0.1979],\n         [0.9245, 0.5943, 0.1246, 0.9037, 0.6892, 0.4747, 0.9296, 0.8620, 0.7818,\n          0.9454, 0.0380, 0.8724, 0.0188, 0.3312, 0.9679, 0.2687, 0.5937, 0.8603,\n          0.5096, 0.7176, 0.1633, 0.7190, 0.7946, 0.7344, 0.0217, 0.2135, 0.6252,\n          0.6828, 0.6063, 0.8863, 0.3375, 0.2796, 0.6647, 0.7603, 0.0338, 0.3640,\n          0.5946, 0.4327, 0.4599, 0.0823, 0.6040, 0.0187, 0.7267, 0.1947, 0.9795,\n          0.7903, 0.0681, 0.0837, 0.6118, 0.3374],\n         [0.4739, 0.6917, 0.3476, 0.1769, 0.7306, 0.4365, 0.1864, 0.6460, 0.0606,\n          0.8509, 0.3799, 0.7178, 0.4959, 0.7883, 0.5159, 0.2568, 0.9741, 0.5861,\n          0.6957, 0.3645, 0.1321, 0.3352, 0.5881, 0.2779, 0.1134, 0.0941, 0.4765,\n          0.3734, 0.7780, 0.6035, 0.9821, 0.4333, 0.8143, 0.5804, 0.8126, 0.4089,\n          0.9897, 0.5883, 0.1216, 0.3332, 0.3670, 0.6335, 0.3828, 0.2079, 0.4933,\n          0.7185, 0.9263, 0.0911, 0.7833, 0.7949],\n         [0.5032, 0.5277, 0.4405, 0.7093, 0.2052, 0.5098, 0.4099, 0.4722, 0.5338,\n          0.9776, 0.7555, 0.7163, 0.2440, 0.3572, 0.1376, 0.0158, 0.6696, 0.0885,\n          0.7775, 0.4824, 0.7306, 0.5109, 0.1105, 0.2793, 0.9436, 0.2791, 0.4714,\n          0.2958, 0.7605, 0.2945, 0.5355, 0.5120, 0.0093, 0.9315, 0.2976, 0.8729,\n          0.9924, 0.7848, 0.2289, 0.6595, 0.6131, 0.9784, 0.9596, 0.9625, 0.2705,\n          0.2961, 0.7593, 0.4607, 0.9455, 0.9875],\n         [0.3858, 0.8383, 0.4908, 0.2534, 0.0431, 0.5627, 0.7792, 0.5202, 0.1046,\n          0.6807, 0.0065, 0.3672, 0.9135, 0.3758, 0.6192, 0.2634, 0.1849, 0.0601,\n          0.1136, 0.3131, 0.0268, 0.0572, 0.7953, 0.3400, 0.6641, 0.4478, 0.3820,\n          0.4256, 0.3619, 0.8494, 0.8632, 0.4889, 0.6113, 0.5961, 0.2082, 0.3814,\n          0.2755, 0.7126, 0.1756, 0.1558, 0.9267, 0.4210, 0.0498, 0.1664, 0.4620,\n          0.1905, 0.4988, 0.3452, 0.0026, 0.8766]]), TensorCategory(8)), (tensor([[2.9933e-01, 8.2202e-01, 3.7656e-01, 5.8596e-01, 3.5499e-01, 3.5988e-01,\n          4.6862e-01, 7.6412e-01, 8.1862e-02, 3.8619e-01, 8.3507e-01, 9.6320e-01,\n          2.6491e-01, 1.1739e-01, 7.4671e-01, 1.6857e-01, 4.1803e-01, 3.6275e-01,\n          2.5102e-01, 7.8340e-01, 1.3938e-01, 8.1179e-01, 1.3981e-01, 6.0063e-01,\n          3.1625e-02, 9.1186e-01, 1.3220e-01, 7.8207e-01, 1.8575e-01, 1.5887e-01,\n          3.9574e-01, 3.3722e-01, 6.9353e-01, 5.8760e-01, 4.0886e-01, 8.4150e-02,\n          8.5479e-01, 3.0219e-01, 8.1665e-01, 3.5641e-01, 6.9631e-02, 6.9424e-01,\n          6.6629e-01, 2.3864e-01, 2.8181e-01, 1.4432e-01, 3.6855e-01, 4.0707e-01,\n          7.0908e-01, 9.0207e-01],\n         [4.2411e-01, 3.6326e-01, 4.7003e-01, 6.2281e-01, 2.7289e-01, 8.3707e-01,\n          6.7848e-02, 8.6735e-01, 6.3958e-01, 8.1132e-01, 4.9198e-01, 2.0898e-01,\n          5.1095e-01, 4.9600e-01, 4.1710e-01, 1.0046e-01, 7.5031e-01, 9.4411e-01,\n          7.4545e-01, 1.8224e-01, 8.6504e-01, 1.2869e-01, 8.5232e-02, 4.9695e-01,\n          7.4185e-01, 7.3800e-01, 4.0463e-01, 3.6400e-01, 1.8617e-01, 6.7047e-01,\n          7.9691e-03, 1.6765e-01, 3.0461e-01, 7.9984e-01, 3.1969e-01, 3.8994e-01,\n          9.9382e-01, 5.5041e-01, 5.8965e-01, 7.1523e-01, 3.7090e-01, 3.7452e-01,\n          2.2493e-01, 6.4727e-01, 2.1324e-01, 3.9298e-01, 3.5642e-01, 2.7549e-01,\n          4.4773e-01, 7.2346e-02],\n         [3.6269e-01, 5.7875e-02, 7.0141e-02, 4.9759e-01, 2.7215e-01, 1.8837e-01,\n          8.0823e-02, 6.7251e-01, 2.4830e-01, 2.8716e-01, 6.9724e-01, 5.4848e-04,\n          3.5806e-01, 9.8262e-01, 7.3279e-01, 1.8107e-02, 4.4758e-02, 9.1824e-02,\n          3.0290e-01, 6.7592e-02, 6.8286e-01, 1.5158e-01, 9.4673e-01, 9.2773e-01,\n          6.7799e-01, 1.4371e-01, 7.4452e-01, 7.3375e-01, 7.2332e-03, 3.2732e-01,\n          9.9615e-01, 2.2926e-01, 5.5190e-01, 8.1034e-01, 7.7724e-01, 3.6016e-01,\n          2.7257e-02, 1.3019e-01, 5.2309e-02, 6.2116e-01, 1.6624e-02, 2.3656e-01,\n          2.9132e-01, 4.9089e-01, 1.7346e-01, 9.3851e-01, 7.7748e-01, 1.3974e-01,\n          4.0214e-01, 8.4022e-01],\n         [3.8935e-01, 9.2747e-01, 2.0496e-01, 6.8011e-01, 6.7873e-01, 9.4878e-01,\n          1.4583e-01, 8.0525e-01, 5.3356e-01, 5.8170e-01, 3.9563e-01, 1.5595e-01,\n          9.5608e-01, 2.4406e-01, 4.7267e-01, 9.6044e-01, 4.4698e-01, 4.6883e-01,\n          5.3270e-01, 5.4049e-01, 3.2573e-01, 8.3300e-01, 8.4506e-01, 3.6632e-01,\n          6.6326e-01, 4.4297e-01, 3.4640e-01, 1.6936e-01, 5.5261e-01, 8.8733e-01,\n          5.0815e-01, 1.1175e-01, 2.6063e-01, 4.6595e-01, 5.4046e-01, 3.3102e-01,\n          7.9525e-01, 1.2369e-01, 4.7363e-01, 4.9903e-01, 5.1257e-01, 1.4239e-01,\n          8.1257e-01, 8.2323e-01, 1.6896e-01, 2.2311e-01, 5.1178e-01, 1.6772e-01,\n          6.3062e-01, 7.4821e-02],\n         [5.8521e-01, 9.7334e-02, 3.1683e-01, 8.4878e-01, 5.5618e-01, 4.3529e-02,\n          3.3815e-01, 2.8810e-01, 2.5916e-01, 7.3121e-01, 9.5577e-01, 4.4830e-01,\n          9.8740e-01, 6.3400e-01, 1.9755e-01, 9.5913e-01, 9.1498e-01, 8.2773e-01,\n          9.6942e-01, 1.0368e-01, 6.1009e-01, 5.7969e-01, 2.6909e-01, 8.8548e-01,\n          2.8113e-01, 6.4904e-02, 6.6546e-01, 2.3859e-01, 6.2308e-01, 7.1326e-01,\n          5.6182e-01, 2.3650e-01, 8.1552e-01, 3.5689e-01, 3.1267e-01, 7.7860e-01,\n          6.7371e-02, 8.7438e-01, 1.1958e-01, 8.2926e-01, 3.1729e-01, 5.9391e-01,\n          4.1063e-01, 2.1675e-01, 3.3540e-01, 8.6369e-01, 3.7700e-02, 7.4238e-01,\n          7.0310e-01, 4.5944e-01]]), TensorCategory(8)), (tensor([[0.4771, 0.3465, 0.2735, 0.8881, 0.4774, 0.2327, 0.1427, 0.4813, 0.8968,\n          0.3685, 0.6578, 0.4781, 0.3216, 0.9291, 0.4140, 0.8003, 0.3085, 0.8153,\n          0.4718, 0.4611, 0.1689, 0.0340, 0.0127, 0.7334, 0.7397, 0.2096, 0.6345,\n          0.1785, 0.2497, 0.6105, 0.7121, 0.5098, 0.8882, 0.2979, 0.8177, 0.3861,\n          0.7644, 0.6291, 0.6936, 0.6876, 0.4847, 0.9732, 0.9556, 0.8093, 0.0808,\n          0.1772, 0.1999, 0.3905, 0.5532, 0.4982],\n         [0.0522, 0.5119, 0.9743, 0.5291, 0.0826, 0.0541, 0.4608, 0.7468, 0.4171,\n          0.1188, 0.9730, 0.9354, 0.6511, 0.9196, 0.5223, 0.9622, 0.5804, 0.1376,\n          0.2355, 0.1694, 0.5065, 0.6361, 0.7353, 0.8004, 0.7141, 0.5799, 0.0996,\n          0.6401, 0.6353, 0.7134, 0.9128, 0.1666, 0.3593, 0.4730, 0.9984, 0.7561,\n          0.2890, 0.1422, 0.3766, 0.7569, 0.1610, 0.2504, 0.5662, 0.0461, 0.8442,\n          0.9593, 0.9727, 0.3548, 0.3897, 0.5747],\n         [0.2996, 0.6321, 0.6812, 0.0470, 0.6586, 0.3479, 0.7638, 0.9962, 0.2685,\n          0.6105, 0.6271, 0.2872, 0.9271, 0.8993, 0.0108, 0.1981, 0.5792, 0.8245,\n          0.4689, 0.2262, 0.8245, 0.5237, 0.0849, 0.3656, 0.6025, 0.4496, 0.2667,\n          0.3116, 0.0316, 0.3776, 0.9906, 0.8499, 0.1156, 0.7293, 0.7830, 0.0714,\n          0.0747, 0.8022, 0.7693, 0.2593, 0.4023, 0.0763, 0.9561, 0.8122, 0.3237,\n          0.0555, 0.1759, 0.1492, 0.4959, 0.7555],\n         [0.8009, 0.9163, 0.4069, 0.6375, 0.1095, 0.0012, 0.3879, 0.3970, 0.8785,\n          0.7880, 0.1033, 0.1587, 0.9639, 0.8406, 0.7077, 0.1687, 0.0640, 0.8173,\n          0.5289, 0.4080, 0.5693, 0.2807, 0.5101, 0.3527, 0.1448, 0.7454, 0.0942,\n          0.0575, 0.5541, 0.5818, 0.2037, 0.0708, 0.8386, 0.4664, 0.9040, 0.2748,\n          0.9475, 0.2935, 0.8415, 0.6649, 0.5025, 0.7226, 0.9727, 0.4712, 0.8790,\n          0.7375, 0.1842, 0.5399, 0.3269, 0.6945],\n         [0.6481, 0.2194, 0.9986, 0.7972, 0.2504, 0.0608, 0.1451, 0.7697, 0.5119,\n          0.6897, 0.5853, 0.1800, 0.6335, 0.6973, 0.6107, 0.4054, 0.7002, 0.4178,\n          0.7886, 0.6401, 0.2017, 0.6756, 0.1704, 0.3046, 0.5040, 0.3781, 0.7670,\n          0.6252, 0.0039, 0.4711, 0.0250, 0.9568, 0.4608, 0.8320, 0.4001, 0.0074,\n          0.2935, 0.4375, 0.5760, 0.4653, 0.1775, 0.4077, 0.9157, 0.3638, 0.6072,\n          0.4298, 0.9860, 0.7791, 0.7873, 0.7876]]), TensorCategory(1)), (tensor([[0.9554, 0.6716, 0.6949, 0.8437, 0.8596, 0.8451, 0.0959, 0.6148, 0.9016,\n          0.4440, 0.5675, 0.5549, 0.4203, 0.8124, 0.8239, 0.2439, 0.5368, 0.6854,\n          0.0945, 0.8910, 0.2647, 0.7133, 0.9598, 0.1530, 0.7266, 0.3153, 0.5267,\n          0.0972, 0.2201, 0.5043, 0.3568, 0.6698, 0.7209, 0.5507, 0.2248, 0.6752,\n          0.8431, 0.4890, 0.2546, 0.4836, 0.4364, 0.7638, 0.4544, 0.1818, 0.7381,\n          0.7352, 0.2316, 0.8124, 0.3958, 0.8268],\n         [0.5835, 0.3806, 0.0321, 0.7230, 0.8928, 0.7843, 0.8702, 0.3976, 0.6267,\n          0.9008, 0.3541, 0.1950, 0.7730, 0.3324, 0.1646, 0.5648, 0.3693, 0.2750,\n          0.7024, 0.8024, 0.5890, 0.3270, 0.9205, 0.0413, 0.2343, 0.8362, 0.0328,\n          0.2269, 0.2233, 0.7937, 0.5875, 0.7967, 0.5992, 0.8129, 0.2971, 0.5278,\n          0.1051, 0.7322, 0.3893, 0.9505, 0.2980, 0.8793, 0.5756, 0.0407, 0.6240,\n          0.8554, 0.1889, 0.4197, 0.0596, 0.5318],\n         [0.4366, 0.7611, 0.0339, 0.3130, 0.6306, 0.4621, 0.5996, 0.5259, 0.1327,\n          0.4048, 0.2819, 0.4189, 0.7973, 0.9721, 0.8249, 0.0632, 0.6517, 0.6566,\n          0.8002, 0.1965, 0.1962, 0.2087, 0.2105, 0.1175, 0.3843, 0.2434, 0.8605,\n          0.1568, 0.5471, 0.0950, 0.0535, 0.2887, 0.2732, 0.3731, 0.9504, 0.5869,\n          0.5137, 0.9288, 0.2685, 0.6397, 0.6888, 0.7453, 0.5844, 0.8846, 0.9729,\n          0.0661, 0.5052, 0.3187, 0.7161, 0.6322],\n         [0.7901, 0.2498, 0.2656, 0.7804, 0.3232, 0.5128, 0.8012, 0.9579, 0.3311,\n          0.9485, 0.5964, 0.6336, 0.5927, 0.7739, 0.1202, 0.2620, 0.7852, 0.1455,\n          0.1979, 0.2281, 0.8850, 0.0067, 0.1394, 0.9207, 0.9910, 0.1453, 0.8781,\n          0.0988, 0.3076, 0.3424, 0.6191, 0.6965, 0.1837, 0.6369, 0.2438, 0.6117,\n          0.4926, 0.7395, 0.4962, 0.2981, 0.9782, 0.4717, 0.2973, 0.0845, 0.2121,\n          0.6358, 0.2380, 0.9576, 0.6397, 0.2547],\n         [0.0486, 0.7485, 0.1689, 0.9219, 0.1072, 0.6361, 0.9943, 0.6120, 0.9915,\n          0.7609, 0.0519, 0.0183, 0.5270, 0.4359, 0.3300, 0.0328, 0.5497, 0.3627,\n          0.9290, 0.8003, 0.1052, 0.4404, 0.9856, 0.0711, 0.1660, 0.4190, 0.5879,\n          0.8038, 0.9581, 0.8877, 0.1309, 0.4395, 0.1283, 0.1020, 0.8000, 0.3567,\n          0.3546, 0.3000, 0.9232, 0.7480, 0.2611, 0.8657, 0.7216, 0.1201, 0.2615,\n          0.1968, 0.3238, 0.3489, 0.6132, 0.9277]]), TensorCategory(8))] ...]]\n\n\n\nmetadataset = TSMetaDataset(dsets)\nmetadataset, metadataset.vars, metadataset.len\n\n(<__main__.TSMetaDataset>, 5, 50)\n\n\nWe’ll apply splits now to create train and valid metadatasets:\n\nsplits = TimeSplitter()(metadataset)\nsplits\n\n\n\n\n((#229) [0,1,2,3,4,5,6,7,8,9...],\n (#57) [229,230,231,232,233,234,235,236,237,238...])\n\n\n\nmetadatasets = TSMetaDatasets(metadataset, splits=splits)\nmetadatasets.train, metadatasets.valid\n\n(<__main__.TSMetaDataset>,\n <__main__.TSMetaDataset>)\n\n\n\ndls = TSDataLoaders.from_dsets(metadatasets.train, metadatasets.valid)\nxb, yb = first(dls.train)\nxb, yb\n\n(tensor([[[0.8752, 0.5482, 0.1431,  ..., 0.7127, 0.4197, 0.9664],\n          [0.0559, 0.8831, 0.1578,  ..., 0.0591, 0.5617, 0.3838],\n          [0.5392, 0.7855, 0.7521,  ..., 0.5047, 0.8506, 0.1313],\n          [0.4440, 0.9960, 0.6786,  ..., 0.6453, 0.9042, 0.2527],\n          [0.0823, 0.9433, 0.5384,  ..., 0.6447, 0.9944, 0.5354]],\n \n         [[0.2265, 0.0395, 0.9052,  ..., 0.7869, 0.9466, 0.1153],\n          [0.4234, 0.9881, 0.8599,  ..., 0.6623, 0.9961, 0.3682],\n          [0.9345, 0.5121, 0.8887,  ..., 0.9360, 0.9324, 0.0162],\n          [0.0203, 0.8945, 0.2923,  ..., 0.7833, 0.0689, 0.1614],\n          [0.3560, 0.7624, 0.3965,  ..., 0.0127, 0.9626, 0.8172]],\n \n         [[0.9938, 0.3386, 0.5924,  ..., 0.2472, 0.2415, 0.9198],\n          [0.2426, 0.7178, 0.4649,  ..., 0.9183, 0.8499, 0.5130],\n          [0.0607, 0.0858, 0.6221,  ..., 0.2853, 0.5936, 0.3377],\n          [0.9573, 0.0614, 0.1261,  ..., 0.0328, 0.9800, 0.8054],\n          [0.2125, 0.7493, 0.2588,  ..., 0.9945, 0.9888, 0.1356]],\n \n         ...,\n \n         [[0.0974, 0.0516, 0.0276,  ..., 0.1863, 0.7539, 0.8741],\n          [0.8785, 0.2510, 0.5249,  ..., 0.6668, 0.4847, 0.7100],\n          [0.3333, 0.1275, 0.3714,  ..., 0.2961, 0.6714, 0.5146],\n          [0.3700, 0.0050, 0.7371,  ..., 0.5814, 0.3340, 0.8768],\n          [0.2044, 0.9778, 0.6840,  ..., 0.9703, 0.0027, 0.4351]],\n \n         [[0.8430, 0.5124, 0.4333,  ..., 0.3940, 0.4992, 0.2371],\n          [0.4996, 0.8064, 0.9245,  ..., 0.3415, 0.1951, 0.6976],\n          [0.4762, 0.4601, 0.3726,  ..., 0.4145, 0.6503, 0.1075],\n          [0.6555, 0.5250, 0.1669,  ..., 0.0026, 0.4341, 0.8231],\n          [0.4474, 0.5167, 0.1399,  ..., 0.9505, 0.2093, 0.9665]],\n \n         [[0.1280, 0.9457, 0.1662,  ..., 0.5143, 0.7700, 0.3715],\n          [0.0199, 0.7191, 0.8884,  ..., 0.4384, 0.0192, 0.2718],\n          [0.4964, 0.8469, 0.9123,  ..., 0.2484, 0.9000, 0.3844],\n          [0.4899, 0.4696, 0.3371,  ..., 0.2568, 0.3307, 0.6563],\n          [0.4686, 0.1641, 0.2404,  ..., 0.8990, 0.3863, 0.8734]]]),\n TensorCategory([3, 5, 3, 8, 1, 4, 2, 4, 9, 1, 2, 5, 0, 6, 8, 7, 8, 7, 8, 8, 2, 0, 2, 8,\n         5, 4, 0, 7, 4, 4, 2, 9, 8, 3, 9, 2, 4, 3, 1, 5, 4, 1, 8, 2, 8, 0, 4, 4,\n         6, 4, 1, 4, 7, 4, 1, 5, 2, 7, 9, 5, 3, 7, 7, 4]))\n\n\nThere also en easy way to map any particular sample in a batch to the original dataset and id:\n\ndls = TSDataLoaders.from_dsets(metadatasets.train, metadatasets.valid)\nxb, yb = first(dls.train)\nmappings = dls.train.dataset.mapping_idxs\nfor i, (xbi, ybi) in enumerate(zip(xb, yb)):\n    ds, idx = mappings[i]\n    test_close(dsets[ds][idx][0].data.cpu(), xbi.cpu())\n    test_close(dsets[ds][idx][1].data.cpu(), ybi.cpu())\n\nFor example the 3rd sample in this batch would be:\n\ndls.train.dataset.mapping_idxs[2]\n\narray([ 0, 72], dtype=int32)"
  },
  {
    "objectID": "data.core.html",
    "href": "data.core.html",
    "title": "Data Core",
    "section": "",
    "text": "Main Numpy and Times Series functions used throughout the library.\n\n\nfrom tsai.data.external import get_UCR_data\n\n\ndsid = 'OliveOil'\nX_train, y_train, X_valid, y_valid = get_UCR_data(dsid, on_disk=True, force_download=True)\nX_on_disk, y_on_disk, splits = get_UCR_data(dsid, on_disk=True, return_split=False, force_download=True)\nX_in_memory, y_in_memory, splits = get_UCR_data(dsid, on_disk=False, return_split=False, force_download=True)\ny_tensor = cat2int(y_on_disk)\ny_array = y_tensor.numpy()\n\n\nsource\n\nToNumpyTensor\n\n ToNumpyTensor (enc=None, dec=None, split_idx=None, order=None)\n\nTransforms an object into NumpyTensor\n\nsource\n\n\nNumpyTensor\n\n NumpyTensor (o, dtype=None, device=None, copy=None, requires_grad=False,\n              **kwargs)\n\nReturns a tensor with subclass NumpyTensor that has a show method\n\nsource\n\n\nTSTensor\n\n TSTensor (o, dtype=None, device=None, copy=None, requires_grad=False,\n           **kwargs)\n\nReturns a tensor with subclass TSTensor that has a show method\n\nsource\n\n\nshow_tuple\n\n show_tuple (tup, nrows=1, ncols=1, sharex=False, sharey=False,\n             squeeze=True, width_ratios=None, height_ratios=None,\n             subplot_kw=None, gridspec_kw=None)\n\nDisplay a timeseries plot from a decoded tuple\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ntup\n\n\n\n\n\nnrows\nint\n1\n\n\n\nncols\nint\n1\n\n\n\nsharex\nbool\nFalse\n\n\n\nsharey\nbool\nFalse\n\n\n\nsqueeze\nbool\nTrue\n- If True, extra dimensions are squeezed out from the returned array of ~matplotlib.axes.Axes: - if only one subplot is constructed (nrows=ncols=1), the resulting single Axes object is returned as a scalar. - for Nx1 or 1xM subplots, the returned object is a 1D numpy object array of Axes objects. - for NxM, subplots with N>1 and M>1 are returned as a 2D array.- If False, no squeezing at all is done: the returned Axes object is always a 2D array containing Axes instances, even if it ends up being 1x1.\n\n\nwidth_ratios\nNoneType\nNone\nDefines the relative widths of the columns. Each column gets arelative width of width_ratios[i] / sum(width_ratios).If not given, all columns will have the same width. Equivalentto gridspec_kw={'width_ratios': [...]}.\n\n\nheight_ratios\nNoneType\nNone\nDefines the relative heights of the rows. Each row gets arelative height of height_ratios[i] / sum(height_ratios).If not given, all rows will have the same height. Conveniencefor gridspec_kw={'height_ratios': [...]}.\n\n\nsubplot_kw\nNoneType\nNone\nDict with keywords passed to the~matplotlib.figure.Figure.add_subplot call used to create eachsubplot.\n\n\ngridspec_kw\nNoneType\nNone\nDict with keywords passed to the ~matplotlib.gridspec.GridSpecconstructor used to create the grid the subplots are placed on.\n\n\n\n\nsource\n\n\nToTSTensor\n\n ToTSTensor (enc=None, dec=None, split_idx=None, order=None)\n\nTransforms an object into TSTensor\n\na = np.random.randn(2, 3, 4).astype(np.float16)\nassert np.shares_memory(a, NumpyTensor(a))\nassert np.shares_memory(a, TSTensor(a))\n\n\na = np.random.randn(2, 3, 4).astype(np.float32)\nassert np.shares_memory(a, NumpyTensor(a))\nassert np.shares_memory(a, TSTensor(a))\n\n\na = np.random.randint(10, size=10).astype(np.int64)\nassert np.shares_memory(a, NumpyTensor(a))\nassert np.shares_memory(a, TSTensor(a))\n\n\na = np.random.randint(10, size=10).astype(np.int32)\nassert np.shares_memory(a, NumpyTensor(a))\nassert np.shares_memory(a, TSTensor(a))\n\n\na = torch.rand(2, 3, 4).float()\nassert np.shares_memory(a, NumpyTensor(a))\nassert np.shares_memory(a, TSTensor(a))\n\n\na = torch.randint(3, (10,))\nassert np.shares_memory(a, NumpyTensor(a))\nassert np.shares_memory(a, TSTensor(a))\n\n\nt = TSTensor(torch.randn(2, 3, 4))\np = torch.tensor(3., requires_grad=True)\ntest = torch.add(t, p)\ntest_eq(test.requires_grad, True)\ntest_eq(type(t.data), torch.Tensor)\ntest_eq(type(t), TSTensor)\n\n\nl = L([0,1,2,3], [4,5,6,7], [8, 9, 10, 11])\nTSTensor(l), TSTensor(l).data\n\n(TSTensor(vars:3, len:4, device=cpu, dtype=torch.int64),\n tensor([[ 0,  1,  2,  3],\n         [ 4,  5,  6,  7],\n         [ 8,  9, 10, 11]]))\n\n\n\nt = TSTensor(X_train)\nfor i in range(4):\n    print(t, t.ndim, torch.is_tensor(t))\n    if i < 3: t = t[0]\n\nTSTensor(samples:30, vars:1, len:570, device=cpu, dtype=torch.float32) 3 True\nTSTensor(vars:1, len:570, device=cpu, dtype=torch.float32) 2 True\nTSTensor(len:570, device=cpu, dtype=torch.float32) 1 True\nTSTensor([-0.6113752722740173], device=cpu, dtype=torch.float32) 0 True\n\n\n\nTSTensor(X_on_disk)\n\nTSTensor(samples:60, vars:1, len:570, device=cpu, dtype=torch.float32)\n\n\n\nToTSTensor()(X_on_disk)\n\nTSTensor(samples:60, vars:1, len:570, device=cpu, dtype=torch.float32)\n\n\n\nTSTensor(X_train).show();\n\n\n\n\n\nTSTensor(X_train).show(title='1');\n\n\n\n\n\nshow_tuple((TSTensor(X_train), ['1', '2']))\n\n\n\n\n\nshow_tuple((TSTensor(np.arange(10).reshape(2,5)), 1))\n\n\n\n\n\nshow_tuple((TSTensor(np.arange(10).reshape(2,5)), '1'))\n\n\n\n\n\nshow_tuple((TSTensor(np.arange(10).reshape(2,5)), [1,2]))\n\n\n\n\n\nshow_tuple((TSTensor(np.arange(10).reshape(2,5)), ['1', '2']))\n\n\n\n\n\nsource\n\n\nTSMaskTensor\n\n TSMaskTensor (o, dtype=None, device=None, copy=None, requires_grad=False,\n               **kwargs)\n\nReturns a tensor with subclass NumpyTensor that has a show method\n\nsource\n\n\nTSLabelTensor\n\n TSLabelTensor (o, dtype=None, device=None, copy=None,\n                requires_grad=False, **kwargs)\n\nReturns a tensor with subclass NumpyTensor that has a show method\n\nt = TSLabelTensor(torch.randint(0,10,(1, 2, 3)))\nt, t[0], t[0][0], t[0][0][0]\n\n(TSLabelTensor(shape:(1, 2, 3), device=cpu, dtype=torch.int64),\n TSLabelTensor(shape:(2, 3), device=cpu, dtype=torch.int64),\n TSLabelTensor(shape:(3,), device=cpu, dtype=torch.int64),\n 0)\n\n\n\nt = TSMaskTensor(torch.randint(0,10,(1, 2, 3)))\nt, t[0], t[0][0], t[0][0][0]\n\n(TSMaskTensor(shape:(1, 2, 3), device=cpu, dtype=torch.int64),\n TSMaskTensor(shape:(2, 3), device=cpu, dtype=torch.int64),\n TSMaskTensor(shape:(3,), device=cpu, dtype=torch.int64),\n 8)\n\n\n\nsource\n\n\nTSClassification\n\n TSClassification (vocab=None, sort=True)\n\nVectorized, reversible transform of category string to vocab id\n\nsource\n\n\nToInt\n\n ToInt (enc=None, dec=None, split_idx=None, order=None)\n\nTransforms an object dtype to int\n\nsource\n\n\nToFloat\n\n ToFloat (enc=None, dec=None, split_idx=None, order=None)\n\nTransforms an object dtype to float (vectorized)\n\na = np.random.randint(0, 2, 10)\nb = np.array(['1', '2', '3'])\nc = np.array(['1.0', '2.0', '3.0'])\nt = torch.randint(0, 2, (10, ))\ntest_eq(ToFloat()(a).dtype, 'float32')\ntest_eq(ToFloat()(b).dtype, 'float32')\ntest_eq(ToFloat()(c).dtype, 'float32')\ntest_eq(ToFloat()(t).dtype, torch.float32)\n\n\na = np.random.rand(10)*10\nb = np.array(['1.0', '2.0', '3.0'])\nt = torch.rand(10)*10\ntest_eq(ToInt()(a).dtype, 'int64')\ntest_eq(ToInt()(b).dtype, 'int64')\ntest_eq(ToInt()(t).dtype, torch.long)\n\n\nt = TSClassification()\nt.setup(y_on_disk[splits[0]])\ny_encoded = t(y_on_disk)\nprint(y_encoded)\ntest_eq(t.decodes(y_encoded), y_on_disk)\n\nTensorCategory([0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 3, 3, 3, 3,\n                3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1,\n                1, 1, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3])\n\n\n\ny_multi= np.random.randint(0,3,20)\ny_multi = np.asarray(alphabet[y_multi]).reshape(4,5)\ntfm = TSClassification()\ntfm.setup(y_multi)\nenc_y_multi = tfm(y_multi)\ntest_eq(y_multi, tfm.decode(enc_y_multi))\nenc_y_multi\n\nTensorCategory([[0, 0, 2, 0, 2],\n                [2, 0, 1, 2, 0],\n                [2, 2, 2, 2, 2],\n                [2, 1, 2, 0, 1]])\n\n\n\nsource\n\n\nTSMultiLabelClassification\n\n TSMultiLabelClassification (c=None, vocab=None, add_na=False, sort=True)\n\nReversible combined transform of multi-category strings to one-hot encoded vocab id\n\nsource\n\n\nTSTensorBlock\n\n TSTensorBlock (type_tfms=None, item_tfms=None, batch_tfms=None,\n                dl_type=None, dls_kwargs=None)\n\nInitialize self. See help(type(self)) for accurate signature.\n\nsource\n\n\nNumpyTensorBlock\n\n NumpyTensorBlock (type_tfms=None, item_tfms=None, batch_tfms=None,\n                   dl_type=None, dls_kwargs=None)\n\nInitialize self. See help(type(self)) for accurate signature.\n\ntest_eq(NumpyTensorBlock().item_tfms[0].__name__, 'ToNumpyTensor')\ntest_eq(TSTensorBlock().item_tfms[0].__name__, 'ToTSTensor')\n\n\nsource\n\n\nTSDataset\n\n TSDataset (X, y=None, split=None, sel_vars=None, sel_steps=None,\n            types=None, dtype=None, device=None)\n\nInitialize self. See help(type(self)) for accurate signature.\n\nsource\n\n\nNumpyDataset\n\n NumpyDataset (X, y=None, types=None)\n\nInitialize self. See help(type(self)) for accurate signature.\n\nsource\n\n\nTorchDataset\n\n TorchDataset (X, y=None)\n\nInitialize self. See help(type(self)) for accurate signature.\n\na = np.random.rand(5,6,7)\nb = np.random.rand(5)\nds = NumpyDataset(a,b)\nxb, yb = ds[[0,4]]\ntest_eq(xb.shape, (2,6,7))\ntest_eq(yb.shape, (2,))\n\n\nsource\n\n\nTSTfmdLists\n\n TSTfmdLists (items=None, *rest, use_list=False, match=None)\n\nA Pipeline of tfms applied to a collection of items\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nitems\nlist\n\nItems to apply Transforms to\n\n\nuse_list\nbool\nNone\nUse list in L\n\n\n\n\nsource\n\n\nNoTfmLists\n\n NoTfmLists (items=None, *rest, use_list=False, match=None)\n\nA Pipeline of tfms applied to a collection of items\n\nitems = X_on_disk\ntl = TfmdLists(items, tfms=None, splits=splits)\ntest_eq(len(tl), len(X_on_disk))\ntest_eq(len(tl.train), len(splits[0]))\ntest_eq(len(tl.valid), len(splits[1]))\ntest_eq(tl[[0,4,7]], X_on_disk[[0,4,7]])\ntest_eq(tl.train[[0,4,7]], X_on_disk[splits[0][0,4,7]])\ntest_eq(tl.valid[[0,4,7]], X_on_disk[splits[1][0,4,7]])\ntest_eq(tl[0], items[0])\ntest_eq(tl[[0,1]], items[[0,1]])\ntest_eq(tl.decode(tl[0]), tl[0])\ntest_eq((tl.split_idx, tl.train.split_idx, tl.valid.split_idx), (None, 0, 1))\n\n\nitems = X_on_disk\ntl = TSTfmdLists(items, tfms=None, splits=splits)\ntest_eq(len(tl), len(X_on_disk))\ntest_eq(len(tl.train), len(splits[0]))\ntest_eq(len(tl.valid), len(splits[1]))\ntest_eq(tl[[0,4,7]], X_on_disk[[0,4,7]])\ntest_eq(tl.train[[0,4,7]], X_on_disk[splits[0][0,4,7]])\ntest_eq(tl.valid[[0,4,7]], X_on_disk[splits[1][0,4,7]])\ntest_eq(tl[0], items[0])\ntest_eq(tl[[0,1]], items[[0,1]])\ntest_eq(tl.decode(tl[0]), tl[0])\ntest_eq((tl.split_idx, tl.train.split_idx, tl.valid.split_idx), (None, 0, 1))\n\n\nitems = X_on_disk\nntl = NoTfmLists(items, splits=splits)\ntest_eq(len(ntl), len(X_on_disk))\ntest_eq(len(ntl.train), len(splits[0]))\ntest_eq(len(ntl.valid), len(splits[1]))\ntest_eq(ntl._splits, np.arange(len(X_on_disk)))\ntest_eq(ntl.train._splits, np.arange(len(splits[0])))\ntest_eq(ntl.valid._splits, np.arange(len(splits[0]), len(X_on_disk)))\nprint(ntl)\nprint(ntl.train)\nprint(ntl.valid)\ntest_eq(ntl[[0,4,7]], X_on_disk[[0,4,7]])\ntest_eq(ntl.train[[0,4,7]], X_on_disk[splits[0][0,4,7]])\ntest_eq(ntl.valid[[0,4,7]], X_on_disk[splits[1][0,4,7]])\ntest_eq(ntl[0], items[0])\ntest_eq(ntl[[0,1]], items[[0,1]])\ntest_eq(ntl[:], X_on_disk)\nntl[0].shape, stack(ntl[[0,1]]).shape\ntest_eq(ntl.decode(ntl[0]), ntl[0])\nassert id(items) == id(ntl.items) == id(ntl.train.items) == id(ntl.valid.items)\ntest_eq((ntl.split_idx, ntl.train.split_idx, ntl.valid.split_idx), (None, 0, 1))\n\nNoTfmLists: memmap(60, 1, 570)\nNoTfmLists: memmap(30, 1, 570)\nNoTfmLists: memmap(30, 1, 570)\n\n\n\nsubitems = X_on_disk\nnew_ntl = ntl._new(X_on_disk)\ntest_eq(new_ntl[:], X_on_disk)\n\n\nidxs = random_choice(len(X_on_disk), 10, False)\nnew_ntl = ntl._new(X_on_disk[idxs])\ntest_eq(new_ntl[:], X_on_disk[idxs])\n\n\nidxs = random_choice(len(X_on_disk), 10, False)\nnew_ntl = ntl.valid._new(X_on_disk[idxs])\ntest_eq(new_ntl[:], X_on_disk[idxs])\n\n\nsource\n\n\ntscoll_repr\n\n tscoll_repr (c, max_n=10)\n\nString repr of up to max_n items of (possibly lazy) collection c\n\nsource\n\n\nNumpyDatasets\n\n NumpyDatasets (items:list=None, tfms:MutableSequence|Pipeline=None,\n                tls:TfmdLists=None, n_inp:int=None, dl_type=None,\n                use_list:bool=None, do_setup:bool=True,\n                split_idx:int=None, train_setup:bool=True,\n                splits:list=None, types=None, verbose:bool=False)\n\nA dataset that creates tuples from X (and y) and applies tfms of type item_tfms\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nitems\nlist\n\nItems to apply Transforms to\n\n\ntfms\nMutableSequence | Pipeline\n\nTransform(s) or Pipeline to apply\n\n\ntls\nNoneType\nNone\n\n\n\nn_inp\nNoneType\nNone\n\n\n\ndl_type\nTfmdDL\nNone\nType of DataLoader\n\n\nuse_list\nbool\nNone\nUse list in L\n\n\ndo_setup\nbool\nTrue\nCall setup() for Transform\n\n\nsplit_idx\nint\nNone\nApply Transform(s) to training or validation set. 0 for training set and 1 for validation set\n\n\ntrain_setup\nbool\nTrue\nApply Transform(s) only on training DataLoader\n\n\nsplits\nlist\nNone\nIndices for training and validation sets\n\n\ntypes\nNoneType\nNone\nTypes of data in items\n\n\nverbose\nbool\nFalse\nPrint verbose output\n\n\n\n\nsource\n\n\nTSDatasets\n\n TSDatasets (items:list=None, tfms:MutableSequence|Pipeline=None,\n             tls:TfmdLists=None, n_inp:int=None, dl_type=None,\n             use_list:bool=None, do_setup:bool=True, split_idx:int=None,\n             train_setup:bool=True, splits:list=None, types=None,\n             verbose:bool=False)\n\nA dataset that creates tuples from X (and optionally y) and applies item_tfms\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nitems\nlist\n\nItems to apply Transforms to\n\n\ntfms\nMutableSequence | Pipeline\n\nTransform(s) or Pipeline to apply\n\n\ntls\nNoneType\nNone\n\n\n\nn_inp\nNoneType\nNone\n\n\n\ndl_type\nTfmdDL\nNone\nType of DataLoader\n\n\nuse_list\nbool\nNone\nUse list in L\n\n\ndo_setup\nbool\nTrue\nCall setup() for Transform\n\n\nsplit_idx\nint\nNone\nApply Transform(s) to training or validation set. 0 for training set and 1 for validation set\n\n\ntrain_setup\nbool\nTrue\nApply Transform(s) only on training DataLoader\n\n\nsplits\nlist\nNone\nIndices for training and validation sets\n\n\ntypes\nNoneType\nNone\nTypes of data in items\n\n\nverbose\nbool\nFalse\nPrint verbose output\n\n\n\n\ndsets = TSDatasets(X_on_disk, y_on_disk, splits=splits, tfms=[None, TSClassification()], inplace=True)\ni = random_choice(len(splits[0]), 10, False).tolist()\ntest_eq(dsets.subset(i), dsets.train.subset(i))\ndsets.valid.subset(i)\ndsets.valid.subset(i)[[0,6,8]]\ntest_eq(dsets.subset(i)[[0,6,8]], dsets.train.subset(i)[[0,6,8]])\ndsets.subset([0,7,3])\ndsets.subset(i), dsets.train.subset(i), dsets.valid.subset(i)\n\n((#10) [(TSTensor(vars:1, len:570, device=cpu, dtype=torch.float32), TensorCategory(0)), (TSTensor(vars:1, len:570, device=cpu, dtype=torch.float32), TensorCategory(3)), (TSTensor(vars:1, len:570, device=cpu, dtype=torch.float32), TensorCategory(1)), (TSTensor(vars:1, len:570, device=cpu, dtype=torch.float32), TensorCategory(1)), (TSTensor(vars:1, len:570, device=cpu, dtype=torch.float32), TensorCategory(3)), (TSTensor(vars:1, len:570, device=cpu, dtype=torch.float32), TensorCategory(3)), (TSTensor(vars:1, len:570, device=cpu, dtype=torch.float32), TensorCategory(0)), (TSTensor(vars:1, len:570, device=cpu, dtype=torch.float32), TensorCategory(3)), (TSTensor(vars:1, len:570, device=cpu, dtype=torch.float32), TensorCategory(1)), (TSTensor(vars:1, len:570, device=cpu, dtype=torch.float32), TensorCategory(3))] ...],\n (#10) [(TSTensor(vars:1, len:570, device=cpu, dtype=torch.float32), TensorCategory(0)), (TSTensor(vars:1, len:570, device=cpu, dtype=torch.float32), TensorCategory(3)), (TSTensor(vars:1, len:570, device=cpu, dtype=torch.float32), TensorCategory(1)), (TSTensor(vars:1, len:570, device=cpu, dtype=torch.float32), TensorCategory(1)), (TSTensor(vars:1, len:570, device=cpu, dtype=torch.float32), TensorCategory(3)), (TSTensor(vars:1, len:570, device=cpu, dtype=torch.float32), TensorCategory(3)), (TSTensor(vars:1, len:570, device=cpu, dtype=torch.float32), TensorCategory(0)), (TSTensor(vars:1, len:570, device=cpu, dtype=torch.float32), TensorCategory(3)), (TSTensor(vars:1, len:570, device=cpu, dtype=torch.float32), TensorCategory(1)), (TSTensor(vars:1, len:570, device=cpu, dtype=torch.float32), TensorCategory(3))] ...],\n (#10) [(TSTensor(vars:1, len:570, device=cpu, dtype=torch.float32), TensorCategory(0)), (TSTensor(vars:1, len:570, device=cpu, dtype=torch.float32), TensorCategory(3)), (TSTensor(vars:1, len:570, device=cpu, dtype=torch.float32), TensorCategory(1)), (TSTensor(vars:1, len:570, device=cpu, dtype=torch.float32), TensorCategory(1)), (TSTensor(vars:1, len:570, device=cpu, dtype=torch.float32), TensorCategory(3)), (TSTensor(vars:1, len:570, device=cpu, dtype=torch.float32), TensorCategory(3)), (TSTensor(vars:1, len:570, device=cpu, dtype=torch.float32), TensorCategory(0)), (TSTensor(vars:1, len:570, device=cpu, dtype=torch.float32), TensorCategory(3)), (TSTensor(vars:1, len:570, device=cpu, dtype=torch.float32), TensorCategory(1)), (TSTensor(vars:1, len:570, device=cpu, dtype=torch.float32), TensorCategory(3))] ...])\n\n\n\ntfms = [None, TSClassification()]\ndsets = TSDatasets(X_on_disk, y_on_disk, splits=splits, tfms=tfms, inplace=False)\nassert id(X_on_disk) == id(dsets.ptls[0].items) == id(dsets.train.ptls[0].items) == id(dsets.valid.ptls[0].items)\n\ntfms = None\ndsets = TSDatasets(X_on_disk, splits=splits, tfms=tfms, inplace=False)\nassert id(X_on_disk) == id(dsets.ptls[0].items) == id(dsets.train.ptls[0].items) == id(dsets.valid.ptls[0].items)\n\n\nsource\n\n\nNumpyDatasets.add_unlabeled\n\n NumpyDatasets.add_unlabeled (X, inplace=True)\n\n\nsource\n\n\nNumpyDatasets.add_test\n\n NumpyDatasets.add_test (X, y=None, inplace=True)\n\n\nsource\n\n\nNumpyDatasets.add_dataset\n\n NumpyDatasets.add_dataset (X, y=None, inplace=True)\n\n\nsource\n\n\nadd_ds\n\n add_ds (dsets, X, y=None, inplace=True)\n\nCreate test datasets from X (and y) using validation transforms of dsets\n\ndsets = TSDatasets(X_on_disk, y_on_disk, splits=splits, tfms=[None, TSClassification()], inplace=True)\nprint(dsets.train[0][0].shape, dsets.train[[0,1]][0].shape)\nprint(dsets.split_idx, dsets.train.split_idx, dsets.valid.split_idx)\nprint(dsets.new_empty())\ndsets\n\ntorch.Size([1, 570]) torch.Size([2, 1, 570])\nNone 0 1\n(#0) []\n\n\n(#60) [(TSTensor(vars:1, len:570, device=cpu, dtype=torch.float32), TensorCategory(0)), (TSTensor(vars:1, len:570, device=cpu, dtype=torch.float32), TensorCategory(0)), (TSTensor(vars:1, len:570, device=cpu, dtype=torch.float32), TensorCategory(0)), (TSTensor(vars:1, len:570, device=cpu, dtype=torch.float32), TensorCategory(0)), (TSTensor(vars:1, len:570, device=cpu, dtype=torch.float32), TensorCategory(0)), (TSTensor(vars:1, len:570, device=cpu, dtype=torch.float32), TensorCategory(1)), (TSTensor(vars:1, len:570, device=cpu, dtype=torch.float32), TensorCategory(1)), (TSTensor(vars:1, len:570, device=cpu, dtype=torch.float32), TensorCategory(1)), (TSTensor(vars:1, len:570, device=cpu, dtype=torch.float32), TensorCategory(1)), (TSTensor(vars:1, len:570, device=cpu, dtype=torch.float32), TensorCategory(1))] ...]\n\n\n\ndsets = TSDatasets(X_on_disk, y_on_disk, splits=splits, tfms=[None, TSClassification()], inplace=False)\nprint(dsets.train[0][0].shape, dsets.train[[0,1]][0].shape)\nprint(dsets.split_idx, dsets.train.split_idx, dsets.valid.split_idx)\nprint(dsets.new_empty())\ndsets\n\ntorch.Size([1, 570]) torch.Size([2, 1, 570])\nNone 0 1\n(#0) []\n\n\n(#60) [(TSTensor(vars:1, len:570, device=cpu, dtype=torch.float32), TensorCategory([0])), (TSTensor(vars:1, len:570, device=cpu, dtype=torch.float32), TensorCategory([0])), (TSTensor(vars:1, len:570, device=cpu, dtype=torch.float32), TensorCategory([0])), (TSTensor(vars:1, len:570, device=cpu, dtype=torch.float32), TensorCategory([0])), (TSTensor(vars:1, len:570, device=cpu, dtype=torch.float32), TensorCategory([0])), (TSTensor(vars:1, len:570, device=cpu, dtype=torch.float32), TensorCategory([1])), (TSTensor(vars:1, len:570, device=cpu, dtype=torch.float32), TensorCategory([1])), (TSTensor(vars:1, len:570, device=cpu, dtype=torch.float32), TensorCategory([1])), (TSTensor(vars:1, len:570, device=cpu, dtype=torch.float32), TensorCategory([1])), (TSTensor(vars:1, len:570, device=cpu, dtype=torch.float32), TensorCategory([1]))] ...]\n\n\n\ndsets = TSDatasets(X_on_disk, y_on_disk, tfms=[None, TSClassification()], splits=splits, inplace=True)\n\nidxs = random_choice(len(dsets), 10, False)\ntest_eq(dsets[idxs][0].numpy(), X_on_disk[idxs])\ntest_eq(dsets[idxs][1].numpy(), y_array[idxs])\n\nidxs = random_choice(len(dsets.train), 10, False)\ntest_eq(dsets.train[idxs][0].numpy(), X_on_disk[splits[0][idxs]])\ntest_eq(dsets.train[idxs][1].numpy(), y_array[splits[0][idxs]])\n\nidxs = random_choice(len(dsets.valid), 10, False)\ntest_eq(dsets.valid[idxs][0].numpy(), X_on_disk[splits[1][idxs]])\ntest_eq(dsets.valid[idxs][1].numpy(), y_array[splits[1][idxs]])\n\n\ndsets = TSDatasets(X_on_disk, y_on_disk, tfms=[None, TSClassification()], splits=splits, inplace=False)\nassert id(X_on_disk) == id(dsets.tls[0].items) == id(dsets.ptls[0].items)\nassert id(X_on_disk) == id(dsets.train.tls[0].items) == id(dsets.train.ptls[0].items)\nassert id(X_on_disk) == id(dsets.valid.tls[0].items) == id(dsets.valid.ptls[0].items)\n\nidxs = random_choice(len(dsets), 10, False)\ntest_eq(dsets[idxs][0].numpy(), X_on_disk[idxs])\ntest_eq(dsets[idxs][1].numpy(), y_array[idxs])\n\n\nidxs = random_choice(len(dsets.train), 10, False)\ntest_eq(dsets.train[idxs][0].numpy(), X_on_disk[splits[0][idxs]])\ntest_eq(dsets.train[idxs][1].numpy(), y_array[splits[0][idxs]])\n\nidxs = random_choice(len(dsets.valid), 10, False)\ntest_eq(dsets.valid[idxs][0].numpy(), X_on_disk[splits[1][idxs]])\ntest_eq(dsets.valid[idxs][1].numpy(), y_array[splits[1][idxs]])\n\n\ndsets = TSDatasets(X_on_disk, splits=splits, inplace=True)\n\nidxs = random_choice(len(dsets), 10, False)\ntest_eq(dsets[idxs][0].numpy(), X_on_disk[idxs])\n\nidxs = random_choice(len(dsets.train), 10, False)\ntest_eq(dsets.train[idxs][0].numpy(), X_on_disk[splits[0][idxs]])\n\nidxs = random_choice(len(dsets.valid), 10, False)\ntest_eq(dsets.valid[idxs][0].numpy(), X_on_disk[splits[1][idxs]])\n\n\ndsets = TSDatasets(X_on_disk, splits=splits, inplace=False)\nassert np.shares_memory(X_on_disk, dsets.tls[0].items) \nassert np.shares_memory(X_on_disk, dsets.ptls[0].items)\nassert np.shares_memory(X_on_disk, dsets.train.tls[0].items) \nassert np.shares_memory(X_on_disk, dsets.train.ptls[0].items)\nassert np.shares_memory(X_on_disk, dsets.valid.tls[0].items)\nassert np.shares_memory(X_on_disk, dsets.valid.ptls[0].items)\n\nidxs = random_choice(len(dsets), 10, False)\ntest_eq(dsets[idxs][0].numpy(), X_on_disk[idxs])\n\nidxs = random_choice(len(dsets.train), 10, False)\ntest_eq(dsets.train[idxs][0].numpy(), X_on_disk[splits[0][idxs]])\n\nidxs = random_choice(len(dsets.valid), 10, False)\ntest_eq(dsets.valid[idxs][0].numpy(), X_on_disk[splits[1][idxs]])\n\n\ndsets = TSDatasets(X_on_disk, y_array, tfms=None, splits=splits, inplace=True)\n\nidxs = random_choice(len(dsets), 10, False)\ntest_eq(dsets[idxs][0].numpy(), X_on_disk[idxs])\ntest_eq(dsets[idxs][1].numpy(), y_array[idxs])\n\nidxs = random_choice(len(dsets.train), 10, False)\ntest_eq(dsets.train[idxs][0].numpy(), X_on_disk[splits[0][idxs]])\ntest_eq(dsets.train[idxs][1].numpy(), y_array[splits[0][idxs]])\n\nidxs = random_choice(len(dsets.valid), 10, False)\ntest_eq(dsets.valid[idxs][0].numpy(), X_on_disk[splits[1][idxs]])\ntest_eq(dsets.valid[idxs][1].numpy(), y_array[splits[1][idxs]])\n\n\ndsets = TSDatasets(X_on_disk, y_array, tfms=None, splits=splits, inplace=False)\nassert np.shares_memory(X_on_disk, dsets.tls[0].items) \nassert np.shares_memory(X_on_disk, dsets.ptls[0].items)\nassert np.shares_memory(X_on_disk, dsets.train.tls[0].items) \nassert np.shares_memory(X_on_disk, dsets.train.ptls[0].items)\nassert np.shares_memory(X_on_disk, dsets.valid.tls[0].items) \nassert np.shares_memory(X_on_disk, dsets.valid.ptls[0].items)\n\nidxs = random_choice(len(dsets), 10, False)\ntest_eq(dsets[idxs][0].numpy(), X_on_disk[idxs])\ntest_eq(dsets[idxs][1].numpy(), y_array[idxs])\n\nidxs = random_choice(len(dsets.train), 10, False)\ntest_eq(dsets.train[idxs][0].numpy(), X_on_disk[splits[0][idxs]])\ntest_eq(dsets.train[idxs][1].numpy(), y_array[splits[0][idxs]])\n\nidxs = random_choice(len(dsets.valid), 10, False)\ntest_eq(dsets.valid[idxs][0].numpy(), X_on_disk[splits[1][idxs]])\ntest_eq(dsets.valid[idxs][1].numpy(), y_array[splits[1][idxs]])\n\n\ndsets = TSDatasets(X_on_disk, y_on_disk, tfms=[None, TSClassification()], splits=None, inplace=True)\n\nidxs = random_choice(len(dsets), 10, False)\ntest_eq(dsets[idxs][0].numpy(), X_on_disk[idxs])\ntest_eq(dsets[idxs][1].numpy(), y_array[idxs])\n\n\ndsets = TSDatasets(X_on_disk, y_on_disk, tfms=[None, TSClassification()], splits=None, inplace=False)\nassert id(X_on_disk) == id(dsets.tls[0].items) == id(dsets.ptls[0].items)\nassert id(X_on_disk) == id(dsets.train.tls[0].items) == id(dsets.train.ptls[0].items)\n\nidxs = random_choice(len(dsets), 10, False)\ntest_eq(dsets[idxs][0].numpy(), X_on_disk[idxs])\ntest_eq(dsets[idxs][1].numpy(), y_array[idxs])\n\n\ndsets = TSDatasets(X_on_disk, y_array, tfms=None, splits=splits)\ntest_eq(dsets.train[0:10], dsets.add_dataset(X_on_disk[0:10], y_array[0:10])[:])\ntest_eq(dsets.train[0:10][0], dsets.add_dataset(X_on_disk[0:10])[:][0])\n\n\ndsets = TSDatasets(X_on_disk, y_array, tfms=None, splits=splits)\ntorch.save(dsets, 'export/dsets.pth')\ndel dsets\ndsets = torch.load('export/dsets.pth')\ndsets\n\n(#60) [(TSTensor(vars:1, len:570, device=cpu, dtype=torch.float32), tensor(0)), (TSTensor(vars:1, len:570, device=cpu, dtype=torch.float32), tensor(0)), (TSTensor(vars:1, len:570, device=cpu, dtype=torch.float32), tensor(0)), (TSTensor(vars:1, len:570, device=cpu, dtype=torch.float32), tensor(0)), (TSTensor(vars:1, len:570, device=cpu, dtype=torch.float32), tensor(0)), (TSTensor(vars:1, len:570, device=cpu, dtype=torch.float32), tensor(1)), (TSTensor(vars:1, len:570, device=cpu, dtype=torch.float32), tensor(1)), (TSTensor(vars:1, len:570, device=cpu, dtype=torch.float32), tensor(1)), (TSTensor(vars:1, len:570, device=cpu, dtype=torch.float32), tensor(1)), (TSTensor(vars:1, len:570, device=cpu, dtype=torch.float32), tensor(1))] ...]\n\n\n\ndsets = TSDatasets(X_on_disk, y_array, tfms=None, splits=splits)\ntorch.save(dsets.train, 'export/dsets.pth')\ndel dsets\ndsets = torch.load('export/dsets.pth')\ndsets\n\n(#30) [(TSTensor(vars:1, len:570, device=cpu, dtype=torch.float32), tensor(0)), (TSTensor(vars:1, len:570, device=cpu, dtype=torch.float32), tensor(0)), (TSTensor(vars:1, len:570, device=cpu, dtype=torch.float32), tensor(0)), (TSTensor(vars:1, len:570, device=cpu, dtype=torch.float32), tensor(0)), (TSTensor(vars:1, len:570, device=cpu, dtype=torch.float32), tensor(0)), (TSTensor(vars:1, len:570, device=cpu, dtype=torch.float32), tensor(1)), (TSTensor(vars:1, len:570, device=cpu, dtype=torch.float32), tensor(1)), (TSTensor(vars:1, len:570, device=cpu, dtype=torch.float32), tensor(1)), (TSTensor(vars:1, len:570, device=cpu, dtype=torch.float32), tensor(1)), (TSTensor(vars:1, len:570, device=cpu, dtype=torch.float32), tensor(1))] ...]\n\n\n\ndsets = TSDatasets(X_on_disk, y_array, tfms=None, splits=splits)\ntest_eq(len(dsets.train), len(X_train))\ndsets = TSDatasets(X_on_disk, y_array, tfms=None, splits=splits)\ntest_eq(len(dsets.train), len(X_train))\ndsets = TSDatasets(X_on_disk, y_array, tfms=[add(1), TSCategorize()], splits=splits)\ntest_eq(len(dsets.train), len(X_train))\n# test_eq(dsets.train[0][0].data, tensor(X_train[0] + 1))\ntest_eq(dsets.train[0][1].item(), y_tensor[0])\n\n\ndsets = TSDatasets(X_on_disk, y_on_disk, tfms=[None, TSCategorize()], splits=splits)\ntest_eq(len(dsets.add_test(X_train, y_train)), len(X_train))\ntest_eq(len(dsets.add_unlabeled(X_train)), len(X_train))\n\n\nsource\n\n\nTSDataLoader\n\n TSDataLoader (dataset, bs=64, shuffle=False, drop_last=False,\n               num_workers=0, verbose=False, do_setup=True, vocab=None,\n               sort=False, weights=None, partial_n=None, sampler=None,\n               pin_memory=False, timeout=0, batch_size=None, indexed=None,\n               n=None, device=None, persistent_workers=False,\n               pin_memory_device='', wif=None, before_iter=None,\n               after_item=None, before_batch=None, after_batch=None,\n               after_iter=None, create_batches=None, create_item=None,\n               create_batch=None, retain=None, get_idxs=None, sample=None,\n               shuffle_fn=None, do_batch=None)\n\nTransformed DataLoader\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ndataset\n\n\nMap- or iterable-style dataset from which to load the data\n\n\nbs\nint\n64\nSize of batch\n\n\nshuffle\nbool\nFalse\nWhether to shuffle data\n\n\ndrop_last\nbool\nFalse\n\n\n\nnum_workers\nint\nNone\nNumber of CPU cores to use in parallel (default: All available up to 16)\n\n\nverbose\nbool\nFalse\nWhether to print verbose logs\n\n\ndo_setup\nbool\nTrue\nWhether to run setup() for batch transform(s)\n\n\nvocab\nNoneType\nNone\n\n\n\nsort\nbool\nFalse\n\n\n\nweights\nNoneType\nNone\n\n\n\npartial_n\nNoneType\nNone\n\n\n\nsampler\nNoneType\nNone\n\n\n\npin_memory\nbool\nFalse\n\n\n\ntimeout\nint\n0\n\n\n\nbatch_size\nNoneType\nNone\n\n\n\nindexed\nNoneType\nNone\n\n\n\nn\nNoneType\nNone\n\n\n\ndevice\nNoneType\nNone\n\n\n\npersistent_workers\nbool\nFalse\n\n\n\npin_memory_device\nstr\n\n\n\n\nwif\nNoneType\nNone\n\n\n\nbefore_iter\nNoneType\nNone\n\n\n\nafter_item\nNoneType\nNone\n\n\n\nbefore_batch\nNoneType\nNone\n\n\n\nafter_batch\nNoneType\nNone\n\n\n\nafter_iter\nNoneType\nNone\n\n\n\ncreate_batches\nNoneType\nNone\n\n\n\ncreate_item\nNoneType\nNone\n\n\n\ncreate_batch\nNoneType\nNone\n\n\n\nretain\nNoneType\nNone\n\n\n\nget_idxs\nNoneType\nNone\n\n\n\nsample\nNoneType\nNone\n\n\n\nshuffle_fn\nNoneType\nNone\n\n\n\ndo_batch\nNoneType\nNone\n\n\n\n\n\nsource\n\n\nNumpyDataLoader\n\n NumpyDataLoader (dataset, bs=64, shuffle=False, drop_last=False,\n                  num_workers=0, verbose=False, do_setup=True, vocab=None,\n                  sort=False, weights=None, partial_n=None, sampler=None,\n                  pin_memory=False, timeout=0, batch_size=None,\n                  indexed=None, n=None, device=None,\n                  persistent_workers=False, pin_memory_device='',\n                  wif=None, before_iter=None, after_item=None,\n                  before_batch=None, after_batch=None, after_iter=None,\n                  create_batches=None, create_item=None,\n                  create_batch=None, retain=None, get_idxs=None,\n                  sample=None, shuffle_fn=None, do_batch=None)\n\nTransformed DataLoader\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ndataset\n\n\nMap- or iterable-style dataset from which to load the data\n\n\nbs\nint\n64\nSize of batch\n\n\nshuffle\nbool\nFalse\nWhether to shuffle data\n\n\ndrop_last\nbool\nFalse\n\n\n\nnum_workers\nint\nNone\nNumber of CPU cores to use in parallel (default: All available up to 16)\n\n\nverbose\nbool\nFalse\nWhether to print verbose logs\n\n\ndo_setup\nbool\nTrue\nWhether to run setup() for batch transform(s)\n\n\nvocab\nNoneType\nNone\n\n\n\nsort\nbool\nFalse\n\n\n\nweights\nNoneType\nNone\n\n\n\npartial_n\nNoneType\nNone\n\n\n\nsampler\nNoneType\nNone\n\n\n\npin_memory\nbool\nFalse\n\n\n\ntimeout\nint\n0\n\n\n\nbatch_size\nNoneType\nNone\n\n\n\nindexed\nNoneType\nNone\n\n\n\nn\nNoneType\nNone\n\n\n\ndevice\nNoneType\nNone\n\n\n\npersistent_workers\nbool\nFalse\n\n\n\npin_memory_device\nstr\n\n\n\n\nwif\nNoneType\nNone\n\n\n\nbefore_iter\nNoneType\nNone\n\n\n\nafter_item\nNoneType\nNone\n\n\n\nbefore_batch\nNoneType\nNone\n\n\n\nafter_batch\nNoneType\nNone\n\n\n\nafter_iter\nNoneType\nNone\n\n\n\ncreate_batches\nNoneType\nNone\n\n\n\ncreate_item\nNoneType\nNone\n\n\n\ncreate_batch\nNoneType\nNone\n\n\n\nretain\nNoneType\nNone\n\n\n\nget_idxs\nNoneType\nNone\n\n\n\nsample\nNoneType\nNone\n\n\n\nshuffle_fn\nNoneType\nNone\n\n\n\ndo_batch\nNoneType\nNone\n\n\n\n\n\nsource\n\n\nTSDataLoaders\n\n TSDataLoaders (*loaders, path='.', device=None)\n\nBasic wrapper around several DataLoaders.\n\nsource\n\n\nNumpyDataLoaders\n\n NumpyDataLoaders (*loaders, path='.', device=None)\n\nBasic wrapper around several DataLoaders.\n\nsource\n\n\nStratifiedSampler\n\n StratifiedSampler (y, bs:int=64, shuffle:bool=False,\n                    drop_last:bool=False)\n\nSampler where batches preserve the percentage of samples for each class\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ny\n\n\nThe target variable for supervised learning problems. Stratification is done based on the y labels.\n\n\nbs\nint\n64\nBatch size\n\n\nshuffle\nbool\nFalse\nFlag to shuffle each class’s samples before splitting into batches.\n\n\ndrop_last\nbool\nFalse\nFlag to drop the last incomplete batch.\n\n\n\n\na = np.concatenate([np.zeros(90), np.ones(10)])\nsampler = StratifiedSampler(a, bs=32, shuffle=True, drop_last=True)\nidxs = np.array(list(iter(sampler)))\nprint(idxs[:32])\nprint(a[idxs][:32])\ntest_eq(a[idxs][:32].mean(), .1)\n\n[[ 3 13 20 23 24 27 34 35 41 43 44 46 47 53 54 56 60 62 65 69 70 71 72 75\n  76 77 80 82 86 89 94 95 97 98  0  1  2  4  6  7  9 11 14 16 18 21 29 36\n  38 39 40 48 55 57 58 59 61 64 66 67 74 79 84 88 91 92 96  5  8 10 12 15\n  17 19 22 25 26 28 30 31 32 33 37 42 45 49 50 51 52 63 68 73 78 81 83 85\n  87 90 93 99]]\n[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n  0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 0. 0. 0. 0. 0.\n  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n  0. 1. 1. 1.]]\n\n\n\nsource\n\n\nget_c\n\n get_c (dls)\n\n\nsource\n\n\nget_best_dls_params\n\n get_best_dls_params (dls, n_iters=10, num_workers=[0, 1, 2, 4, 8],\n                      pin_memory=[True, False], prefetch_factor=[2, 4, 8],\n                      return_best=True, verbose=True)\n\n\nsource\n\n\nget_best_dl_params\n\n get_best_dl_params (dl, n_iters=10, num_workers=[0, 1, 2, 4, 8],\n                     pin_memory=[True, False], prefetch_factor=[2, 4, 8],\n                     return_best=True, verbose=True)\n\n\nsource\n\n\nget_ts_dls\n\n get_ts_dls (X, y=None, splits=None, sel_vars=None, sel_steps=None,\n             tfms=None, inplace=True, path='.', bs=64, batch_tfms=None,\n             num_workers=0, device=None, shuffle_train=True,\n             drop_last=True, weights=None, partial_n=None, sampler=None,\n             sort=False, **kwargs)\n\n\n# Tests\na = np.arange(10)\n\nfor s in [None, np.arange(10), np.arange(10).tolist(), L(np.arange(10).tolist()), (np.arange(10).tolist(), None), (np.arange(10).tolist(), L())]:\n    test_eq(_check_splits(a, s), (L(np.arange(10).tolist()), L()))\n\n\nsource\n\n\nget_subset_dl\n\n get_subset_dl (dl, idxs)\n\n\nsource\n\n\nget_ts_dl\n\n get_ts_dl (X, y=None, split=None, sel_vars=None, sel_steps=None,\n            tfms=None, inplace=True, path='.', bs=64, batch_tfms=None,\n            num_workers=0, device=None, shuffle_train=True,\n            drop_last=True, weights=None, partial_n=None, sampler=None,\n            sort=False, **kwargs)\n\n\nX, y, splits = get_UCR_data(dsid, on_disk=False, split_data=False)\ndls = get_ts_dls(X, y, tfms=[None, TSClassification()], splits=splits, bs=8)\ndls = get_best_dls_params(dls, prefetch_factor=[2, 4, 8, 16])\n\n\nDataloader 0\n\n   num_workers:  0  pin_memory: True   prefetch_factor:  2  -  time:    1.224 ms/iter\n   num_workers:  0  pin_memory: False  prefetch_factor:  2  -  time:    0.583 ms/iter\n\n   best dl params:\n       best num_workers    : 0\n       best pin_memory     : False\n       best prefetch_factor: 2\n       return_best         : True\n\n\n\nDataloader 1\n\n   num_workers:  0  pin_memory: True   prefetch_factor:  2  -  time:    0.206 ms/iter\n   num_workers:  0  pin_memory: False  prefetch_factor:  2  -  time:    0.278 ms/iter\n\n   best dl params:\n       best num_workers    : 0\n       best pin_memory     : True\n       best prefetch_factor: 2\n       return_best         : True\n\n\n\n\n\ny_int = np.random.randint(0, 4, size=len(X))\ndls = get_ts_dls(X, y_int, splits=splits, bs=8)\ntest_eq(hasattr(dls, \"vocab\"), False)\n\ndls = get_ts_dls(X, y_int, splits=splits, bs=8, vocab=[0,1,2,3])\ntest_eq(dls.vocab, [0,1,2,3])\ntest_eq(dls.c, 4)\ntest_eq(dls.cat, True)\n\n\nX, y, splits = get_UCR_data(dsid, on_disk=False, split_data=False)\ndls = get_ts_dls(X, y, tfms=[None, TSClassification()], splits=splits, bs=8)\nb=first(dls.train)\ndls.decode(b)\ntest_eq(X.shape[1], dls.vars)\ntest_eq(X.shape[-1], dls.len)\n\n\nX, y, splits = get_UCR_data('OliveOil', on_disk=False, split_data=False)\ndls = get_ts_dls(X, y, tfms=[None, TSClassification()], splits=splits, bs=64, inplace=True)\n\nidxs = random_choice(len(dls.valid_ds), 10, False)\nnew_dl = get_subset_dl(dls.train, idxs)\n\nidxs = random_choice(len(dls.valid_ds), 10, False)\nnew_dl = get_subset_dl(dls.valid, idxs)\ntest_eq(new_dl.one_batch()[0].cpu().numpy(), X[splits[1][idxs]])\n\n\nX, y, splits = get_UCR_data('OliveOil', on_disk=False, split_data=False)\nweights = np.random.rand(len(X))\ndls = get_ts_dls(X, y, tfms=[None, TSClassification()], splits=splits, bs=64, inplace=True, weights=weights)\nweights2 = weights[splits[0]] / weights[splits[0]].sum()\ntest_eq(dls.train.weights, weights2)\ntest_eq(dls.valid.weights, None)\n\n\npartial_n = 12\nX, y, splits = get_UCR_data('OliveOil', on_disk=False, split_data=False)\ndls = get_ts_dls(X, y, splits=splits, tfms=[None, TSClassification()], bs=64, inplace=True, partial_n=partial_n)\ntest_eq(len(dls.train.one_batch()[0]), partial_n)\n\npartial_n = .1\nX, y, splits = get_UCR_data('OliveOil', on_disk=False, split_data=False)\ndls = get_ts_dls(X, y, tfms=[None, TSClassification()], bs=64, inplace=True, partial_n=partial_n)\ntest_eq(len(dls.train.one_batch()[0]), int(round(len(dls.train.dataset) * partial_n)))\n\nYou’ll now be able to pass a sampler to a tsai dataloader.\nYou should use a sampler for the train set and a sampler for the validation set. You’ll need to pass an object with the same length as each dataset. For example, the splits like in the case below.\n⚠️ Remember to set shuffle=False when using a sampler since they a mutually exclusive. This means that when you use a sampler, you always need to set the shuffle in the dataloader to False. The sampler will control whether the indices are shuffled or not (you can set shuffle to True or False in the sampler).\ndrop_last is managed in the dataloder though.\n\nX, y, splits = get_UCR_data('OliveOil', on_disk=False, split_data=False)\ntrain_sampler = torch.utils.data.sampler.RandomSampler(splits[0])\nvalid_sampler = torch.utils.data.sampler.SequentialSampler(splits[1])\ndls = get_ts_dls(X, y, splits=splits, tfms=[None, TSClassification()], bs=8, inplace=True, \n                 shuffle=False, drop_last=True, sampler=[train_sampler, valid_sampler])\nprint('train')\nfor _ in dls.train:\n    print(dls.train.idxs)\nprint('valid')\nfor _ in dls.valid:\n    print(dls.valid.idxs)\n\ntrain\n[2, 26, 20, 22, 14, 15, 6, 18]\n[8, 12, 16, 21, 4, 28, 23, 10]\n[17, 3, 13, 19, 29, 9, 7, 5]\nvalid\n[0, 1, 2, 3, 4, 5, 6, 7]\n[8, 9, 10, 11, 12, 13, 14, 15]\n[16, 17, 18, 19, 20, 21, 22, 23]\n[24, 25, 26, 27, 28, 29]\n\n\n\nX, y, splits = get_UCR_data('OliveOil', on_disk=False, split_data=False)\ntrain_sampler = torch.utils.data.sampler.SequentialSampler(splits[0])\nvalid_sampler = torch.utils.data.sampler.SequentialSampler(splits[1])\ndls = get_ts_dls(X, y, splits=splits, tfms=[None, TSClassification()], bs=64, inplace=True, \n                 shuffle=False, sampler=[train_sampler, valid_sampler])\ntest_eq(dls.get_idxs(), np.arange(len(splits[0])))\ntest_eq(dls.train.get_idxs(), np.arange(len(splits[0])))\ntest_eq(dls.valid.get_idxs(), np.arange(len(splits[1])))\nxb = dls.valid.one_batch()[0].cpu().numpy()\ntest_close(xb, X[dls.valid.split_idxs])\n\nX, y, splits = get_UCR_data('OliveOil', on_disk=False, split_data=False)\ntrain_sampler = torch.utils.data.sampler.RandomSampler(splits[0])\nvalid_sampler = torch.utils.data.sampler.SequentialSampler(splits[0])\ndls = get_ts_dls(X, y, splits=splits, tfms=[None, TSClassification()], bs=32, inplace=True, \n                 shuffle=False, drop_last=True, sampler=[train_sampler, valid_sampler])\ntest_ne(dls.train.get_idxs(), np.arange(len(splits[0])))\ntest_eq(np.sort(dls.train.get_idxs()), np.arange(len(splits[0])))\ntest_eq(dls.valid.get_idxs(), np.arange(len(splits[1])))\n\n\nX, y, splits = get_UCR_data('OliveOil', on_disk=False, split_data=False)\ndls = get_ts_dls(X, y, tfms=[None, TSClassification()], splits=splits, bs=64, inplace=False)\n\nidxs = random_choice(len(dls.valid_ds), 10, False)\nnew_dl = get_subset_dl(dls.train, idxs)\n\nidxs = random_choice(len(dls.valid_ds), 10, False)\nnew_dl = get_subset_dl(dls.valid, idxs)\ntest_eq(new_dl.one_batch()[0].cpu().numpy(), X[splits[1][idxs]])\n\n\nX, y, splits = get_UCR_data('OliveOil', on_disk=False, split_data=False)\ndls = get_ts_dls(X, y, tfms=[None, TSClassification()], splits=splits, bs=8)\nb = dls.one_batch()\ninput_idxs = dls.input_idxs\ntest_eq(b[0].cpu().numpy(), X[input_idxs])\nb = dls.train.one_batch()\ninput_idxs = dls.train.input_idxs\ntest_eq(b[0].cpu().numpy(), X[input_idxs])\nassert max(input_idxs) < len(splits[0])\nb = dls.valid.one_batch()\ninput_idxs = dls.valid.input_idxs\ntest_eq(b[0].cpu().numpy(), X[input_idxs])\nassert min(input_idxs) >= len(splits[0])\n\n\nX, y, splits = get_UCR_data('OliveOil', on_disk=False, split_data=False)\ndls = get_ts_dls(X, y, tfms=[None, TSCategorize()], splits=splits, bs=8)\nb=first(dls.train)\ndls.decode(b)\ntest_eq(X.shape[1], dls.vars)\ntest_eq(X.shape[-1], dls.len)\n\n\nX, y, splits = get_UCR_data('OliveOil', on_disk=False, split_data=False)\ndls = get_ts_dls(X, y, tfms=[None, TSCategorize()], splits=splits, bs=8, weights=np.random.randint(0, 3, len(y)))\nb=first(dls.train)\ndls.decode(b)\ntest_eq(X.shape[1], dls.vars)\ntest_eq(X.shape[-1], dls.len)\n\n\nX, y, splits = get_UCR_data('OliveOil', on_disk=False, split_data=False)\ndsets = TSDatasets(X, y, tfms=[None, TSCategorize()], splits=splits)\nts_dls = TSDataLoaders.from_dsets(dsets.train, dsets.valid, device=default_device(), bs=4)\ntorch.save(ts_dls, 'export/ts_dls.pth')\ndel ts_dls\nts_dls = torch.load('export/ts_dls.pth')\nfor xb,yb in ts_dls.train: \n    test_eq(tensor(X[ts_dls.train.idxs]), xb.cpu())\n\n\nX, y, splits = get_UCR_data('OliveOil', on_disk=False, split_data=False)\ndls = get_ts_dls(X, y, tfms=[None, TSCategorize()], splits=splits, bs=4)\nfor xb,yb in dls.train:\n    test_eq(xb.cpu().numpy(), X[dls.train.input_idxs])\nfor xb,yb in dls.valid:\n    test_eq(xb.cpu().numpy(), X[dls.valid.input_idxs])\n\n\ntest_eq((ts_dls.train.shuffle, ts_dls.valid.shuffle, ts_dls.train.drop_last, ts_dls.valid.drop_last), (True, False, True, False))\n\n\ndsid = 'OliveOil'\nX, y, splits = get_UCR_data(dsid, split_data=False)\ndls = get_ts_dls(X, y, tfms=[None, TSCategorize()], splits=splits, bs=8, num_workers=0)\nxb, yb = first(dls.train)\ntest_eq(tensor(X[dls.train.idxs]), xb.cpu())\n\n\ntest_eq((dls.train.shuffle, dls.valid.shuffle, dls.train.drop_last, dls.valid.drop_last), (True, False, True, False))\n\n\n# multiclass\ndsid = 'OliveOil'\nX, y, splits = get_UCR_data(dsid, on_disk=True, split_data=False)\ndls = get_ts_dls(X, y, tfms=[None, TSCategorize()], splits=splits, inplace=True)\ndls.show_dist()\ndls.train.show_dist()\nxb,yb = first(dls.train)\ntest_eq((dls.cat, dls.c), (True, 4))\ntest_ne(dls.cws.cpu().numpy(), None)\ndls.decoder((xb, ))\ndls.decoder((xb[0], ))\ndls.decoder((xb, yb))\ndls.decoder((xb[0], yb[0]))\ndls.decoder(yb)\ndls.decoder(yb[0])\n\n\n\n\n\n\n\n'3'\n\n\n\nnew_dl = dls.new_dl(X)\nfirst(new_dl)\n\n(TSTensor(samples:60, vars:1, len:570, device=cpu, dtype=torch.float32),)\n\n\n\nnew_dl = dls.new_dl(X, y=y)\nfirst(new_dl)\n\n(TSTensor(samples:60, vars:1, len:570, device=cpu, dtype=torch.float32),\n TensorCategory([3, 1, 3, 3, 3, 3, 3, 0, 3, 1, 3, 2, 1, 1, 3, 3, 3, 2, 1, 3, 3,\n                 2, 2, 3, 0, 1, 0, 2, 2, 1, 3, 3, 0, 1, 1, 3, 2, 1, 2, 1, 3, 1,\n                 3, 1, 0, 3, 0, 1, 1, 1, 3, 3, 0, 1, 0, 3, 3, 3, 0, 0]))\n\n\n\ndls.train.dataset.split_idxs, dls.train.dataset.splits, dls.valid.split_idxs\n\n(array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n        17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29], dtype=int8),\n (#30) [0,1,2,3,4,5,6,7,8,9...],\n array([30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46,\n        47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59], dtype=int8))\n\n\n\n# 2d input array and tfms == None return a NoTfmLists object\nX, y, splits = get_UCR_data('OliveOil', on_disk=False, split_data=False)\nX = X[:, 0]\ntfms=[None, TSCategorize()]\ndls = get_ts_dls(X, y, splits=splits, tfms=tfms, bs=8)\ntest_eq(1, dls.vars)\ntest_eq(X.shape[-1], dls.len)\ntest_eq(type(dls.tls[0]).__name__, 'NoTfmLists')\ndls = get_ts_dls(X, y, splits=splits, tfms=tfms, bs=8, inplace=False)\ntest_eq(1, dls.vars)\ntest_eq(X.shape[-1], dls.len)\ntest_eq(type(dls.tls[0]).__name__, 'NoTfmLists')\n\n\n# regression\ndsid = 'OliveOil'\nX, y, splits = get_UCR_data(dsid, on_disk=True, split_data=False)\ndls = get_ts_dls(X, np.random.rand(60, ), tfms=[None, ToNumpyTensor], splits=splits)\ndls.show_dist()\ndls.train.show_dist()\nxb,yb = first(dls.train)\ndls.decoder((xb, ))\ndls.decoder((xb[0], ))\ndls.decoder((xb, yb))\ndls.decoder((xb[0], yb[0]))\ndls.decoder(yb)\ndls.decoder(yb[0])\ntest_eq((dls.cat, dls.c), (False, 1))\ntest_eq(dls.cws, None)\n\n\n\n\n\n\n\n\n# regression, multilabel\ndsid = 'OliveOil'\nX, y, splits = get_UCR_data(dsid, on_disk=True, split_data=False)\ndls = get_ts_dls(X, np.random.rand(60, 3) * 5, tfms=[None, ToNumpyTensor], splits=splits)\ndls.show_dist()\ndls.train.show_dist()\nxb,yb = first(dls.train)\ndls.decoder((xb, ))\ndls.decoder((xb[0], ))\ndls.decoder((xb, yb))\ndls.decoder((xb[0], yb[0]))\ndls.decoder(yb)\ndls.decoder(yb[0])\ntest_eq((dls.cat, dls.c, dls.d),(False, 1, 3)) \ntest_eq(dls.cws, None)\n\n\n\n\n\n\n\n\n# multiclass, multilabel\ndsid = 'OliveOil'\nX, y, splits = get_UCR_data(dsid, on_disk=True, split_data=False)\ncm = {\n    '1':'A',        \n    '2':['B', 'C'],\n    '3':['B', 'D'] , \n    '4':'E', \n    }\nkeys = cm.keys()\nnew_cm = {k:v for k,v in zip(keys, [listify(v) for v in cm.values()])}\ny_multi = np.array([new_cm[yi] if yi in keys else listify(yi) for yi in y], dtype=object)\ndls = get_ts_dls(X, y_multi, tfms=[None, TSMultiLabelClassification()], splits=splits)\ndls.show_dist()\ndls.train.show_dist()\nxb,yb = first(dls.train)\ndls.decoder((xb, ))\ndls.decoder((xb[0], ))\ndls.decoder((xb, yb))\ndls.decoder((xb[0], yb[0]))\ndls.decoder(yb)\ndls.decoder(yb[0])\ntest_eq((dls.cat, dls.c), (True, 5))\ntest_ne(dls.cws.cpu().numpy(), None)\n\n\n\n\n\n\n\n\ndsid = 'OliveOil'\nX, y, splits = get_UCR_data(dsid, on_disk=True, split_data=False)\ncm = {\n    '1':'A',        \n    '2':['B', 'C'],\n    '3':['B', 'D'] , \n    '4':'E', \n    }\nkeys = cm.keys()\nnew_cm = {k:v for k,v in zip(keys, [listify(v) for v in cm.values()])}\ny_multi = np.array([new_cm[yi] if yi in keys else listify(yi) for yi in y], dtype=object)\ndls = get_ts_dls(X, y_multi, tfms=[None, TSMultiLabelClassification()], splits=splits)\ntest_eq(dls.new(X[0]).one_batch().shape, (1, 570))\ntest_eq(dls.new(X[:15]).one_batch().shape, (15, 1, 570))\ntest_eq(dls.train.new(X[0]).one_batch().shape, (1, 570))\ntest_eq(dls.valid.new(X[:15]).one_batch().shape, (15, 1, 570))\n\n\nbs = 25\ndsets = TSDatasets(X, y, tfms=[None, TSCategorize()], splits=splits)\ndls   = TSDataLoaders.from_dsets(dsets.train, dsets.valid, bs=[bs, bs*2], batch_tfms=add(1), num_workers=0)\nxb,yb = dls.train.one_batch()\ntest_eq(xb.cpu().data, tensor(X_on_disk[splits[0]][dls.train.idxs]) + 1)\n\n\ndsets = TSDatasets(X, y, tfms=[None, TSCategorize()], splits=splits)\ndls   = TSDataLoaders.from_dsets(dsets.train, dsets.valid, bs=[bs, bs*2])\nxb,yb = dls.train.one_batch()\ntest_eq(xb.shape, (min(bs, len(splits[0])), X.shape[1], X.shape[-1]))\nit = iter(dls.valid)\nfor xb,yb in it: \n    test_close(xb.cpu(), TSTensor(X[splits[1]][dls.valid.idxs]))\n\n\nbs = 64\ndsets = TSDatasets(X, y, tfms=[add(1), TSCategorize()], splits=RandomSplitter(valid_pct=.3)(y_array))\ndls = TSDataLoaders.from_dsets(dsets.train, dsets.valid, bs=[bs, bs*2])\nxb,yb = dls.train.one_batch()\ntest_eq(xb.shape, (min(bs, len(dsets.train)), X_on_disk.shape[1], X_on_disk.shape[-1]))\nxb,yb = dls.valid.one_batch()\ntest_eq(xb.shape, (min(bs*2, len(dsets.valid)), X_on_disk.shape[1], X_on_disk.shape[-1]))\n\n\ndsets = TSDatasets(X_on_disk, y_array, tfms=[None, TSCategorize()], splits=splits)\ndls   = TSDataLoaders.from_dsets(dsets.train, dsets.valid, bs=[32, 64])\nfor i in range(10):\n    dl = dls.train if random.random() < .5 else dls.valid\n    xb,yb = dl.one_batch()\n    torch.equal(xb.cpu(), TSTensor(X_on_disk[dl.input_idxs]))\n    \ndsets = TSDatasets(X_on_disk, y_array, tfms=[None, TSCategorize()])\ndls   = TSDataLoaders.from_dsets(dsets, bs=32)\nfor i in range(10):\n    xb,yb = dls.one_batch()\n    torch.equal(xb.cpu(), TSTensor(X_on_disk[dl.input_idxs]))\n    \ndsets = TSDatasets(X_on_disk, tfms=None)\ndls   = TSDataLoaders.from_dsets(dsets, bs=32)\nfor i in range(10):\n    xb = dls.one_batch()\n    torch.equal(xb[0].cpu(), TSTensor(X_on_disk[dl.input_idxs]))\n\n\ndsets = TSDatasets(X_on_disk, y_array, tfms=[None, TSCategorize()])\ndls   = TSDataLoaders.from_dsets(dsets, bs=32)\ntest_eq(dls.split_idxs, L(np.arange(len(X_on_disk)).tolist()))\n\n\nX, y, splits = get_UCR_data('NATOPS', return_split=False)\ntfms  = [None, [TSCategorize()]]\ndls = get_ts_dls(X, y, tfms=tfms, splits=splits, bs=[64, 128])\ndls.show_batch()\ndls.show_dist()\n\n\n\n\n\n\n\n\n# test passing a list with categories instead of a numpy array \ndsid = 'NATOPS'\nbs = 64\nX2, y2, splits2 = get_UCR_data(dsid, return_split=False)\nvocab = sorted(set(y))\ntfms = [None, [TSCategorize(vocab=vocab)]]\ndsets = TSDatasets(X2, y2, tfms=tfms, splits=splits2)\ndls = TSDataLoaders.from_dsets(dsets.train, dsets.valid, bs=[bs, bs*2])\ndls.train.one_batch()\n\n(TSTensor(samples:64, vars:24, len:51, device=cpu, dtype=torch.float32),\n TensorCategory([2, 3, 4, 0, 2, 1, 2, 1, 1, 0, 2, 5, 1, 1, 1, 4, 5, 4, 5, 1, 5,\n                 2, 0, 4, 5, 1, 5, 3, 2, 5, 3, 0, 0, 2, 0, 2, 1, 3, 0, 1, 5, 3,\n                 1, 0, 2, 0, 1, 1, 0, 2, 4, 3, 1, 3, 1, 4, 1, 1, 3, 0, 5, 4, 5,\n                 5]))\n\n\n\n# MultiCategory\nbs = 64\nn_epochs = 100\ntfms = [None, [MultiCategorize()]]\ndsets = TSDatasets(X2, y2, tfms=tfms, splits=splits2)\ndls = TSDataLoaders.from_dsets(dsets.train, dsets.valid, bs=bs)\ndls.train.one_batch()\n\n(TSTensor(samples:64, vars:24, len:51, device=cpu, dtype=torch.float32),\n TensorMultiCategory([[4, 0, 1],\n                      [5, 0, 1],\n                      [3, 0, 1],\n                      [6, 0, 1],\n                      [3, 0, 1],\n                      [7, 0, 1],\n                      [4, 0, 1],\n                      [4, 0, 1],\n                      [4, 0, 1],\n                      [4, 0, 1],\n                      [3, 0, 1],\n                      [7, 0, 1],\n                      [2, 0, 1],\n                      [7, 0, 1],\n                      [2, 0, 1],\n                      [6, 0, 1],\n                      [2, 0, 1],\n                      [3, 0, 1],\n                      [3, 0, 1],\n                      [2, 0, 1],\n                      [7, 0, 1],\n                      [6, 0, 1],\n                      [5, 0, 1],\n                      [3, 0, 1],\n                      [3, 0, 1],\n                      [2, 0, 1],\n                      [3, 0, 1],\n                      [3, 0, 1],\n                      [7, 0, 1],\n                      [2, 0, 1],\n                      [3, 0, 1],\n                      [7, 0, 1],\n                      [3, 0, 1],\n                      [2, 0, 1],\n                      [6, 0, 1],\n                      [7, 0, 1],\n                      [4, 0, 1],\n                      [3, 0, 1],\n                      [2, 0, 1],\n                      [6, 0, 1],\n                      [4, 0, 1],\n                      [5, 0, 1],\n                      [2, 0, 1],\n                      [3, 0, 1],\n                      [3, 0, 1],\n                      [2, 0, 1],\n                      [3, 0, 1],\n                      [4, 0, 1],\n                      [3, 0, 1],\n                      [7, 0, 1],\n                      [2, 0, 1],\n                      [6, 0, 1],\n                      [4, 0, 1],\n                      [6, 0, 1],\n                      [7, 0, 1],\n                      [3, 0, 1],\n                      [5, 0, 1],\n                      [5, 0, 1],\n                      [7, 0, 1],\n                      [3, 0, 1],\n                      [7, 0, 1],\n                      [5, 0, 1],\n                      [2, 0, 1],\n                      [4, 0, 1]]))\n\n\nThe combination of splits, sel_vars and sel_steps is very powerful, as it allows you to perform advanced indexing of the array-like X.\n\nfrom tsai.data.validation import TSSplitter\n\n\nX = np.arange(16*5*50).reshape(16,5,50)\ny = alphabet[np.random.randint(0,3, 16)]\nsplits = TSSplitter(show_plot=False)(y)\ntfms = [None, TSCategorize()]\nbatch_tfms = None\ndls = get_ts_dls(X, y, splits=splits, sel_vars=[0, 1, 3], sel_steps=slice(-10, None), tfms=tfms, batch_tfms=batch_tfms)\nxb,yb=dls.train.one_batch()\ntest_close(X[dls.input_idxs][:, [0, 1, 3]][...,slice(-10, None)], xb.cpu().numpy())\nnew_dl = dls.train.new_dl(X[:5], y[:5])\nprint(new_dl.one_batch())\nnew_empty_dl = dls.new_empty() # when exported\ndl = new_empty_dl.new_dl(X[:10], y[:10], bs=64) # after export\ndl.one_batch()\n\n(TSTensor(samples:5, vars:3, len:10, device=cpu, dtype=torch.int64), TensorCategory([1, 2, 2, 1, 2]))\n\n\n(TSTensor(samples:10, vars:3, len:10, device=cpu, dtype=torch.int64),\n TensorCategory([2, 1, 0, 0, 1, 1, 2, 2, 0, 1]))\n\n\n\nsource\n\n\nget_dl_percent_per_epoch\n\n get_dl_percent_per_epoch (dl, model, n_batches=None)\n\n\nsource\n\n\nget_time_per_batch\n\n get_time_per_batch (dl, model=None, n_batches=None)\n\n\nX, y, splits = get_UCR_data('NATOPS', split_data=False)\ntfms  = [None, [TSCategorize()]]\ndls = get_ts_dls(X, y, tfms=tfms, splits=splits)\ntrain_dl = dls.train\nxb, _ = train_dl.one_batch()\nmodel = nn.Linear(xb.shape[-1], 2).to(xb.device)\nt = get_dl_percent_per_epoch(train_dl, model, n_batches=10)\nprint(t)\n\n\n\n\n\n\n    \n      \n      50.00% [1/2 00:00<00:00]\n    \n    \n\n\n\n\n\n\n\n    \n      \n      0.00% [0/2 00:00<?]\n    \n    \n\n\n10.70%"
  },
  {
    "objectID": "calibration.html",
    "href": "calibration.html",
    "title": "Calibration",
    "section": "",
    "text": "Functionality to calibrate a trained, binary classification model using temperature scaling.\n\n\nsource\n\nECELoss\n\n ECELoss (n_bins=10)\n\nCalculates the Expected Calibration Error of a model.\n\nsource\n\n\nTemperatureSetter\n\n TemperatureSetter (model, lr=0.01, max_iter=1000, line_search_fn=None,\n                    n_bins=10, verbose=True)\n\nCalibrates a binary classification model optimizing temperature\n\nsource\n\n\nModelWithTemperature\n\n ModelWithTemperature (model)\n\nA decorator which wraps a model with temperature scaling\n\nsource\n\n\nplot_calibration_curve\n\n plot_calibration_curve (labels, logits, cal_logits=None, figsize=(6, 6),\n                         n_bins=10, strategy='uniform')\n\n\nsource\n\n\nLearner.calibrate_model\n\n Learner.calibrate_model (X=None, y=None, lr=0.01, max_iter=10000,\n                          line_search_fn=None, n_bins=10,\n                          strategy='uniform', show_plot=True, figsize=(6,\n                          6), verbose=True)\n\n\nfrom tsai.basics import *\nfrom tsai.models.FCNPlus import FCNPlus\n\n\nX, y, splits = get_UCR_data('FingerMovements', split_data=False)\ntfms  = [None, [TSClassification()]]\nbatch_tfms = TSRobustScale()\ndls = get_ts_dls(X, y, splits=splits, tfms=tfms, batch_tfms=batch_tfms)\nlearn = ts_learner(dls, FCNPlus, metrics=accuracy)\nlearn.fit_one_cycle(2)\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      accuracy\n      time\n    \n  \n  \n    \n      0\n      0.696826\n      0.706016\n      0.430000\n      00:04\n    \n    \n      1\n      0.690209\n      0.699720\n      0.490000\n      00:03\n    \n  \n\n\n\n\nlearn.calibrate_model()\ncalibrated_model = learn.calibrated_model\n\nBefore temperature - NLL: 0.700, ECE: 0.066\nCalibrating the model...\n...model calibrated\nOptimal temperature: 6.383\nAfter temperature  - NLL: 0.693, ECE: 0.019"
  },
  {
    "objectID": "tutorials.html",
    "href": "tutorials.html",
    "title": "Tutorial notebooks",
    "section": "",
    "text": "A number of tutorials have been created to help you get started to use tsai with time series data. Please, feel free to open the notebooks (you can open them in Colab if you want) and tweak them to do your own experiments."
  },
  {
    "objectID": "tutorials.html#time-series-classification-using-raw-data",
    "href": "tutorials.html#time-series-classification-using-raw-data",
    "title": "Tutorial notebooks",
    "section": "Time series classification (using raw data)",
    "text": "Time series classification (using raw data)\nI’d recommend you to start with:\n\nIntroduction to Time Series Classification. This notebook contains a detailed walk through the steps to perform time series classification.\n\n\nData preparation:\nIf you need help preparing your data you may find the following tutorials useful:\n\nTime Series data preparation: this will show how you can do classify both univariate or multivariate time series.\nHow to work with (very) large numpy arrays in tsai?\nHow to use numpy arrays in tsai?\n\nThese last 2 provide more details in case you need them. They explain how datasets and dataloaders are created.\n\n\nTypes of architectures:\nOnce you feel comfortable, you can start exploring different types of architectures:\n\nYou can use the Time Series data preparation notebook and replace the InceptionTime architecture by any other of your choice:\n\nMLPs\nRNNs (LSTM, GRU)\nCNNs (FCN, ResNet, XResNet)\nWavelet-based architectures\nTransformers (like TST - 2020)\nThey all (except ROCKET) work in the same way, for univariate or multivariate time series.\n\nHow to use Transformers with Time Series? may also help you understand how to successfully apply this new type of architecture to time series.\nYou can also use Time Series Classification Benchmark to perform bechmarks with different architectures and/ or configurations.\n\nROCKET (2019) is a new technique used to generate 10-20k features from time series. These features are used in a different classifier. This is the only implementation I’m aware of that uses GPU and allows both univariate and multivariate time series. To explain this method that works very well in many cases you can use the following notebook:\n\nROCKET: a new state-of-the-art time series classifier\n\nThere are many types of classifiers as you can see, and it’s very difficult to know in advance which one will perform well in our task. However, the ones that have consistently deliver the best results in recent benchmark studies are Inceptiontime (Fawaz, 2019) and ROCKET (Dempster, 2019). Transformers, like TST (Zerveas, 2020), also show a lot of promise, but the application to time series data is so new that they have not been benchmarked against other architectures. But I’d say these are 3 architectures you should know well."
  },
  {
    "objectID": "tutorials.html#time-series-classification-using-time-series-images",
    "href": "tutorials.html#time-series-classification-using-time-series-images",
    "title": "Tutorial notebooks",
    "section": "Time series classification (using time series images)",
    "text": "Time series classification (using time series images)\nIn these tutorials, I’ve also included a section on how to transform time series into images. This will allow you to then use DL vision models like ResNet50 for example. This approach works very well in some cases, even if you have limited data. You can learn about this technique in this notebook:\n\nImaging time series"
  },
  {
    "objectID": "tutorials.html#time-series-regression",
    "href": "tutorials.html#time-series-regression",
    "title": "Tutorial notebooks",
    "section": "Time series regression",
    "text": "Time series regression\nI’ve also included an example of how you can perform time series regression with your time series using tsai. In this case, the label will be continuous, instead of a category. But as you will see, the use is almost identical to time series classification. You can learn more about this here:\n\nTime series regression"
  },
  {
    "objectID": "tutorials.html#visualization",
    "href": "tutorials.html#visualization",
    "title": "Tutorial notebooks",
    "section": "Visualization",
    "text": "Visualization\nI’ve also created PredictionDynamics callback that allows you to visualize the model’s predictions while it’s training. It can provide you some additional insights that may be useful to improve your model. Here’s the notebook:\n\nPredictionDynamics\n\nI hope you will find these tutorial useful. I’m planning to add more tutorials to demonstrate new techniques, models, etc when they become available. So stay tuned!"
  },
  {
    "objectID": "models.transformermodel.html",
    "href": "models.transformermodel.html",
    "title": "TransformerModel",
    "section": "",
    "text": "This is an unofficial PyTorch implementation created by Ignacio Oguiza - oguiza@timeseriesAI.co\n\n\nsource\n\nTransformerModel\n\n TransformerModel (c_in, c_out, d_model=64, n_head=1, d_ffn=128,\n                   dropout=0.1, activation='relu', n_layers=1)\n\nSame as nn.Module, but no need for subclasses to call super().__init__\n\nbs = 16\nnvars = 3\nseq_len = 96\nc_out = 2\nxb = torch.rand(bs, nvars, seq_len)\n\n\nmodel = TransformerModel(nvars, c_out, d_model=64, n_head=1, d_ffn=128, dropout=0.1, activation='gelu', n_layers=3)\ntest_eq(model(xb).shape, [bs, c_out])\nprint(count_parameters(model))\nmodel\n\n100930\n\n\nTransformerModel(\n  (permute): Permute(dims=2, 0, 1)\n  (inlinear): Linear(in_features=3, out_features=64, bias=True)\n  (relu): ReLU()\n  (transformer_encoder): TransformerEncoder(\n    (layers): ModuleList(\n      (0): TransformerEncoderLayer(\n        (self_attn): MultiheadAttention(\n          (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=True)\n        )\n        (linear1): Linear(in_features=64, out_features=128, bias=True)\n        (dropout): Dropout(p=0.1, inplace=False)\n        (linear2): Linear(in_features=128, out_features=64, bias=True)\n        (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n        (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n        (dropout1): Dropout(p=0.1, inplace=False)\n        (dropout2): Dropout(p=0.1, inplace=False)\n      )\n      (1): TransformerEncoderLayer(\n        (self_attn): MultiheadAttention(\n          (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=True)\n        )\n        (linear1): Linear(in_features=64, out_features=128, bias=True)\n        (dropout): Dropout(p=0.1, inplace=False)\n        (linear2): Linear(in_features=128, out_features=64, bias=True)\n        (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n        (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n        (dropout1): Dropout(p=0.1, inplace=False)\n        (dropout2): Dropout(p=0.1, inplace=False)\n      )\n      (2): TransformerEncoderLayer(\n        (self_attn): MultiheadAttention(\n          (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=True)\n        )\n        (linear1): Linear(in_features=64, out_features=128, bias=True)\n        (dropout): Dropout(p=0.1, inplace=False)\n        (linear2): Linear(in_features=128, out_features=64, bias=True)\n        (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n        (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n        (dropout1): Dropout(p=0.1, inplace=False)\n        (dropout2): Dropout(p=0.1, inplace=False)\n      )\n    )\n    (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n  )\n  (transpose): Transpose(1, 0)\n  (max): Max(dim=1, keepdim=False)\n  (outlinear): Linear(in_features=64, out_features=2, bias=True)\n)"
  },
  {
    "objectID": "models.rocket_pytorch.html",
    "href": "models.rocket_pytorch.html",
    "title": "ROCKET Pytorch",
    "section": "",
    "text": "ROCKET (RandOm Convolutional KErnel Transform) functions for univariate and multivariate time series developed in Pytorch.\n\n\nsource\n\nROCKET\n\n ROCKET (c_in, seq_len, n_kernels=10000, kss=[7, 9, 11], device=None,\n         verbose=False)\n\nRandOm Convolutional KErnel Transform\nROCKET is a GPU Pytorch implementation of the ROCKET functions generate_kernels and apply_kernels that can be used with univariate and multivariate time series.\n\nsource\n\n\ncreate_rocket_features\n\n create_rocket_features (dl, model, verbose=False)\n\nArgs: model : ROCKET model instance dl : single TSDataLoader (for example dls.train or dls.valid)\n\nbs = 16\nc_in = 7  # aka channels, features, variables, dimensions\nc_out = 2\nseq_len = 15\nxb = torch.randn(bs, c_in, seq_len).to(default_device())\n\nm = ROCKET(c_in, seq_len, n_kernels=1_000, kss=[7, 9, 11]) # 1_000 for testing with a cpu. Default is 10k with a gpu!\ntest_eq(m(xb).shape, [bs, 2_000])\n\n\nfrom tsai.data.all import *\nfrom tsai.models.utils import *\n\n\nX, y, splits = get_UCR_data('OliveOil', split_data=False)\ntfms = [None, TSRegression()]\nbatch_tfms = TSStandardize(by_var=True)\ndls = get_ts_dls(X, y, splits=splits, tfms=tfms, batch_tfms=batch_tfms, shuffle_train=False, drop_last=False)\nmodel = build_ts_model(ROCKET, dls=dls, n_kernels=1_000) # 1_000 for testing with a cpu. Default is 10k with a gpu!\nX_train, y_train = create_rocket_features(dls.train, model) \nX_valid, y_valid = create_rocket_features(dls.valid, model)\nX_train.shape, X_valid.shape\n\n((30, 2000), (30, 2000))"
  },
  {
    "objectID": "models.fcnplus.html",
    "href": "models.fcnplus.html",
    "title": "FCNPlus",
    "section": "",
    "text": "This is an unofficial PyTorch implementation created by Ignacio Oguiza - oguiza@timeseriesAI.co\n\n\nsource\n\nFCNPlus\n\n FCNPlus (c_in, c_out, layers=[128, 256, 128], kss=[7, 5, 3], coord=False,\n          separable=False, use_bn=False, fc_dropout=0.0, zero_norm=False,\n          act=<class 'torch.nn.modules.activation.ReLU'>, act_kwargs={},\n          residual=False, custom_head=None)\n\nA sequential container. Modules will be added to it in the order they are passed in the constructor. Alternatively, an OrderedDict of modules can be passed in. The forward() method of [Sequential](https://timeseriesAI.github.io/models.layers.html#sequential) accepts any input and forwards it to the first module it contains. It then “chains” outputs to inputs sequentially for each subsequent module, finally returning the output of the last module.\nThe value a [Sequential](https://timeseriesAI.github.io/models.layers.html#sequential) provides over manually calling a sequence of modules is that it allows treating the whole container as a single module, such that performing a transformation on the [Sequential](https://timeseriesAI.github.io/models.layers.html#sequential) applies to each of the modules it stores (which are each a registered submodule of the [Sequential](https://timeseriesAI.github.io/models.layers.html#sequential)).\nWhat’s the difference between a [Sequential](https://timeseriesAI.github.io/models.layers.html#sequential) and a :class:torch.nn.ModuleList? A ModuleList is exactly what it sounds like–a list for storing Module s! On the other hand, the layers in a [Sequential](https://timeseriesAI.github.io/models.layers.html#sequential) are connected in a cascading way.\nExample::\n# Using Sequential to create a small model. When `model` is run,\n# input will first be passed to `Conv2d(1,20,5)`. The output of\n# `Conv2d(1,20,5)` will be used as the input to the first\n# `ReLU`; the output of the first `ReLU` will become the input\n# for `Conv2d(20,64,5)`. Finally, the output of\n# `Conv2d(20,64,5)` will be used as input to the second `ReLU`\nmodel = nn.Sequential(\n          nn.Conv2d(1,20,5),\n          nn.ReLU(),\n          nn.Conv2d(20,64,5),\n          nn.ReLU()\n        )\n\n# Using Sequential with OrderedDict. This is functionally the\n# same as the above code\nmodel = nn.Sequential(OrderedDict([\n          ('conv1', nn.Conv2d(1,20,5)),\n          ('relu1', nn.ReLU()),\n          ('conv2', nn.Conv2d(20,64,5)),\n          ('relu2', nn.ReLU())\n        ]))\n\nxb = torch.rand(16, 3, 10)\ntest_eq(FCNPlus(3, 2)(xb).shape, [xb.shape[0], 2])\ntest_eq(FCNPlus(3, 2, coord=True, separable=True, act=Swish, residual=True)(xb).shape, [xb.shape[0], 2])\ntest_eq(nn.Sequential(*FCNPlus(3, 2).children())(xb).shape, [xb.shape[0], 2])\ntest_eq(FCNPlus(3, 2, custom_head=partial(mlp_head, seq_len=10))(xb).shape, [xb.shape[0], 2])\n\n\nfrom tsai.models.utils import *\n\n\nmodel = build_ts_model(FCNPlus, 2, 3)\nmodel[-1]\n\nSequential(\n  (0): AdaptiveAvgPool1d(output_size=1)\n  (1): Squeeze(dim=-1)\n  (2): Linear(in_features=128, out_features=3, bias=True)\n)\n\n\n\nfrom tsai.models.FCN import *\n\n\ntest_eq(count_parameters(FCN(3,2)), count_parameters(FCNPlus(3,2)))\n\n\nFCNPlus(3,2)\n\nFCNPlus(\n  (backbone): _FCNBlockPlus(\n    (convblock1): ConvBlock(\n      (0): Conv1d(3, 128, kernel_size=(7,), stride=(1,), padding=(3,), bias=False)\n      (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (2): ReLU()\n    )\n    (convblock2): ConvBlock(\n      (0): Conv1d(128, 256, kernel_size=(5,), stride=(1,), padding=(2,), bias=False)\n      (1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (2): ReLU()\n    )\n    (convblock3): ConvBlock(\n      (0): Conv1d(256, 128, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)\n      (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (2): ReLU()\n    )\n    (add): Sequential()\n  )\n  (head): Sequential(\n    (0): AdaptiveAvgPool1d(output_size=1)\n    (1): Squeeze(dim=-1)\n    (2): Linear(in_features=128, out_features=2, bias=True)\n  )\n)"
  },
  {
    "objectID": "data.transforms.html",
    "href": "data.transforms.html",
    "title": "Time Series Data Augmentation",
    "section": "",
    "text": "Functions used to transform TSTensors (Data Augmentation)\n\n\nfrom tsai.data.core import TSCategorize\nfrom tsai.data.external import get_UCR_data\nfrom tsai.data.preprocessing import TSStandardize\n\n\ndsid = 'NATOPS'\nX, y, splits = get_UCR_data(dsid, return_split=False)\ntfms = [None, TSCategorize()]\nbatch_tfms = TSStandardize()\ndls = get_ts_dls(X, y, tfms=tfms, splits=splits, batch_tfms=batch_tfms, bs=128)\nxb, yb = next(iter(dls.train))\n\n\nsource\n\nTSIdentity\n\n TSIdentity (magnitude=None, **kwargs)\n\nApplies the identity tfm to a TSTensor batch\n\ntest_eq(TSIdentity()(xb, split_idx=0).shape, xb.shape)\n\n\nsource\n\n\nTSShuffle_HLs\n\n TSShuffle_HLs (magnitude=1.0, ex=None, **kwargs)\n\nRandomly shuffles HIs/LOs of an OHLC TSTensor batch\n\ntest_eq(TSShuffle_HLs()(xb, split_idx=0).shape, xb.shape)\n\n\nsource\n\n\nTSShuffleSteps\n\n TSShuffleSteps (magnitude=1.0, ex=None, **kwargs)\n\nRandomly shuffles consecutive sequence datapoints in batch\n\nt = TSTensor(torch.arange(11).float())\ntt_ = []\nfor _ in range(1000): \n    tt = TSShuffleSteps()(t, split_idx=0)\n    test_eq(len(set(tt.tolist())), len(t))\n    test_ne(tt, t)\n    tt_.extend([t for i,t in enumerate(tt) if t!=i])\nx, y = np.unique(tt_, return_counts=True) # This is to visualize distribution which should be equal for all and half for first and last items\nplt.bar(x, y);\n\n\n\n\n\nsource\n\n\nTSGaussianNoise\n\n TSGaussianNoise (magnitude=0.5, additive=True, ex=None, **kwargs)\n\nApplies additive or multiplicative gaussian noise\n\ntest_eq(TSGaussianNoise(.1, additive=True)(xb, split_idx=0).shape, xb.shape)\ntest_eq(TSGaussianNoise(.1, additive=False)(xb, split_idx=0).shape, xb.shape)\n\n\nsource\n\n\nTSMagMulNoise\n\n TSMagMulNoise (magnitude=1, ex=None, **kwargs)\n\nApplies multiplicative noise on the y-axis for each step of a TSTensor batch\n\nsource\n\n\nTSMagAddNoise\n\n TSMagAddNoise (magnitude=1, ex=None, **kwargs)\n\nApplies additive noise on the y-axis for each step of a TSTensor batch\n\ntest_eq(TSMagAddNoise()(xb, split_idx=0).shape, xb.shape)\ntest_eq(TSMagMulNoise()(xb, split_idx=0).shape, xb.shape)\ntest_ne(TSMagAddNoise()(xb, split_idx=0), xb)\ntest_ne(TSMagMulNoise()(xb, split_idx=0), xb)\n\n\nsource\n\n\nrandom_cum_linear_generator\n\n random_cum_linear_generator (o, magnitude=0.1)\n\n\nsource\n\n\nrandom_cum_noise_generator\n\n random_cum_noise_generator (o, magnitude=0.1, noise=None)\n\n\nsource\n\n\nrandom_cum_curve_generator\n\n random_cum_curve_generator (o, magnitude=0.1, order=4, noise=None)\n\n\nsource\n\n\nrandom_curve_generator\n\n random_curve_generator (o, magnitude=0.1, order=4, noise=None)\n\n\nsource\n\n\nTSTimeNoise\n\n TSTimeNoise (magnitude=0.1, ex=None, **kwargs)\n\nApplies noise to each step in the x-axis of a TSTensor batch based on smooth random curve\n\ntest_eq(TSTimeNoise()(xb, split_idx=0).shape, xb.shape)\ntest_ne(TSTimeNoise()(xb, split_idx=0), xb)\n\n\nsource\n\n\nTSMagWarp\n\n TSMagWarp (magnitude=0.02, ord=4, ex=None, **kwargs)\n\nApplies warping to the y-axis of a TSTensor batch based on a smooth random curve\n\ntest_eq(TSMagWarp()(xb, split_idx=0).shape, xb.shape)\ntest_ne(TSMagWarp()(xb, split_idx=0), xb)\n\n\nsource\n\n\nTSTimeWarp\n\n TSTimeWarp (magnitude=0.1, ord=6, ex=None, **kwargs)\n\nApplies time warping to the x-axis of a TSTensor batch based on a smooth random curve\n\ntest_eq(TSTimeWarp()(xb, split_idx=0).shape, xb.shape)\ntest_ne(TSTimeWarp()(xb, split_idx=0), xb)\n\n\nsource\n\n\nTSWindowWarp\n\n TSWindowWarp (magnitude=0.1, ex=None, **kwargs)\n\nApplies window slicing to the x-axis of a TSTensor batch based on a random linear curve based on https://halshs.archives-ouvertes.fr/halshs-01357973/document\n\ntest_eq(TSWindowWarp()(xb, split_idx=0).shape, xb.shape)\n\n\nsource\n\n\nTSMagScalePerVar\n\n TSMagScalePerVar (magnitude=0.5, ex=None, **kwargs)\n\nApplies per_var scaling to the y-axis of a TSTensor batch based on a scalar\n\nsource\n\n\nTSMagScale\n\n TSMagScale (magnitude=0.5, ex=None, **kwargs)\n\nApplies scaling to the y-axis of a TSTensor batch based on a scalar\n\ntest_eq(TSMagScale()(xb, split_idx=0).shape, xb.shape)\ntest_eq(TSMagScalePerVar()(xb, split_idx=0).shape, xb.shape)\ntest_ne(TSMagScale()(xb, split_idx=0), xb)\ntest_ne(TSMagScalePerVar()(xb, split_idx=0), xb)\n\n\nsource\n\n\nTSRandomResizedCrop\n\n TSRandomResizedCrop (magnitude=0.1, size=None, scale=None, ex=None,\n                      mode='linear', **kwargs)\n\nRandomly amplifies a sequence focusing on a random section of the steps\n\ntest_eq(TSRandomResizedCrop(.5)(xb, split_idx=0).shape, xb.shape)\ntest_ne(TSRandomResizedCrop(size=.8, scale=(.5, 1))(xb, split_idx=0).shape, xb.shape)\ntest_ne(TSRandomResizedCrop(size=20, scale=(.5, 1))(xb, split_idx=0).shape, xb.shape)\n\n\nsource\n\n\nTSWindowSlicing\n\n TSWindowSlicing (magnitude=0.1, ex=None, mode='linear', **kwargs)\n\nRandomly extracts an resize a ts slice based on https://halshs.archives-ouvertes.fr/halshs-01357973/document\n\ntest_eq(TSWindowSlicing()(xb, split_idx=0).shape, xb.shape)\ntest_ne(TSWindowSlicing()(xb, split_idx=0), xb)\n\n\nsource\n\n\nTSRandomZoomOut\n\n TSRandomZoomOut (magnitude=0.1, ex=None, mode='linear', **kwargs)\n\nRandomly compresses a sequence on the x-axis\n\ntest_eq(TSRandomZoomOut(.5)(xb, split_idx=0).shape, xb.shape)\n\n\nsource\n\n\nTSRandomTimeScale\n\n TSRandomTimeScale (magnitude=0.1, ex=None, mode='linear', **kwargs)\n\nRandomly amplifies/ compresses a sequence on the x-axis keeping the same length\n\ntest_eq(TSRandomTimeScale(.5)(xb, split_idx=0).shape, xb.shape)\n\n\nsource\n\n\nTSRandomTimeStep\n\n TSRandomTimeStep (magnitude=0.02, ex=None, mode='linear', **kwargs)\n\nCompresses a sequence on the x-axis by randomly selecting sequence steps and interpolating to previous size\n\ntest_eq(TSRandomTimeStep()(xb, split_idx=0).shape, xb.shape)\n\n\nsource\n\n\nTSResampleSteps\n\n TSResampleSteps (step_pct=1.0, same_seq_len=True, magnitude=None,\n                  **kwargs)\n\nTransform that randomly selects and sorts sequence steps (with replacement) maintaining the sequence length\n\ntest_eq(TSResampleSteps(step_pct=.9, same_seq_len=False)(xb, split_idx=0).shape[-1], round(.9*xb.shape[-1]))\ntest_eq(TSResampleSteps(step_pct=.9, same_seq_len=True)(xb, split_idx=0).shape[-1], xb.shape[-1])\n\n\nsource\n\n\nTSBlur\n\n TSBlur (magnitude=1.0, ex=None, filt_len=None, **kwargs)\n\nBlurs a sequence applying a filter of type [1, 0, 1]\n\ntest_eq(TSBlur(filt_len=7)(xb, split_idx=0).shape, xb.shape)\ntest_ne(TSBlur()(xb, split_idx=0), xb)\n\n\nsource\n\n\nTSSmooth\n\n TSSmooth (magnitude=1.0, ex=None, filt_len=None, **kwargs)\n\nSmoothens a sequence applying a filter of type [1, 5, 1]\n\ntest_eq(TSSmooth(filt_len=7)(xb, split_idx=0).shape, xb.shape)\ntest_ne(TSSmooth()(xb, split_idx=0), xb)\n\n\nsource\n\n\nTSFreqDenoise\n\n TSFreqDenoise (magnitude=0.1, ex=None, wavelet='db4', level=2, thr=None,\n                thr_mode='hard', pad_mode='per', **kwargs)\n\nDenoises a sequence applying a wavelet decomposition method\n\nsource\n\n\nmaddest\n\n maddest (d, axis=None)\n\n\ntry: import pywt \nexcept ImportError: pass\n\n\nif 'pywt' in dir():\n    test_eq(TSFreqDenoise()(xb, split_idx=0).shape, xb.shape)\n    test_ne(TSFreqDenoise()(xb, split_idx=0), xb)\n\n\nsource\n\n\nTSRandomFreqNoise\n\n TSRandomFreqNoise (magnitude=0.1, ex=None, wavelet='db4', level=2,\n                    mode='constant', **kwargs)\n\nApplys random noise using a wavelet decomposition method\n\nif 'pywt' in dir():\n    test_eq(TSRandomFreqNoise()(xb, split_idx=0).shape, xb.shape)\n\n\nsource\n\n\nTSRandomResizedLookBack\n\n TSRandomResizedLookBack (magnitude=0.1, mode='linear', **kwargs)\n\nSelects a random number of sequence steps starting from the end and return an output of the same shape\n\nfor i in range(100): \n    o = TSRandomResizedLookBack()(xb, split_idx=0)\n    test_eq(o.shape[-1], xb.shape[-1])\n\n\nsource\n\n\nTSRandomLookBackOut\n\n TSRandomLookBackOut (magnitude=0.1, **kwargs)\n\nSelects a random number of sequence steps starting from the end and set them to zero\n\nfor i in range(100): \n    o = TSRandomLookBackOut()(xb, split_idx=0)\n    test_eq(o.shape[-1], xb.shape[-1])\n\n\nsource\n\n\nTSVarOut\n\n TSVarOut (magnitude=0.05, ex=None, **kwargs)\n\nSet the value of a random number of variables to zero\n\ntest_eq(TSVarOut()(xb, split_idx=0).shape, xb.shape)\n\n\nsource\n\n\nTSCutOut\n\n TSCutOut (magnitude=0.05, ex=None, **kwargs)\n\nSets a random section of the sequence to zero\n\ntest_eq(TSCutOut()(xb, split_idx=0).shape, xb.shape)\n\n\nsource\n\n\nTSTimeStepOut\n\n TSTimeStepOut (magnitude=0.05, ex=None, **kwargs)\n\nSets random sequence steps to zero\n\ntest_eq(TSTimeStepOut()(xb, split_idx=0).shape, xb.shape)\n\n\nsource\n\n\nTSRandomCropPad\n\n TSRandomCropPad (magnitude=0.05, ex=None, **kwargs)\n\nCrops a section of the sequence of a random length\n\ntest_eq(TSRandomCropPad()(xb, split_idx=0).shape, xb.shape)\n\n\nsource\n\n\nTSMaskOut\n\n TSMaskOut (magnitude=0.1, compensate:bool=False, ex=None, **kwargs)\n\nApplies a random mask\n\ntest_eq(TSMaskOut()(xb, split_idx=0).shape, xb.shape)\ntest_ne(TSMaskOut()(xb, split_idx=0), xb)\n\n\nsource\n\n\nTSInputDropout\n\n TSInputDropout (magnitude=0.0, ex=None, **kwargs)\n\nApplies input dropout with required_grad=False\n\ntest_eq(TSInputDropout(.1)(xb, split_idx=0).shape, xb.shape)\ntest_ne(TSInputDropout(.1)(xb, split_idx=0), xb)\n\n\nsource\n\n\nTSTranslateX\n\n TSTranslateX (magnitude=0.1, ex=None, **kwargs)\n\nMoves a selected sequence window a random number of steps\n\ntest_eq(TSTranslateX()(xb, split_idx=0).shape, xb.shape)\n\n\nsource\n\n\nTSRandomShift\n\n TSRandomShift (magnitude=0.02, ex=None, **kwargs)\n\nShifts and splits a sequence\n\ntest_eq(TSRandomShift()(xb, split_idx=0).shape, xb.shape)\n\n\nsource\n\n\nTSHorizontalFlip\n\n TSHorizontalFlip (magnitude=1.0, ex=None, **kwargs)\n\nFlips the sequence along the x-axis\n\ntest_eq(TSHorizontalFlip()(xb, split_idx=0).shape, xb.shape)\ntest_ne(TSHorizontalFlip()(xb, split_idx=0), xb)\n\n\nsource\n\n\nTSRandomTrend\n\n TSRandomTrend (magnitude=0.1, ex=None, **kwargs)\n\nRandomly rotates the sequence along the z-axis\n\ntest_eq(TSRandomTrend()(xb, split_idx=0).shape, xb.shape)\n\n\nsource\n\n\nTSVerticalFlip\n\n TSVerticalFlip (magnitude=1.0, ex=None, **kwargs)\n\nApplies a negative value to the time sequence\n\ntest_eq(TSVerticalFlip()(xb, split_idx=0).shape, xb.shape)\ntest_ne(TSVerticalFlip()(xb, split_idx=0), xb)\n\n\nsource\n\n\nTSResize\n\n TSResize (magnitude=-0.5, size=None, ex=None, mode='linear', **kwargs)\n\nResizes the sequence length of a time series\n\nfor sz in np.linspace(.2, 2, 10): test_eq(TSResize(sz)(xb, split_idx=0).shape[-1], int(round(xb.shape[-1]*(1+sz))))\ntest_ne(TSResize(1)(xb, split_idx=0), xb)\n\n\nsource\n\n\nTSRandomSize\n\n TSRandomSize (magnitude=0.1, ex=None, mode='linear', **kwargs)\n\nRandomly resizes the sequence length of a time series\n\nseq_len_ = []\nfor i in range(100): \n    o = TSRandomSize(.5)(xb, split_idx=0)\n    seq_len_.append(o.shape[-1])\ntest_lt(min(seq_len_), xb.shape[-1])\ntest_gt(max(seq_len_), xb.shape[-1])\n\n\nsource\n\n\nTSRandomLowRes\n\n TSRandomLowRes (magnitude=0.5, ex=None, mode='linear', **kwargs)\n\nRandomly resizes the sequence length of a time series to a lower resolution\n\nsource\n\n\nTSDownUpScale\n\n TSDownUpScale (magnitude=0.5, ex=None, mode='linear', **kwargs)\n\nDownscales a time series and upscales it again to previous sequence length\n\ntest_eq(TSDownUpScale()(xb, split_idx=0).shape, xb.shape)\n\n\nsource\n\n\nTSRandomDownUpScale\n\n TSRandomDownUpScale (magnitude=0.5, ex=None, mode='linear', **kwargs)\n\nRandomly downscales a time series and upscales it again to previous sequence length\n\ntest_eq(TSRandomDownUpScale()(xb, split_idx=0).shape, xb.shape)\ntest_ne(TSDownUpScale()(xb, split_idx=0), xb)\ntest_eq(TSDownUpScale()(xb, split_idx=1), xb)\n\n\nsource\n\n\nTSRandomConv\n\n TSRandomConv (magnitude=0.05, ex=None, ks=[1, 3, 5, 7], **kwargs)\n\nApplies a convolution with a random kernel and random weights with required_grad=False\n\nfor i in range(5): \n    o = TSRandomConv(magnitude=0.05, ex=None, ks=[1, 3, 5, 7])(xb, split_idx=0)\n    test_eq(o.shape, xb.shape)\n\n\nsource\n\n\nTSRandom2Value\n\n TSRandom2Value (magnitude=0.1, sel_vars=None, sel_steps=None,\n                 static=False, value=nan, **kwargs)\n\nRandomly sets selected variables of type TSTensor to predefined value (default: np.nan)\n\nt = TSTensor(torch.ones(2, 3, 10))\nTSRandom2Value(magnitude=0.5, sel_vars=None, sel_steps=None, static=False, value=0)(t, split_idx=0).data\n\ntensor([[[1., 0., 0., 1., 0., 1., 0., 1., 0., 1.],\n         [0., 0., 1., 0., 1., 1., 0., 0., 0., 0.],\n         [1., 1., 0., 1., 0., 0., 1., 0., 1., 1.]],\n\n        [[0., 1., 1., 0., 0., 1., 1., 1., 1., 0.],\n         [0., 0., 0., 0., 1., 1., 1., 1., 0., 1.],\n         [0., 1., 1., 1., 0., 0., 1., 1., 0., 0.]]])\n\n\n\nt = TSTensor(torch.ones(2, 3, 10))\nTSRandom2Value(magnitude=0.5, sel_vars=[1], sel_steps=slice(-5, None), static=False, value=0)(t, split_idx=0).data\n\ntensor([[[1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n         [1., 1., 1., 1., 1., 1., 1., 0., 0., 0.],\n         [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]],\n\n        [[1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n         [1., 1., 1., 1., 1., 0., 1., 0., 0., 0.],\n         [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]]])\n\n\n\nt = TSTensor(torch.ones(2, 3, 10))\nTSRandom2Value(magnitude=0.5, sel_vars=[1], sel_steps=None, static=True, value=0)(t, split_idx=0).data\n\ntensor([[[1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n         [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n         [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]],\n\n        [[1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n         [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]]])\n\n\n\nt = TSTensor(torch.ones(2, 3, 10))\nTSRandom2Value(magnitude=1, sel_vars=1, sel_steps=None, static=False, value=0)(t, split_idx=0).data\n\ntensor([[[1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n         [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]],\n\n        [[1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n         [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]]])\n\n\n\nt = TSTensor(torch.ones(2, 3, 10))\nTSRandom2Value(magnitude=1, sel_vars=[1,2], sel_steps=None, static=False, value=0)(t, split_idx=0).data\n\ntensor([[[1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n\n        [[1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]])\n\n\n\nt = TSTensor(torch.ones(2, 3, 10))\nTSRandom2Value(magnitude=1, sel_vars=1, sel_steps=[1,3,5], static=False, value=0)(t, split_idx=0).data\n\ntensor([[[1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n         [1., 0., 1., 0., 1., 0., 1., 1., 1., 1.],\n         [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]],\n\n        [[1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n         [1., 0., 1., 0., 1., 0., 1., 1., 1., 1.],\n         [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]]])\n\n\n\nt = TSTensor(torch.ones(2, 3, 10))\nTSRandom2Value(magnitude=1, sel_vars=[1,2], sel_steps=[1,3,5], static=False, value=0)(t, split_idx=0).data\n\ntensor([[[1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n         [1., 0., 1., 0., 1., 0., 1., 1., 1., 1.],\n         [1., 0., 1., 0., 1., 0., 1., 1., 1., 1.]],\n\n        [[1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n         [1., 0., 1., 0., 1., 0., 1., 1., 1., 1.],\n         [1., 0., 1., 0., 1., 0., 1., 1., 1., 1.]]])\n\n\n\nt = TSTensor(torch.ones(2,3,4))\nTSRandom2Value(magnitude=.5, sel_vars=[0,2])(t, split_idx=0).data\n\ntensor([[[nan, nan, 1., 1.],\n         [1., 1., 1., 1.],\n         [nan, nan, 1., nan]],\n\n        [[1., nan, 1., nan],\n         [1., 1., 1., 1.],\n         [1., nan, nan, nan]]])\n\n\n\nt = TSTensor(torch.ones(2,3,4))\nTSRandom2Value(magnitude=.5, sel_steps=slice(2, None))(t, split_idx=0).data\n\ntensor([[[1., 1., 1., nan],\n         [1., 1., 1., 1.],\n         [1., 1., 1., nan]],\n\n        [[1., 1., 1., 1.],\n         [1., 1., 1., 1.],\n         [1., 1., 1., 1.]]])\n\n\n\nt = TSTensor(torch.ones(2,3,100))\ntest_gt(np.isnan(TSRandom2Value(magnitude=.5)(t, split_idx=0)).sum().item(), 0)\nt = TSTensor(torch.ones(2,3,100))\ntest_gt(np.isnan(TSRandom2Value(magnitude=.5, sel_vars=[0,2])(t, split_idx=0)[:, [0,2]]).sum().item(), 0)\nt = TSTensor(torch.ones(2,3,100))\ntest_eq(np.isnan(TSRandom2Value(magnitude=.5, sel_vars=[0,2])(t, split_idx=0)[:, 1]).sum().item(), 0)\n\n\nsource\n\n\nTSMask2Value\n\n TSMask2Value (mask_fn, value=nan, sel_vars=None, **kwargs)\n\nRandomly sets selected variables of type TSTensor to predefined value (default: np.nan)\n\nt = TSTensor(torch.ones(2,3,100))\ndef _mask_fn(o, r=.15, value=np.nan):\n    return torch.rand_like(o) > (1-r)\ntest_gt(np.isnan(TSMask2Value(_mask_fn)(t, split_idx=0)).sum().item(), 0)\n\n\nsource\n\n\nRandAugment\n\n RandAugment (tfms:list, N:int=1, M:int=3, **kwargs)\n\nA transform that before_call its state at each __call__\n\ntest_ne(RandAugment(TSMagAddNoise, N=5, M=10)(xb, split_idx=0), xb)\n\n\nsource\n\n\nTestTfm\n\n TestTfm (tfm, magnitude=1.0, ex=None, **kwargs)\n\nUtility class to test the output of selected tfms during training\n\nsource\n\n\nget_tfm_name\n\n get_tfm_name (tfm)\n\n\ntest_eq(get_tfm_name(partial(TSMagScale()))==get_tfm_name((partial(TSMagScale()), 0.1, .05))==get_tfm_name(TSMagScale())==get_tfm_name((TSMagScale(), 0.1, .05)), True)\n\n\nall_TS_randaugs_names = [get_tfm_name(t) for t in all_TS_randaugs]"
  },
  {
    "objectID": "models.inceptiontimeplus.html",
    "href": "models.inceptiontimeplus.html",
    "title": "InceptionTimePlus",
    "section": "",
    "text": "This is an unofficial PyTorch implementation of InceptionTime (Fawaz, 2019) created by Ignacio Oguiza.\n\nReferences: * Fawaz, H. I., Lucas, B., Forestier, G., Pelletier, C., Schmidt, D. F., Weber, J., … & Petitjean, F. (2020). Inceptiontime: Finding alexnet for time series classification. Data Mining and Knowledge Discovery, 34(6), 1936-1962. * Official InceptionTime tensorflow implementation: https://github.com/hfawaz/InceptionTime\n\nsource\n\nInceptionBlockPlus\n\n InceptionBlockPlus (ni, nf, residual=True, depth=6, coord=False,\n                     norm='Batch', zero_norm=False, act=<class\n                     'torch.nn.modules.activation.ReLU'>, act_kwargs={},\n                     sa=False, se=None, stoch_depth=1.0, ks=40,\n                     bottleneck=True, padding='same', separable=False,\n                     dilation=1, stride=1, conv_dropout=0.0, bn_1st=True)\n\nSame as nn.Module, but no need for subclasses to call super().__init__\n\nsource\n\n\nInceptionModulePlus\n\n InceptionModulePlus (ni, nf, ks=40, bottleneck=True, padding='same',\n                      coord=False, separable=False, dilation=1, stride=1,\n                      conv_dropout=0.0, sa=False, se=None, norm='Batch',\n                      zero_norm=False, bn_1st=True, act=<class\n                      'torch.nn.modules.activation.ReLU'>, act_kwargs={})\n\nSame as nn.Module, but no need for subclasses to call super().__init__\n\nsource\n\n\nInceptionTimePlus\n\n InceptionTimePlus (c_in, c_out, seq_len=None, nf=32, nb_filters=None,\n                    flatten=False, concat_pool=False, fc_dropout=0.0,\n                    bn=False, y_range=None, custom_head=None, ks=40,\n                    bottleneck=True, padding='same', coord=False,\n                    separable=False, dilation=1, stride=1,\n                    conv_dropout=0.0, sa=False, se=None, norm='Batch',\n                    zero_norm=False, bn_1st=True, act=<class\n                    'torch.nn.modules.activation.ReLU'>, act_kwargs={})\n\nA sequential container. Modules will be added to it in the order they are passed in the constructor. Alternatively, an OrderedDict of modules can be passed in. The forward() method of [Sequential](https://timeseriesAI.github.io/models.layers.html#sequential) accepts any input and forwards it to the first module it contains. It then “chains” outputs to inputs sequentially for each subsequent module, finally returning the output of the last module.\nThe value a [Sequential](https://timeseriesAI.github.io/models.layers.html#sequential) provides over manually calling a sequence of modules is that it allows treating the whole container as a single module, such that performing a transformation on the [Sequential](https://timeseriesAI.github.io/models.layers.html#sequential) applies to each of the modules it stores (which are each a registered submodule of the [Sequential](https://timeseriesAI.github.io/models.layers.html#sequential)).\nWhat’s the difference between a [Sequential](https://timeseriesAI.github.io/models.layers.html#sequential) and a :class:torch.nn.ModuleList? A ModuleList is exactly what it sounds like–a list for storing Module s! On the other hand, the layers in a [Sequential](https://timeseriesAI.github.io/models.layers.html#sequential) are connected in a cascading way.\nExample::\n# Using Sequential to create a small model. When `model` is run,\n# input will first be passed to `Conv2d(1,20,5)`. The output of\n# `Conv2d(1,20,5)` will be used as the input to the first\n# `ReLU`; the output of the first `ReLU` will become the input\n# for `Conv2d(20,64,5)`. Finally, the output of\n# `Conv2d(20,64,5)` will be used as input to the second `ReLU`\nmodel = nn.Sequential(\n          nn.Conv2d(1,20,5),\n          nn.ReLU(),\n          nn.Conv2d(20,64,5),\n          nn.ReLU()\n        )\n\n# Using Sequential with OrderedDict. This is functionally the\n# same as the above code\nmodel = nn.Sequential(OrderedDict([\n          ('conv1', nn.Conv2d(1,20,5)),\n          ('relu1', nn.ReLU()),\n          ('conv2', nn.Conv2d(20,64,5)),\n          ('relu2', nn.ReLU())\n        ]))\n\nsource\n\n\nXCoordTime\n\n XCoordTime (c_in, c_out, seq_len=None, nf=32, nb_filters=None,\n             flatten=False, concat_pool=False, fc_dropout=0.0, bn=False,\n             y_range=None, custom_head=None, ks=40, bottleneck=True,\n             padding='same', coord=False, separable=False, dilation=1,\n             stride=1, conv_dropout=0.0, sa=False, se=None, norm='Batch',\n             zero_norm=False, bn_1st=True, act=<class\n             'torch.nn.modules.activation.ReLU'>, act_kwargs={})\n\nA sequential container. Modules will be added to it in the order they are passed in the constructor. Alternatively, an OrderedDict of modules can be passed in. The forward() method of [Sequential](https://timeseriesAI.github.io/models.layers.html#sequential) accepts any input and forwards it to the first module it contains. It then “chains” outputs to inputs sequentially for each subsequent module, finally returning the output of the last module.\nThe value a [Sequential](https://timeseriesAI.github.io/models.layers.html#sequential) provides over manually calling a sequence of modules is that it allows treating the whole container as a single module, such that performing a transformation on the [Sequential](https://timeseriesAI.github.io/models.layers.html#sequential) applies to each of the modules it stores (which are each a registered submodule of the [Sequential](https://timeseriesAI.github.io/models.layers.html#sequential)).\nWhat’s the difference between a [Sequential](https://timeseriesAI.github.io/models.layers.html#sequential) and a :class:torch.nn.ModuleList? A ModuleList is exactly what it sounds like–a list for storing Module s! On the other hand, the layers in a [Sequential](https://timeseriesAI.github.io/models.layers.html#sequential) are connected in a cascading way.\nExample::\n# Using Sequential to create a small model. When `model` is run,\n# input will first be passed to `Conv2d(1,20,5)`. The output of\n# `Conv2d(1,20,5)` will be used as the input to the first\n# `ReLU`; the output of the first `ReLU` will become the input\n# for `Conv2d(20,64,5)`. Finally, the output of\n# `Conv2d(20,64,5)` will be used as input to the second `ReLU`\nmodel = nn.Sequential(\n          nn.Conv2d(1,20,5),\n          nn.ReLU(),\n          nn.Conv2d(20,64,5),\n          nn.ReLU()\n        )\n\n# Using Sequential with OrderedDict. This is functionally the\n# same as the above code\nmodel = nn.Sequential(OrderedDict([\n          ('conv1', nn.Conv2d(1,20,5)),\n          ('relu1', nn.ReLU()),\n          ('conv2', nn.Conv2d(20,64,5)),\n          ('relu2', nn.ReLU())\n        ]))\n\nsource\n\n\nInCoordTime\n\n InCoordTime (c_in, c_out, seq_len=None, nf=32, nb_filters=None,\n              flatten=False, concat_pool=False, fc_dropout=0.0, bn=False,\n              y_range=None, custom_head=None, ks=40, bottleneck=True,\n              padding='same', coord=False, separable=False, dilation=1,\n              stride=1, conv_dropout=0.0, sa=False, se=None, norm='Batch',\n              zero_norm=False, bn_1st=True, act=<class\n              'torch.nn.modules.activation.ReLU'>, act_kwargs={})\n\nA sequential container. Modules will be added to it in the order they are passed in the constructor. Alternatively, an OrderedDict of modules can be passed in. The forward() method of [Sequential](https://timeseriesAI.github.io/models.layers.html#sequential) accepts any input and forwards it to the first module it contains. It then “chains” outputs to inputs sequentially for each subsequent module, finally returning the output of the last module.\nThe value a [Sequential](https://timeseriesAI.github.io/models.layers.html#sequential) provides over manually calling a sequence of modules is that it allows treating the whole container as a single module, such that performing a transformation on the [Sequential](https://timeseriesAI.github.io/models.layers.html#sequential) applies to each of the modules it stores (which are each a registered submodule of the [Sequential](https://timeseriesAI.github.io/models.layers.html#sequential)).\nWhat’s the difference between a [Sequential](https://timeseriesAI.github.io/models.layers.html#sequential) and a :class:torch.nn.ModuleList? A ModuleList is exactly what it sounds like–a list for storing Module s! On the other hand, the layers in a [Sequential](https://timeseriesAI.github.io/models.layers.html#sequential) are connected in a cascading way.\nExample::\n# Using Sequential to create a small model. When `model` is run,\n# input will first be passed to `Conv2d(1,20,5)`. The output of\n# `Conv2d(1,20,5)` will be used as the input to the first\n# `ReLU`; the output of the first `ReLU` will become the input\n# for `Conv2d(20,64,5)`. Finally, the output of\n# `Conv2d(20,64,5)` will be used as input to the second `ReLU`\nmodel = nn.Sequential(\n          nn.Conv2d(1,20,5),\n          nn.ReLU(),\n          nn.Conv2d(20,64,5),\n          nn.ReLU()\n        )\n\n# Using Sequential with OrderedDict. This is functionally the\n# same as the above code\nmodel = nn.Sequential(OrderedDict([\n          ('conv1', nn.Conv2d(1,20,5)),\n          ('relu1', nn.ReLU()),\n          ('conv2', nn.Conv2d(20,64,5)),\n          ('relu2', nn.ReLU())\n        ]))\n\nfrom tsai.data.core import TSCategorize\nfrom tsai.models.utils import count_parameters\n\n\nbs = 16\nn_vars = 3\nseq_len = 51\nc_out = 2\nxb = torch.rand(bs, n_vars, seq_len)\n\ntest_eq(InceptionTimePlus(n_vars,c_out)(xb).shape, [bs, c_out])\ntest_eq(InceptionTimePlus(n_vars,c_out,concat_pool=True)(xb).shape, [bs, c_out])\ntest_eq(InceptionTimePlus(n_vars,c_out, bottleneck=False)(xb).shape, [bs, c_out])\ntest_eq(InceptionTimePlus(n_vars,c_out, residual=False)(xb).shape, [bs, c_out])\ntest_eq(InceptionTimePlus(n_vars,c_out, conv_dropout=.5)(xb).shape, [bs, c_out])\ntest_eq(InceptionTimePlus(n_vars,c_out, stoch_depth=.5)(xb).shape, [bs, c_out])\ntest_eq(InceptionTimePlus(n_vars, c_out, seq_len=seq_len, zero_norm=True, flatten=True)(xb).shape, [bs, c_out])\ntest_eq(InceptionTimePlus(n_vars,c_out, coord=True, separable=True, \n                          norm='Instance', zero_norm=True, bn_1st=False, fc_dropout=.5, sa=True, se=True, act=nn.PReLU, act_kwargs={})(xb).shape, [bs, c_out])\ntest_eq(InceptionTimePlus(n_vars,c_out, coord=True, separable=True,\n                          norm='Instance', zero_norm=True, bn_1st=False, act=nn.PReLU, act_kwargs={})(xb).shape, [bs, c_out])\ntest_eq(count_parameters(InceptionTimePlus(3, 2)), 455490)\ntest_eq(count_parameters(InceptionTimePlus(6, 2, **{'coord': True, 'separable': True, 'zero_norm': True})), 77204)\ntest_eq(count_parameters(InceptionTimePlus(3, 2, ks=40)), count_parameters(InceptionTimePlus(3, 2, ks=[9, 19, 39])))\n\n\nbs = 16\nn_vars = 3\nseq_len = 51\nc_out = 2\nxb = torch.rand(bs, n_vars, seq_len)\n\nmodel = InceptionTimePlus(n_vars, c_out)\nmodel(xb).shape\ntest_eq(model[0](xb), model.backbone(xb))\ntest_eq(model[1](model[0](xb)), model.head(model[0](xb)))\ntest_eq(model[1].state_dict().keys(), model.head.state_dict().keys())\ntest_eq(len(ts_splitter(model)), 2)\n\n\ntest_eq(check_bias(InceptionTimePlus(2,3, zero_norm=True), is_conv)[0].sum(), 0)\ntest_eq(check_weight(InceptionTimePlus(2,3, zero_norm=True), is_bn)[0].sum(), 6)\ntest_eq(check_weight(InceptionTimePlus(2,3), is_bn)[0], np.array([1., 1., 1., 1., 1., 1., 1., 1.]))\n\n\nfor i in range(10): InceptionTimePlus(n_vars,c_out,stoch_depth=0.8,depth=9,zero_norm=True)(xb)\n\n\nnet = InceptionTimePlus(2,3,**{'coord': True, 'separable': True, 'zero_norm': True})\ntest_eq(check_weight(net, is_bn)[0], np.array([1., 1., 0., 1., 1., 0., 1., 1.]))\nnet\n\nInceptionTimePlus(\n  (backbone): Sequential(\n    (0): InceptionBlockPlus(\n      (inception): ModuleList(\n        (0): InceptionModulePlus(\n          (bottleneck): ConvBlock(\n            (0): AddCoords1d()\n            (1): Conv1d(3, 32, kernel_size=(1,), stride=(1,), bias=False)\n          )\n          (convs): ModuleList(\n            (0): ConvBlock(\n              (0): AddCoords1d()\n              (1): SeparableConv1d(\n                (depthwise_conv): Conv1d(33, 33, kernel_size=(39,), stride=(1,), padding=(19,), groups=33, bias=False)\n                (pointwise_conv): Conv1d(33, 32, kernel_size=(1,), stride=(1,), bias=False)\n              )\n            )\n            (1): ConvBlock(\n              (0): AddCoords1d()\n              (1): SeparableConv1d(\n                (depthwise_conv): Conv1d(33, 33, kernel_size=(19,), stride=(1,), padding=(9,), groups=33, bias=False)\n                (pointwise_conv): Conv1d(33, 32, kernel_size=(1,), stride=(1,), bias=False)\n              )\n            )\n            (2): ConvBlock(\n              (0): AddCoords1d()\n              (1): SeparableConv1d(\n                (depthwise_conv): Conv1d(33, 33, kernel_size=(9,), stride=(1,), padding=(4,), groups=33, bias=False)\n                (pointwise_conv): Conv1d(33, 32, kernel_size=(1,), stride=(1,), bias=False)\n              )\n            )\n          )\n          (mp_conv): Sequential(\n            (0): MaxPool1d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)\n            (1): ConvBlock(\n              (0): AddCoords1d()\n              (1): Conv1d(3, 32, kernel_size=(1,), stride=(1,), bias=False)\n            )\n          )\n          (concat): Concat(dim=1)\n          (norm): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (act): ReLU()\n        )\n        (1): InceptionModulePlus(\n          (bottleneck): ConvBlock(\n            (0): AddCoords1d()\n            (1): Conv1d(129, 32, kernel_size=(1,), stride=(1,), bias=False)\n          )\n          (convs): ModuleList(\n            (0): ConvBlock(\n              (0): AddCoords1d()\n              (1): SeparableConv1d(\n                (depthwise_conv): Conv1d(33, 33, kernel_size=(39,), stride=(1,), padding=(19,), groups=33, bias=False)\n                (pointwise_conv): Conv1d(33, 32, kernel_size=(1,), stride=(1,), bias=False)\n              )\n            )\n            (1): ConvBlock(\n              (0): AddCoords1d()\n              (1): SeparableConv1d(\n                (depthwise_conv): Conv1d(33, 33, kernel_size=(19,), stride=(1,), padding=(9,), groups=33, bias=False)\n                (pointwise_conv): Conv1d(33, 32, kernel_size=(1,), stride=(1,), bias=False)\n              )\n            )\n            (2): ConvBlock(\n              (0): AddCoords1d()\n              (1): SeparableConv1d(\n                (depthwise_conv): Conv1d(33, 33, kernel_size=(9,), stride=(1,), padding=(4,), groups=33, bias=False)\n                (pointwise_conv): Conv1d(33, 32, kernel_size=(1,), stride=(1,), bias=False)\n              )\n            )\n          )\n          (mp_conv): Sequential(\n            (0): MaxPool1d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)\n            (1): ConvBlock(\n              (0): AddCoords1d()\n              (1): Conv1d(129, 32, kernel_size=(1,), stride=(1,), bias=False)\n            )\n          )\n          (concat): Concat(dim=1)\n          (norm): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (act): ReLU()\n        )\n        (2): InceptionModulePlus(\n          (bottleneck): ConvBlock(\n            (0): AddCoords1d()\n            (1): Conv1d(129, 32, kernel_size=(1,), stride=(1,), bias=False)\n          )\n          (convs): ModuleList(\n            (0): ConvBlock(\n              (0): AddCoords1d()\n              (1): SeparableConv1d(\n                (depthwise_conv): Conv1d(33, 33, kernel_size=(39,), stride=(1,), padding=(19,), groups=33, bias=False)\n                (pointwise_conv): Conv1d(33, 32, kernel_size=(1,), stride=(1,), bias=False)\n              )\n            )\n            (1): ConvBlock(\n              (0): AddCoords1d()\n              (1): SeparableConv1d(\n                (depthwise_conv): Conv1d(33, 33, kernel_size=(19,), stride=(1,), padding=(9,), groups=33, bias=False)\n                (pointwise_conv): Conv1d(33, 32, kernel_size=(1,), stride=(1,), bias=False)\n              )\n            )\n            (2): ConvBlock(\n              (0): AddCoords1d()\n              (1): SeparableConv1d(\n                (depthwise_conv): Conv1d(33, 33, kernel_size=(9,), stride=(1,), padding=(4,), groups=33, bias=False)\n                (pointwise_conv): Conv1d(33, 32, kernel_size=(1,), stride=(1,), bias=False)\n              )\n            )\n          )\n          (mp_conv): Sequential(\n            (0): MaxPool1d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)\n            (1): ConvBlock(\n              (0): AddCoords1d()\n              (1): Conv1d(129, 32, kernel_size=(1,), stride=(1,), bias=False)\n            )\n          )\n          (concat): Concat(dim=1)\n          (norm): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n        (3): InceptionModulePlus(\n          (bottleneck): ConvBlock(\n            (0): AddCoords1d()\n            (1): Conv1d(129, 32, kernel_size=(1,), stride=(1,), bias=False)\n          )\n          (convs): ModuleList(\n            (0): ConvBlock(\n              (0): AddCoords1d()\n              (1): SeparableConv1d(\n                (depthwise_conv): Conv1d(33, 33, kernel_size=(39,), stride=(1,), padding=(19,), groups=33, bias=False)\n                (pointwise_conv): Conv1d(33, 32, kernel_size=(1,), stride=(1,), bias=False)\n              )\n            )\n            (1): ConvBlock(\n              (0): AddCoords1d()\n              (1): SeparableConv1d(\n                (depthwise_conv): Conv1d(33, 33, kernel_size=(19,), stride=(1,), padding=(9,), groups=33, bias=False)\n                (pointwise_conv): Conv1d(33, 32, kernel_size=(1,), stride=(1,), bias=False)\n              )\n            )\n            (2): ConvBlock(\n              (0): AddCoords1d()\n              (1): SeparableConv1d(\n                (depthwise_conv): Conv1d(33, 33, kernel_size=(9,), stride=(1,), padding=(4,), groups=33, bias=False)\n                (pointwise_conv): Conv1d(33, 32, kernel_size=(1,), stride=(1,), bias=False)\n              )\n            )\n          )\n          (mp_conv): Sequential(\n            (0): MaxPool1d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)\n            (1): ConvBlock(\n              (0): AddCoords1d()\n              (1): Conv1d(129, 32, kernel_size=(1,), stride=(1,), bias=False)\n            )\n          )\n          (concat): Concat(dim=1)\n          (norm): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (act): ReLU()\n        )\n        (4): InceptionModulePlus(\n          (bottleneck): ConvBlock(\n            (0): AddCoords1d()\n            (1): Conv1d(129, 32, kernel_size=(1,), stride=(1,), bias=False)\n          )\n          (convs): ModuleList(\n            (0): ConvBlock(\n              (0): AddCoords1d()\n              (1): SeparableConv1d(\n                (depthwise_conv): Conv1d(33, 33, kernel_size=(39,), stride=(1,), padding=(19,), groups=33, bias=False)\n                (pointwise_conv): Conv1d(33, 32, kernel_size=(1,), stride=(1,), bias=False)\n              )\n            )\n            (1): ConvBlock(\n              (0): AddCoords1d()\n              (1): SeparableConv1d(\n                (depthwise_conv): Conv1d(33, 33, kernel_size=(19,), stride=(1,), padding=(9,), groups=33, bias=False)\n                (pointwise_conv): Conv1d(33, 32, kernel_size=(1,), stride=(1,), bias=False)\n              )\n            )\n            (2): ConvBlock(\n              (0): AddCoords1d()\n              (1): SeparableConv1d(\n                (depthwise_conv): Conv1d(33, 33, kernel_size=(9,), stride=(1,), padding=(4,), groups=33, bias=False)\n                (pointwise_conv): Conv1d(33, 32, kernel_size=(1,), stride=(1,), bias=False)\n              )\n            )\n          )\n          (mp_conv): Sequential(\n            (0): MaxPool1d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)\n            (1): ConvBlock(\n              (0): AddCoords1d()\n              (1): Conv1d(129, 32, kernel_size=(1,), stride=(1,), bias=False)\n            )\n          )\n          (concat): Concat(dim=1)\n          (norm): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (act): ReLU()\n        )\n        (5): InceptionModulePlus(\n          (bottleneck): ConvBlock(\n            (0): AddCoords1d()\n            (1): Conv1d(129, 32, kernel_size=(1,), stride=(1,), bias=False)\n          )\n          (convs): ModuleList(\n            (0): ConvBlock(\n              (0): AddCoords1d()\n              (1): SeparableConv1d(\n                (depthwise_conv): Conv1d(33, 33, kernel_size=(39,), stride=(1,), padding=(19,), groups=33, bias=False)\n                (pointwise_conv): Conv1d(33, 32, kernel_size=(1,), stride=(1,), bias=False)\n              )\n            )\n            (1): ConvBlock(\n              (0): AddCoords1d()\n              (1): SeparableConv1d(\n                (depthwise_conv): Conv1d(33, 33, kernel_size=(19,), stride=(1,), padding=(9,), groups=33, bias=False)\n                (pointwise_conv): Conv1d(33, 32, kernel_size=(1,), stride=(1,), bias=False)\n              )\n            )\n            (2): ConvBlock(\n              (0): AddCoords1d()\n              (1): SeparableConv1d(\n                (depthwise_conv): Conv1d(33, 33, kernel_size=(9,), stride=(1,), padding=(4,), groups=33, bias=False)\n                (pointwise_conv): Conv1d(33, 32, kernel_size=(1,), stride=(1,), bias=False)\n              )\n            )\n          )\n          (mp_conv): Sequential(\n            (0): MaxPool1d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)\n            (1): ConvBlock(\n              (0): AddCoords1d()\n              (1): Conv1d(129, 32, kernel_size=(1,), stride=(1,), bias=False)\n            )\n          )\n          (concat): Concat(dim=1)\n          (norm): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n      )\n      (shortcut): ModuleList(\n        (0): ConvBlock(\n          (0): AddCoords1d()\n          (1): Conv1d(3, 128, kernel_size=(1,), stride=(1,), bias=False)\n          (2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n        (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n      (act): ModuleList(\n        (0): ReLU()\n        (1): ReLU()\n      )\n      (add): Add\n    )\n  )\n  (head): Sequential(\n    (0): Sequential(\n      (0): GAP1d(\n        (gap): AdaptiveAvgPool1d(output_size=1)\n        (flatten): Reshape(bs)\n      )\n      (1): LinBnDrop(\n        (0): Linear(in_features=128, out_features=3, bias=True)\n      )\n    )\n  )\n)\n\n\n\nsource\n\n\nMultiInceptionTimePlus\n\n MultiInceptionTimePlus (feat_list, c_out, seq_len=None, nf=32,\n                         nb_filters=None, depth=6, stoch_depth=1.0,\n                         flatten=False, concat_pool=False, fc_dropout=0.0,\n                         bn=False, y_range=None, custom_head=None)\n\nClass that allows you to create a model with multiple branches of InceptionTimePlus.\n\nbs = 16\nn_vars = 3\nseq_len = 51\nc_out = 2\nxb = torch.rand(bs, n_vars, seq_len)\n\ntest_eq(count_parameters(MultiInceptionTimePlus([1,1,1], c_out)) > count_parameters(MultiInceptionTimePlus(3, c_out)), True)\ntest_eq(MultiInceptionTimePlus([1,1,1], c_out).to(xb.device)(xb).shape, MultiInceptionTimePlus(3, c_out).to(xb.device)(xb).shape)\n\n[W NNPACK.cpp:53] Could not initialize NNPACK! Reason: Unsupported hardware.\n\n\n\nbs = 16\nn_vars = 3\nseq_len = 12\nc_out = 10\nxb = torch.rand(bs, n_vars, seq_len)\nnew_head = partial(conv_lin_nd_head, d=(5,2))\nnet = MultiInceptionTimePlus(n_vars, c_out, seq_len, custom_head=new_head)\nprint(net.to(xb.device)(xb).shape)\nnet.head\n\ntorch.Size([16, 5, 2, 10])\n\n\nSequential(\n  (0): create_conv_lin_nd_head(\n    (0): Conv1d(128, 10, kernel_size=(1,), stride=(1,))\n    (1): Linear(in_features=12, out_features=10, bias=True)\n    (2): Transpose(-1, -2)\n    (3): Reshape(bs, 5, 2, 10)\n  )\n)\n\n\n\nbs = 16\nn_vars = 6\nseq_len = 12\nc_out = 2\nxb = torch.rand(bs, n_vars, seq_len)\nnet = MultiInceptionTimePlus([1,2,3], c_out, seq_len)\nprint(net.to(xb.device)(xb).shape)\nnet.head\n\ntorch.Size([16, 2])\n\n\nSequential(\n  (0): Sequential(\n    (0): GAP1d(\n      (gap): AdaptiveAvgPool1d(output_size=1)\n      (flatten): Reshape(bs)\n    )\n    (1): LinBnDrop(\n      (0): Linear(in_features=384, out_features=2, bias=True)\n    )\n  )\n)\n\n\n\nbs = 8\nc_in = 7  # aka channels, features, variables, dimensions\nc_out = 2\nseq_len = 10\nxb2 = torch.randn(bs, c_in, seq_len)\nmodel1 = MultiInceptionTimePlus([2, 5], c_out, seq_len)\nmodel2 = MultiInceptionTimePlus([[0,2,5], [0,1,3,4,6]], c_out, seq_len)\ntest_eq(model1.to(xb2.device)(xb2).shape, (bs, c_out))\ntest_eq(model1.to(xb2.device)(xb2).shape, model2.to(xb2.device)(xb2).shape)\n\n\nfrom tsai.data.external import *\nfrom tsai.data.core import *\nfrom tsai.data.preprocessing import *\n\n\nX, y, splits = get_UCR_data('NATOPS', split_data=False)\ntfms  = [None, [TSCategorize()]]\nbatch_tfms = TSStandardize()\ndls = get_ts_dls(X, y, splits=splits, tfms=tfms, batch_tfms=batch_tfms)\nmodel = InceptionTimePlus(dls.vars, dls.c, dls.len)\nxb,yb=first(dls.train)\ntest_eq(model.to(xb.device)(xb).shape, (dls.bs, dls.c))\ntest_eq(count_parameters(model), 460038)\n\n\nX, y, splits = get_UCR_data('NATOPS', split_data=False)\ntfms  = [None, [TSCategorize()]]\nbatch_tfms = TSStandardize()\ndls = get_ts_dls(X, y, splits=splits, tfms=tfms, batch_tfms=batch_tfms)\nmodel = MultiInceptionTimePlus([4, 15, 5], dls.c, dls.len)\nxb,yb=first(dls.train)\ntest_eq(model.to(xb.device)(xb).shape, (dls.bs, dls.c))\ntest_eq(count_parameters(model), 1370886)"
  },
  {
    "objectID": "wandb.html",
    "href": "wandb.html",
    "title": "Weights & Biases Sweeps",
    "section": "",
    "text": "Weights & Biases Sweeps are used to automate hyperparameter optimization and explore the space of possible models.\n\n\nsource\n\nwandb_agent\n\n wandb_agent (script_path, sweep, entity=None, project=None, count=None,\n              run=True)\n\nRun wandb agent with sweep and `script_path\n\nsource\n\n\nupdate_run_config\n\n update_run_config (config, new_config, verbose=False)\n\nUpdate config with new_config\n\nsource\n\n\nget_sweep_config\n\n get_sweep_config (config)\n\nGet sweep config from config"
  },
  {
    "objectID": "callback.mvp.html",
    "href": "callback.mvp.html",
    "title": "MVP (aka TSBERT)",
    "section": "",
    "text": "Self-Supervised Pretraining of Time Series Models\n\nMasked Value Predictor callback used to predict time series step values after a binary mask has been applied.\n\nsource\n\nself_mask\n\n self_mask (o)\n\n\nsource\n\n\ncreate_future_mask\n\n create_future_mask (o, r=0.15, sync=False)\n\n\nsource\n\n\ncreate_variable_mask\n\n create_variable_mask (o, r=0.15)\n\n\nsource\n\n\ncreate_subsequence_mask\n\n create_subsequence_mask (o, r=0.15, lm=3, stateful=True, sync=False)\n\n\nt = torch.rand(16, 3, 100)\nmask = create_subsequence_mask(t, sync=False)\ntest_eq(mask.shape, t.shape)\nmask = create_subsequence_mask(t, sync=True)\ntest_eq(mask.shape, t.shape)\nmask = create_variable_mask(t)\ntest_eq(mask.shape, t.shape)\nmask = create_future_mask(t)\ntest_eq(mask.shape, t.shape)\n\n\no = torch.randn(2, 3, 4)\no[o>.5] = np.nan\ntest_eq(torch.isnan(self_mask(o)).sum(), 0)\n\n\nt = torch.rand(16, 30, 100)\nmask = create_subsequence_mask(t, r=.15) # default settings\ntest_eq(mask.dtype, torch.bool)\nplt.figure(figsize=(10, 3))\nplt.pcolormesh(mask[0], cmap='cool')\nplt.title(f'sample 0 subsequence mask (sync=False) - default mean: {mask[0].float().mean().item():.3f}')\nplt.show()\nplt.figure(figsize=(10, 3))\nplt.pcolormesh(mask[1], cmap='cool')\nplt.title(f'sample 1 subsequence mask (sync=False) - default mean: {mask[1].float().mean().item():.3f}')\nplt.show()\n\n\n\n\n\n\n\n\nt = torch.rand(16, 30, 100)\nmask = create_subsequence_mask(t, r=.5) # 50% of values masked\ntest_eq(mask.dtype, torch.bool)\nplt.figure(figsize=(10, 3))\nplt.pcolormesh(mask[0], cmap='cool')\nplt.title(f'sample 0 subsequence mask (r=.5) mean: {mask[0].float().mean().item():.3f}')\nplt.show()\n\n\n\n\n\nt = torch.rand(16, 30, 100)\nmask = create_subsequence_mask(t, lm=5) # average length of mask = 5 \ntest_eq(mask.dtype, torch.bool)\nplt.figure(figsize=(10, 3))\nplt.pcolormesh(mask[0], cmap='cool')\nplt.title(f'sample 0 subsequence mask (lm=5) mean: {mask[0].float().mean().item():.3f}')\nplt.show()\n\n\n\n\n\nt = torch.rand(16, 30, 100)\nmask = create_subsequence_mask(t, stateful=False) # individual time steps masked \ntest_eq(mask.dtype, torch.bool)\nplt.figure(figsize=(10, 3))\nplt.pcolormesh(mask[0], cmap='cool')\nplt.title(f'per sample subsequence mask (stateful=False) mean: {mask[0].float().mean().item():.3f}')\nplt.show()\n\n\n\n\n\nt = torch.rand(1, 30, 100)\nmask = create_subsequence_mask(t, sync=True) # all time steps masked simultaneously\ntest_eq(mask.dtype, torch.bool)\nplt.figure(figsize=(10, 3))\nplt.pcolormesh(mask[0], cmap='cool')\nplt.title(f'per sample subsequence mask (sync=True) mean: {mask[0].float().mean().item():.3f}')\nplt.show()\n\n\n\n\n\nt = torch.rand(1, 30, 100)\nmask = create_variable_mask(t) # masked variables\ntest_eq(mask.dtype, torch.bool)\nplt.figure(figsize=(10, 3))\nplt.pcolormesh(mask[0], cmap='cool')\nplt.title(f'per sample variable mask mean: {mask[0].float().mean().item():.3f}')\nplt.show()\n\n\n\n\n\nt = torch.rand(1, 30, 100)\nmask = create_future_mask(t, r=.15, sync=True) # masked steps\ntest_eq(mask.dtype, torch.bool)\nplt.figure(figsize=(10, 3))\nplt.pcolormesh(mask[0], cmap='cool')\nplt.title(f'future mask mean: {mask[0].float().mean().item():.3f}')\nplt.show()\n\n\n\n\n\nt = torch.rand(1, 30, 100)\nmask = create_future_mask(t, r=.15, sync=False) # masked steps\nmask = create_future_mask(t, r=.15, sync=True) # masked steps\ntest_eq(mask.dtype, torch.bool)\nplt.figure(figsize=(10, 3))\nplt.pcolormesh(mask[0], cmap='cool')\nplt.title(f'future mask mean: {mask[0].float().mean().item():.3f}')\nplt.show()\n\n\n\n\n\nsource\n\n\ncreate_mask\n\n create_mask (o, r=0.15, lm=3, stateful=True, sync=False,\n              subsequence_mask=True, variable_mask=False,\n              future_mask=False)\n\n\nsource\n\n\nMVP\n\n MVP (r:float=0.15, subsequence_mask:bool=True, lm:float=3.0,\n      stateful:bool=True, sync:bool=False, variable_mask:bool=False,\n      future_mask:bool=False, custom_mask:Optional=None,\n      sel_vars:Optional[list]=None, nan_to_num:int=0,\n      window_size:Optional[tuple]=None, dropout:float=0.1, crit:<built-\n      infunctioncallable>=None, weights_path:Optional[str]=None,\n      target_dir:str='./models/MVP', fname:str='model',\n      save_best:bool=True, verbose:bool=False)\n\nBasic class handling tweaks of the training loop by changing a Learner in various events\n\n\nExperiments\n\nfrom tsai.data.external import get_UCR_data, check_data\nfrom tsai.data.preprocessing import TSStandardize, TSNan2Value\nfrom tsai.data.core import TSCategorize, get_ts_dls\nfrom tsai.learner import ts_learner\nfrom tsai.models.InceptionTimePlus import InceptionTimePlus\n\n\ndsid = 'MoteStrain'\nX, y, splits = get_UCR_data(dsid, split_data=False)\ncheck_data(X, y, splits, False)\nX[X<-1] = np.nan # This is to test the model works well even if nan values are passed through the dataloaders.\n\nX      - shape: [1272 samples x 1 features x 84 timesteps]  type: memmap  dtype:float32  isnan: 0\ny      - shape: (1272,)  type: memmap  dtype:<U1  n_classes: 2 (636 samples per class) ['1', '2']  isnan: False\nsplits - n_splits: 2 shape: [20, 1252]  overlap: False\n\n\n\n# Pre-train\ntfms  = [None, [TSCategorize()]]\nbatch_tfms = [TSStandardize(by_var=True)]\nunlabeled_dls = get_ts_dls(X, splits=splits, tfms=tfms, batch_tfms=batch_tfms)\nlearn = ts_learner(unlabeled_dls, InceptionTimePlus, cbs=[MVP(fname=f'{dsid}', window_size=(.5, 1))]) # trained on variable window size\nlearn.fit_one_cycle(1, 3e-3)\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      time\n    \n  \n  \n    \n      0\n      1.270972\n      1.194974\n      00:06\n    \n  \n\n\n\n\nlearn = ts_learner(unlabeled_dls, InceptionTimePlus, cbs=[MVP(weights_path=f'models/MVP/{dsid}.pth')])\nlearn.fit_one_cycle(1, 3e-3)\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      time\n    \n  \n  \n    \n      0\n      0.837741\n      1.200484\n      00:07\n    \n  \n\n\n\n\nlearn.MVP.show_preds(sharey=True) # these preds are highly inaccurate as the model's been trained for just 1 epoch for testing purposes\n\n\n\n\n\n# Fine-tune\ntfms  = [None, [TSCategorize()]]\nbatch_tfms = [TSStandardize(by_var=True), TSNan2Value()]\nlabeled_dls = get_ts_dls(X, y, splits=splits, tfms=tfms, batch_tfms=batch_tfms, bs=64)\nlearn = ts_learner(labeled_dls, InceptionTimePlus, pretrained=True, weights_path=f'models/MVP/{dsid}.pth', metrics=accuracy)\nlearn.fit_one_cycle(1)\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      accuracy\n      time\n    \n  \n  \n    \n      0\n      0.773015\n      0.744267\n      0.460863\n      00:09\n    \n  \n\n\n\n\ntfms  = [None, [TSCategorize()]]\nbatch_tfms = [TSStandardize(by_var=True), TSNan2Value()]\nunlabeled_dls = get_ts_dls(X, splits=splits, tfms=tfms, batch_tfms=batch_tfms, bs=64)\nfname = f'{dsid}_test'\nmvp = MVP(subsequence_mask=True, sync='random', variable_mask=True, future_mask=True, fname=fname)\nlearn = ts_learner(unlabeled_dls, InceptionTimePlus, metrics=accuracy, cbs=mvp) # Metrics will not be used!\n\n/Users/nacho/opt/anaconda3/envs/py37torch113/lib/python3.7/site-packages/ipykernel_launcher.py:42: UserWarning: Only future_mask will be used\n\n\n\ntfms  = [None, [TSCategorize()]]\nbatch_tfms = [TSStandardize(by_var=True)]\nunlabeled_dls = get_ts_dls(X, splits=splits, tfms=tfms, batch_tfms=batch_tfms, bs=64)\nfname = f'{dsid}_test'\nmvp = MVP(subsequence_mask=True, sync='random', variable_mask=True, future_mask=True, custom_mask=partial(create_future_mask, r=.15),\n                fname=fname)\nlearn = ts_learner(unlabeled_dls, InceptionTimePlus, metrics=accuracy, cbs=mvp) # Metrics will not be used!\n\n/Users/nacho/opt/anaconda3/envs/py37torch113/lib/python3.7/site-packages/ipykernel_launcher.py:40: UserWarning: Only custom_mask will be used\n\n\n\ntry: os.remove(\"models/MVP/MoteStrain.pth\")\nexcept OSError: pass\ntry: os.remove(\"models/MVP/model.pth\")\nexcept OSError: pass"
  },
  {
    "objectID": "models.minirocketplus_pytorch.html",
    "href": "models.minirocketplus_pytorch.html",
    "title": "MINIROCKETPlus Pytorch",
    "section": "",
    "text": "This is a modified Pytorch implementation of MiniRocket originally developed by Malcolm McLean and Ignacio Oguiza and based on:\nDempster, A., Schmidt, D. F., & Webb, G. I. (2020). MINIROCKET: A Very Fast (Almost) Deterministic Transform for Time Series Classification. arXiv preprint arXiv:2012.08791.\nOriginal paper: https://arxiv.org/abs/2012.08791\nOriginal code: https://github.com/angus924/minirocket\n\nsource\n\nMiniRocketFeaturesPlus\n\n MiniRocketFeaturesPlus (c_in, seq_len, num_features=10000,\n                         max_dilations_per_kernel=32, kernel_size=9,\n                         max_num_channels=9, max_num_kernels=84,\n                         add_lsaz=False)\n\nBase class for all neural network modules.\nYour models should also subclass this class.\nModules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes::\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 20, 5)\n        self.conv2 = nn.Conv2d(20, 20, 5)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        return F.relu(self.conv2(x))\nSubmodules assigned in this way will be registered, and will have their parameters converted too when you call :meth:to, etc.\n.. note:: As per the example above, an __init__() call to the parent class must be made before assignment on the child.\n:ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool\n\nsource\n\n\nMiniRocketPlus\n\n MiniRocketPlus (c_in, c_out, seq_len, num_features=10000,\n                 max_dilations_per_kernel=32, kernel_size=9,\n                 max_num_channels=None, max_num_kernels=84, bn=True,\n                 fc_dropout=0, add_lsaz=False, custom_head=None)\n\nA sequential container. Modules will be added to it in the order they are passed in the constructor. Alternatively, an OrderedDict of modules can be passed in. The forward() method of [Sequential](https://timeseriesAI.github.io/models.layers.html#sequential) accepts any input and forwards it to the first module it contains. It then “chains” outputs to inputs sequentially for each subsequent module, finally returning the output of the last module.\nThe value a [Sequential](https://timeseriesAI.github.io/models.layers.html#sequential) provides over manually calling a sequence of modules is that it allows treating the whole container as a single module, such that performing a transformation on the [Sequential](https://timeseriesAI.github.io/models.layers.html#sequential) applies to each of the modules it stores (which are each a registered submodule of the [Sequential](https://timeseriesAI.github.io/models.layers.html#sequential)).\nWhat’s the difference between a [Sequential](https://timeseriesAI.github.io/models.layers.html#sequential) and a :class:torch.nn.ModuleList? A ModuleList is exactly what it sounds like–a list for storing Module s! On the other hand, the layers in a [Sequential](https://timeseriesAI.github.io/models.layers.html#sequential) are connected in a cascading way.\nExample::\n# Using Sequential to create a small model. When `model` is run,\n# input will first be passed to `Conv2d(1,20,5)`. The output of\n# `Conv2d(1,20,5)` will be used as the input to the first\n# `ReLU`; the output of the first `ReLU` will become the input\n# for `Conv2d(20,64,5)`. Finally, the output of\n# `Conv2d(20,64,5)` will be used as input to the second `ReLU`\nmodel = nn.Sequential(\n          nn.Conv2d(1,20,5),\n          nn.ReLU(),\n          nn.Conv2d(20,64,5),\n          nn.ReLU()\n        )\n\n# Using Sequential with OrderedDict. This is functionally the\n# same as the above code\nmodel = nn.Sequential(OrderedDict([\n          ('conv1', nn.Conv2d(1,20,5)),\n          ('relu1', nn.ReLU()),\n          ('conv2', nn.Conv2d(20,64,5)),\n          ('relu2', nn.ReLU())\n        ]))\n\nsource\n\n\nFlatten\n\n Flatten ()\n\nBase class for all neural network modules.\nYour models should also subclass this class.\nModules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes::\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 20, 5)\n        self.conv2 = nn.Conv2d(20, 20, 5)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        return F.relu(self.conv2(x))\nSubmodules assigned in this way will be registered, and will have their parameters converted too when you call :meth:to, etc.\n.. note:: As per the example above, an __init__() call to the parent class must be made before assignment on the child.\n:ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool\n\nsource\n\n\nget_minirocket_features\n\n get_minirocket_features (o, model, chunksize=1024, use_cuda=None,\n                          to_np=False)\n\nFunction used to split a large dataset into chunks, avoiding OOM error.\n\nsource\n\n\nMiniRocketHead\n\n MiniRocketHead (c_in, c_out, seq_len=1, bn=True, fc_dropout=0.0)\n\nA sequential container. Modules will be added to it in the order they are passed in the constructor. Alternatively, an OrderedDict of modules can be passed in. The forward() method of [Sequential](https://timeseriesAI.github.io/models.layers.html#sequential) accepts any input and forwards it to the first module it contains. It then “chains” outputs to inputs sequentially for each subsequent module, finally returning the output of the last module.\nThe value a [Sequential](https://timeseriesAI.github.io/models.layers.html#sequential) provides over manually calling a sequence of modules is that it allows treating the whole container as a single module, such that performing a transformation on the [Sequential](https://timeseriesAI.github.io/models.layers.html#sequential) applies to each of the modules it stores (which are each a registered submodule of the [Sequential](https://timeseriesAI.github.io/models.layers.html#sequential)).\nWhat’s the difference between a [Sequential](https://timeseriesAI.github.io/models.layers.html#sequential) and a :class:torch.nn.ModuleList? A ModuleList is exactly what it sounds like–a list for storing Module s! On the other hand, the layers in a [Sequential](https://timeseriesAI.github.io/models.layers.html#sequential) are connected in a cascading way.\nExample::\n# Using Sequential to create a small model. When `model` is run,\n# input will first be passed to `Conv2d(1,20,5)`. The output of\n# `Conv2d(1,20,5)` will be used as the input to the first\n# `ReLU`; the output of the first `ReLU` will become the input\n# for `Conv2d(20,64,5)`. Finally, the output of\n# `Conv2d(20,64,5)` will be used as input to the second `ReLU`\nmodel = nn.Sequential(\n          nn.Conv2d(1,20,5),\n          nn.ReLU(),\n          nn.Conv2d(20,64,5),\n          nn.ReLU()\n        )\n\n# Using Sequential with OrderedDict. This is functionally the\n# same as the above code\nmodel = nn.Sequential(OrderedDict([\n          ('conv1', nn.Conv2d(1,20,5)),\n          ('relu1', nn.ReLU()),\n          ('conv2', nn.Conv2d(20,64,5)),\n          ('relu2', nn.ReLU())\n        ]))\n\nfrom tsai.imports import default_device\nfrom fastai.metrics import accuracy\nfrom fastai.callback.tracker import ReduceLROnPlateau\nfrom tsai.data.all import *\nfrom tsai.learner import *\n\n\n# Offline feature calculation\ndsid = 'ECGFiveDays'\nX, y, splits = get_UCR_data(dsid, split_data=False)\nmrf = MiniRocketFeaturesPlus(c_in=X.shape[1], seq_len=X.shape[2]).to(default_device())\nX_train = X[splits[0]]  # X_train may either be a np.ndarray or a torch.Tensor\nmrf.fit(X_train)\nX_tfm = get_minirocket_features(X, mrf).cpu().numpy()\ntfms = [None, TSClassification()]\nbatch_tfms = TSStandardize(by_var=True)\ndls = get_ts_dls(X_tfm, y, splits=splits, tfms=tfms, batch_tfms=batch_tfms, bs=256)\nlearn = ts_learner(dls, MiniRocketHead, metrics=accuracy)\nlearn.fit(1, 1e-4, cbs=ReduceLROnPlateau(factor=0.5, min_lr=1e-8, patience=10))\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      accuracy\n      time\n    \n  \n  \n    \n      0\n      0.693147\n      0.529940\n      0.758420\n      00:00\n    \n  \n\n\n\n\n# Online feature calculation\ndsid = 'ECGFiveDays'\nX, y, splits = get_UCR_data(dsid, split_data=False)\ntfms = [None, TSClassification()]\nbatch_tfms = TSStandardize()\ndls = get_ts_dls(X, y, splits=splits, tfms=tfms, batch_tfms=batch_tfms, bs=256)\nlearn = ts_learner(dls, MiniRocketPlus, kernel_size=7, metrics=accuracy)\nlearn.fit_one_cycle(1, 1e-2)\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      accuracy\n      time\n    \n  \n  \n    \n      0\n      0.693147\n      0.667325\n      0.502904\n      00:11\n    \n  \n\n\n\n\nfrom fastcore.test import *\nfrom tsai.models.utils import build_ts_model\nfrom tsai.models.layers import mlp_head\n\n\nbs, c_in, seq_len = 8, 3, 50\nc_out = 2\nxb = torch.randn(bs, c_in, seq_len)\nmodel = build_ts_model(MiniRocketPlus, c_in=c_in, c_out=c_out, seq_len=seq_len)\ntest_eq(model.to(xb.device)(xb).shape, (bs, c_out))\nmodel = build_ts_model(MiniRocketPlus, c_in=c_in, c_out=c_out, seq_len=seq_len, add_lsaz=True)\ntest_eq(model.to(xb.device)(xb).shape, (bs, c_out))\nmodel = build_ts_model(MiniRocketPlus, c_in=c_in, c_out=c_out, seq_len=seq_len, custom_head=mlp_head)\ntest_eq(model.to(xb.device)(xb).shape, (bs, c_out))\n\n\nsource\n\n\nInceptionRocketFeaturesPlus\n\n InceptionRocketFeaturesPlus (c_in, seq_len, num_features=10000,\n                              max_dilations_per_kernel=32,\n                              kernel_sizes=array([3, 5, 7, 9]),\n                              max_num_channels=None, max_num_kernels=84,\n                              add_lsaz=True, same_n_feats_per_ks=False)\n\nBase class for all neural network modules.\nYour models should also subclass this class.\nModules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes::\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 20, 5)\n        self.conv2 = nn.Conv2d(20, 20, 5)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        return F.relu(self.conv2(x))\nSubmodules assigned in this way will be registered, and will have their parameters converted too when you call :meth:to, etc.\n.. note:: As per the example above, an __init__() call to the parent class must be made before assignment on the child.\n:ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool\n\nsource\n\n\nInceptionRocketPlus\n\n InceptionRocketPlus (c_in, c_out, seq_len, num_features=10000,\n                      max_dilations_per_kernel=32, kernel_sizes=[3, 5, 7,\n                      9], max_num_channels=None, max_num_kernels=84,\n                      same_n_feats_per_ks=False, add_lsaz=False, bn=True,\n                      fc_dropout=0)\n\nA sequential container. Modules will be added to it in the order they are passed in the constructor. Alternatively, an OrderedDict of modules can be passed in. The forward() method of [Sequential](https://timeseriesAI.github.io/models.layers.html#sequential) accepts any input and forwards it to the first module it contains. It then “chains” outputs to inputs sequentially for each subsequent module, finally returning the output of the last module.\nThe value a [Sequential](https://timeseriesAI.github.io/models.layers.html#sequential) provides over manually calling a sequence of modules is that it allows treating the whole container as a single module, such that performing a transformation on the [Sequential](https://timeseriesAI.github.io/models.layers.html#sequential) applies to each of the modules it stores (which are each a registered submodule of the [Sequential](https://timeseriesAI.github.io/models.layers.html#sequential)).\nWhat’s the difference between a [Sequential](https://timeseriesAI.github.io/models.layers.html#sequential) and a :class:torch.nn.ModuleList? A ModuleList is exactly what it sounds like–a list for storing Module s! On the other hand, the layers in a [Sequential](https://timeseriesAI.github.io/models.layers.html#sequential) are connected in a cascading way.\nExample::\n# Using Sequential to create a small model. When `model` is run,\n# input will first be passed to `Conv2d(1,20,5)`. The output of\n# `Conv2d(1,20,5)` will be used as the input to the first\n# `ReLU`; the output of the first `ReLU` will become the input\n# for `Conv2d(20,64,5)`. Finally, the output of\n# `Conv2d(20,64,5)` will be used as input to the second `ReLU`\nmodel = nn.Sequential(\n          nn.Conv2d(1,20,5),\n          nn.ReLU(),\n          nn.Conv2d(20,64,5),\n          nn.ReLU()\n        )\n\n# Using Sequential with OrderedDict. This is functionally the\n# same as the above code\nmodel = nn.Sequential(OrderedDict([\n          ('conv1', nn.Conv2d(1,20,5)),\n          ('relu1', nn.ReLU()),\n          ('conv2', nn.Conv2d(20,64,5)),\n          ('relu2', nn.ReLU())\n        ]))\n\nfrom fastcore.test import *\nfrom tsai.models.utils import build_ts_model\n\n\nbs, c_in, seq_len = 8, 3, 50\nc_out = 2\nxb = torch.randn(bs, c_in, seq_len)\nmodel = build_ts_model(InceptionRocketPlus, c_in=c_in, c_out=c_out, seq_len=seq_len)\ntest_eq(model.to(xb.device)(xb).shape, (bs, c_out))\nmodel = build_ts_model(InceptionRocketPlus, c_in=c_in, c_out=c_out, seq_len=seq_len, add_lsaz=True)\ntest_eq(model.to(xb.device)(xb).shape, (bs, c_out))"
  },
  {
    "objectID": "models.resnetplus.html",
    "href": "models.resnetplus.html",
    "title": "ResNetPlus",
    "section": "",
    "text": "This is an unofficial PyTorch implementation created by Ignacio Oguiza - oguiza@timeseriesAI.co\n\n\nsource\n\nResNetPlus\n\n ResNetPlus (c_in, c_out, seq_len=None, nf=64, sa=False, se=None,\n             fc_dropout=0.0, concat_pool=False, flatten=False,\n             custom_head=None, y_range=None, ks=[7, 5, 3], coord=False,\n             separable=False, bn_1st=True, zero_norm=False, act=<class\n             'torch.nn.modules.activation.ReLU'>, act_kwargs={})\n\nA sequential container. Modules will be added to it in the order they are passed in the constructor. Alternatively, an OrderedDict of modules can be passed in. The forward() method of [Sequential](https://timeseriesAI.github.io/models.layers.html#sequential) accepts any input and forwards it to the first module it contains. It then “chains” outputs to inputs sequentially for each subsequent module, finally returning the output of the last module.\nThe value a [Sequential](https://timeseriesAI.github.io/models.layers.html#sequential) provides over manually calling a sequence of modules is that it allows treating the whole container as a single module, such that performing a transformation on the [Sequential](https://timeseriesAI.github.io/models.layers.html#sequential) applies to each of the modules it stores (which are each a registered submodule of the [Sequential](https://timeseriesAI.github.io/models.layers.html#sequential)).\nWhat’s the difference between a [Sequential](https://timeseriesAI.github.io/models.layers.html#sequential) and a :class:torch.nn.ModuleList? A ModuleList is exactly what it sounds like–a list for storing Module s! On the other hand, the layers in a [Sequential](https://timeseriesAI.github.io/models.layers.html#sequential) are connected in a cascading way.\nExample::\n# Using Sequential to create a small model. When `model` is run,\n# input will first be passed to `Conv2d(1,20,5)`. The output of\n# `Conv2d(1,20,5)` will be used as the input to the first\n# `ReLU`; the output of the first `ReLU` will become the input\n# for `Conv2d(20,64,5)`. Finally, the output of\n# `Conv2d(20,64,5)` will be used as input to the second `ReLU`\nmodel = nn.Sequential(\n          nn.Conv2d(1,20,5),\n          nn.ReLU(),\n          nn.Conv2d(20,64,5),\n          nn.ReLU()\n        )\n\n# Using Sequential with OrderedDict. This is functionally the\n# same as the above code\nmodel = nn.Sequential(OrderedDict([\n          ('conv1', nn.Conv2d(1,20,5)),\n          ('relu1', nn.ReLU()),\n          ('conv2', nn.Conv2d(20,64,5)),\n          ('relu2', nn.ReLU())\n        ]))\n\nsource\n\n\nResBlockPlus\n\n ResBlockPlus (ni, nf, ks=[7, 5, 3], coord=False, separable=False,\n               bn_1st=True, zero_norm=False, sa=False, se=None, act=<class\n               'torch.nn.modules.activation.ReLU'>, act_kwargs={})\n\nSame as nn.Module, but no need for subclasses to call super().__init__\n\nfrom tsai.models.layers import Swish\n\n\nxb = torch.rand(2, 3, 4)\ntest_eq(ResNetPlus(3,2)(xb).shape, [xb.shape[0], 2])\ntest_eq(ResNetPlus(3,2,coord=True, separable=True, bn_1st=False, zero_norm=True, act=Swish, act_kwargs={}, fc_dropout=0.5)(xb).shape, [xb.shape[0], 2])\ntest_eq(count_parameters(ResNetPlus(3, 2)), 479490) # for (3,2)\n\n\nfrom tsai.models.ResNet import *\n\n\ntest_eq(count_parameters(ResNet(3, 2)), count_parameters(ResNetPlus(3, 2))) # for (3,2)\n\n\nm = ResNetPlus(3, 2, zero_norm=True, coord=True, separable=True)\nprint('n_params:', count_parameters(m))\nprint(m)\nprint(check_weight(m, is_bn)[0])\n\nn_params: 114820\nResNetPlus(\n  (backbone): Sequential(\n    (0): ResBlockPlus(\n      (convblock1): ConvBlock(\n        (0): AddCoords1d()\n        (1): SeparableConv1d(\n          (depthwise_conv): Conv1d(4, 4, kernel_size=(7,), stride=(1,), padding=(3,), groups=4, bias=False)\n          (pointwise_conv): Conv1d(4, 64, kernel_size=(1,), stride=(1,), bias=False)\n        )\n        (2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (3): ReLU()\n      )\n      (convblock2): ConvBlock(\n        (0): AddCoords1d()\n        (1): SeparableConv1d(\n          (depthwise_conv): Conv1d(65, 65, kernel_size=(5,), stride=(1,), padding=(2,), groups=65, bias=False)\n          (pointwise_conv): Conv1d(65, 64, kernel_size=(1,), stride=(1,), bias=False)\n        )\n        (2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (3): ReLU()\n      )\n      (convblock3): ConvBlock(\n        (0): AddCoords1d()\n        (1): SeparableConv1d(\n          (depthwise_conv): Conv1d(65, 65, kernel_size=(3,), stride=(1,), padding=(1,), groups=65, bias=False)\n          (pointwise_conv): Conv1d(65, 64, kernel_size=(1,), stride=(1,), bias=False)\n        )\n        (2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n      (shortcut): ConvBlock(\n        (0): AddCoords1d()\n        (1): Conv1d(4, 64, kernel_size=(1,), stride=(1,), bias=False)\n        (2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n      (add): Add\n      (act): ReLU()\n    )\n    (1): ResBlockPlus(\n      (convblock1): ConvBlock(\n        (0): AddCoords1d()\n        (1): SeparableConv1d(\n          (depthwise_conv): Conv1d(65, 65, kernel_size=(7,), stride=(1,), padding=(3,), groups=65, bias=False)\n          (pointwise_conv): Conv1d(65, 128, kernel_size=(1,), stride=(1,), bias=False)\n        )\n        (2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (3): ReLU()\n      )\n      (convblock2): ConvBlock(\n        (0): AddCoords1d()\n        (1): SeparableConv1d(\n          (depthwise_conv): Conv1d(129, 129, kernel_size=(5,), stride=(1,), padding=(2,), groups=129, bias=False)\n          (pointwise_conv): Conv1d(129, 128, kernel_size=(1,), stride=(1,), bias=False)\n        )\n        (2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (3): ReLU()\n      )\n      (convblock3): ConvBlock(\n        (0): AddCoords1d()\n        (1): SeparableConv1d(\n          (depthwise_conv): Conv1d(129, 129, kernel_size=(3,), stride=(1,), padding=(1,), groups=129, bias=False)\n          (pointwise_conv): Conv1d(129, 128, kernel_size=(1,), stride=(1,), bias=False)\n        )\n        (2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n      (shortcut): ConvBlock(\n        (0): AddCoords1d()\n        (1): Conv1d(65, 128, kernel_size=(1,), stride=(1,), bias=False)\n        (2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n      (add): Add\n      (act): ReLU()\n    )\n    (2): ResBlockPlus(\n      (convblock1): ConvBlock(\n        (0): AddCoords1d()\n        (1): SeparableConv1d(\n          (depthwise_conv): Conv1d(129, 129, kernel_size=(7,), stride=(1,), padding=(3,), groups=129, bias=False)\n          (pointwise_conv): Conv1d(129, 128, kernel_size=(1,), stride=(1,), bias=False)\n        )\n        (2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (3): ReLU()\n      )\n      (convblock2): ConvBlock(\n        (0): AddCoords1d()\n        (1): SeparableConv1d(\n          (depthwise_conv): Conv1d(129, 129, kernel_size=(5,), stride=(1,), padding=(2,), groups=129, bias=False)\n          (pointwise_conv): Conv1d(129, 128, kernel_size=(1,), stride=(1,), bias=False)\n        )\n        (2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (3): ReLU()\n      )\n      (convblock3): ConvBlock(\n        (0): AddCoords1d()\n        (1): SeparableConv1d(\n          (depthwise_conv): Conv1d(129, 129, kernel_size=(3,), stride=(1,), padding=(1,), groups=129, bias=False)\n          (pointwise_conv): Conv1d(129, 128, kernel_size=(1,), stride=(1,), bias=False)\n        )\n        (2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n      (shortcut): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (add): Add\n      (act): ReLU()\n    )\n  )\n  (head): Sequential(\n    (0): GAP1d(\n      (gap): AdaptiveAvgPool1d(output_size=1)\n      (flatten): Reshape(bs)\n    )\n    (1): Linear(in_features=128, out_features=2, bias=True)\n  )\n)\n[1. 1. 0. 1. 1. 1. 0. 1. 1. 1. 0. 1.]"
  },
  {
    "objectID": "models.xresnet1dplus.html",
    "href": "models.xresnet1dplus.html",
    "title": "XResNet1dPlus",
    "section": "",
    "text": "This is a modified version of fastai’s XResNet model in github\n\n\nsource\n\nXResNet1dPlus\n\n XResNet1dPlus (block, expansion, layers, fc_dropout=0.0, c_in=3,\n                n_out=1000, stem_szs=(32, 32, 64), widen=1.0, sa=False,\n                act_cls=<class 'torch.nn.modules.activation.ReLU'>, ks=3,\n                stride=2, coord=False, groups=1, reduction=None, nh1=None,\n                nh2=None, dw=False, g2=1, sym=False, norm='Batch',\n                zero_norm=True, pool=<function AvgPool>, pool_first=True)\n\nA sequential container. Modules will be added to it in the order they are passed in the constructor. Alternatively, an OrderedDict of modules can be passed in. The forward() method of [Sequential](https://timeseriesAI.github.io/models.layers.html#sequential) accepts any input and forwards it to the first module it contains. It then “chains” outputs to inputs sequentially for each subsequent module, finally returning the output of the last module.\nThe value a [Sequential](https://timeseriesAI.github.io/models.layers.html#sequential) provides over manually calling a sequence of modules is that it allows treating the whole container as a single module, such that performing a transformation on the [Sequential](https://timeseriesAI.github.io/models.layers.html#sequential) applies to each of the modules it stores (which are each a registered submodule of the [Sequential](https://timeseriesAI.github.io/models.layers.html#sequential)).\nWhat’s the difference between a [Sequential](https://timeseriesAI.github.io/models.layers.html#sequential) and a :class:torch.nn.ModuleList? A ModuleList is exactly what it sounds like–a list for storing Module s! On the other hand, the layers in a [Sequential](https://timeseriesAI.github.io/models.layers.html#sequential) are connected in a cascading way.\nExample::\n# Using Sequential to create a small model. When `model` is run,\n# input will first be passed to `Conv2d(1,20,5)`. The output of\n# `Conv2d(1,20,5)` will be used as the input to the first\n# `ReLU`; the output of the first `ReLU` will become the input\n# for `Conv2d(20,64,5)`. Finally, the output of\n# `Conv2d(20,64,5)` will be used as input to the second `ReLU`\nmodel = nn.Sequential(\n          nn.Conv2d(1,20,5),\n          nn.ReLU(),\n          nn.Conv2d(20,64,5),\n          nn.ReLU()\n        )\n\n# Using Sequential with OrderedDict. This is functionally the\n# same as the above code\nmodel = nn.Sequential(OrderedDict([\n          ('conv1', nn.Conv2d(1,20,5)),\n          ('relu1', nn.ReLU()),\n          ('conv2', nn.Conv2d(20,64,5)),\n          ('relu2', nn.ReLU())\n        ]))\n\nsource\n\n\nxresnet1d50_deeperplus\n\n xresnet1d50_deeperplus (c_in, c_out, act=<class\n                         'torch.nn.modules.activation.ReLU'>, stride=1,\n                         groups=1, reduction=None, nh1=None, nh2=None,\n                         dw=False, g2=1, sa=False, sym=False,\n                         norm_type=<NormType.Batch: 1>, act_cls=<class\n                         'torch.nn.modules.activation.ReLU'>, ndim=2,\n                         ks=3, pool=<function AvgPool>, pool_first=True,\n                         padding=None, bias=None, bn_1st=True,\n                         transpose=False, init='auto', xtra=None,\n                         bias_std=0.01,\n                         dilation:Union[int,Tuple[int,int]]=1,\n                         padding_mode:str='zeros', device=None,\n                         dtype=None)\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nc_in\n\n\n\n\n\nc_out\n\n\n\n\n\nact\ntype\nReLU\n\n\n\nstride\nint\n1\n\n\n\ngroups\nint\n1\n\n\n\nreduction\nNoneType\nNone\n\n\n\nnh1\nNoneType\nNone\n\n\n\nnh2\nNoneType\nNone\n\n\n\ndw\nbool\nFalse\n\n\n\ng2\nint\n1\n\n\n\nsa\nbool\nFalse\n\n\n\nsym\nbool\nFalse\n\n\n\nnorm_type\nNormType\nNormType.Batch\n\n\n\nact_cls\ntype\nReLU\n\n\n\nndim\nint\n2\n\n\n\nks\nint\n3\n\n\n\npool\nfunction\nAvgPool\n\n\n\npool_first\nbool\nTrue\n\n\n\npadding\nNoneType\nNone\n\n\n\nbias\nNoneType\nNone\n\n\n\nbn_1st\nbool\nTrue\n\n\n\ntranspose\nbool\nFalse\n\n\n\ninit\nstr\nauto\n\n\n\nxtra\nNoneType\nNone\n\n\n\nbias_std\nfloat\n0.01\n\n\n\ndilation\ntyping.Union[int, typing.Tuple[int, int]]\n1\n\n\n\npadding_mode\nstr\nzeros\nTODO: refine this type\n\n\ndevice\nNoneType\nNone\n\n\n\ndtype\nNoneType\nNone\n\n\n\n\n\nsource\n\n\nxresnet1d34_deeperplus\n\n xresnet1d34_deeperplus (c_in, c_out, act=<class\n                         'torch.nn.modules.activation.ReLU'>, stride=1,\n                         groups=1, reduction=None, nh1=None, nh2=None,\n                         dw=False, g2=1, sa=False, sym=False,\n                         norm_type=<NormType.Batch: 1>, act_cls=<class\n                         'torch.nn.modules.activation.ReLU'>, ndim=2,\n                         ks=3, pool=<function AvgPool>, pool_first=True,\n                         padding=None, bias=None, bn_1st=True,\n                         transpose=False, init='auto', xtra=None,\n                         bias_std=0.01,\n                         dilation:Union[int,Tuple[int,int]]=1,\n                         padding_mode:str='zeros', device=None,\n                         dtype=None)\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nc_in\n\n\n\n\n\nc_out\n\n\n\n\n\nact\ntype\nReLU\n\n\n\nstride\nint\n1\n\n\n\ngroups\nint\n1\n\n\n\nreduction\nNoneType\nNone\n\n\n\nnh1\nNoneType\nNone\n\n\n\nnh2\nNoneType\nNone\n\n\n\ndw\nbool\nFalse\n\n\n\ng2\nint\n1\n\n\n\nsa\nbool\nFalse\n\n\n\nsym\nbool\nFalse\n\n\n\nnorm_type\nNormType\nNormType.Batch\n\n\n\nact_cls\ntype\nReLU\n\n\n\nndim\nint\n2\n\n\n\nks\nint\n3\n\n\n\npool\nfunction\nAvgPool\n\n\n\npool_first\nbool\nTrue\n\n\n\npadding\nNoneType\nNone\n\n\n\nbias\nNoneType\nNone\n\n\n\nbn_1st\nbool\nTrue\n\n\n\ntranspose\nbool\nFalse\n\n\n\ninit\nstr\nauto\n\n\n\nxtra\nNoneType\nNone\n\n\n\nbias_std\nfloat\n0.01\n\n\n\ndilation\ntyping.Union[int, typing.Tuple[int, int]]\n1\n\n\n\npadding_mode\nstr\nzeros\nTODO: refine this type\n\n\ndevice\nNoneType\nNone\n\n\n\ndtype\nNoneType\nNone\n\n\n\n\n\nsource\n\n\nxresnet1d18_deeperplus\n\n xresnet1d18_deeperplus (c_in, c_out, act=<class\n                         'torch.nn.modules.activation.ReLU'>, stride=1,\n                         groups=1, reduction=None, nh1=None, nh2=None,\n                         dw=False, g2=1, sa=False, sym=False,\n                         norm_type=<NormType.Batch: 1>, act_cls=<class\n                         'torch.nn.modules.activation.ReLU'>, ndim=2,\n                         ks=3, pool=<function AvgPool>, pool_first=True,\n                         padding=None, bias=None, bn_1st=True,\n                         transpose=False, init='auto', xtra=None,\n                         bias_std=0.01,\n                         dilation:Union[int,Tuple[int,int]]=1,\n                         padding_mode:str='zeros', device=None,\n                         dtype=None)\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nc_in\n\n\n\n\n\nc_out\n\n\n\n\n\nact\ntype\nReLU\n\n\n\nstride\nint\n1\n\n\n\ngroups\nint\n1\n\n\n\nreduction\nNoneType\nNone\n\n\n\nnh1\nNoneType\nNone\n\n\n\nnh2\nNoneType\nNone\n\n\n\ndw\nbool\nFalse\n\n\n\ng2\nint\n1\n\n\n\nsa\nbool\nFalse\n\n\n\nsym\nbool\nFalse\n\n\n\nnorm_type\nNormType\nNormType.Batch\n\n\n\nact_cls\ntype\nReLU\n\n\n\nndim\nint\n2\n\n\n\nks\nint\n3\n\n\n\npool\nfunction\nAvgPool\n\n\n\npool_first\nbool\nTrue\n\n\n\npadding\nNoneType\nNone\n\n\n\nbias\nNoneType\nNone\n\n\n\nbn_1st\nbool\nTrue\n\n\n\ntranspose\nbool\nFalse\n\n\n\ninit\nstr\nauto\n\n\n\nxtra\nNoneType\nNone\n\n\n\nbias_std\nfloat\n0.01\n\n\n\ndilation\ntyping.Union[int, typing.Tuple[int, int]]\n1\n\n\n\npadding_mode\nstr\nzeros\nTODO: refine this type\n\n\ndevice\nNoneType\nNone\n\n\n\ndtype\nNoneType\nNone\n\n\n\n\n\nsource\n\n\nxresnet1d50_deepplus\n\n xresnet1d50_deepplus (c_in, c_out, act=<class\n                       'torch.nn.modules.activation.ReLU'>, stride=1,\n                       groups=1, reduction=None, nh1=None, nh2=None,\n                       dw=False, g2=1, sa=False, sym=False,\n                       norm_type=<NormType.Batch: 1>, act_cls=<class\n                       'torch.nn.modules.activation.ReLU'>, ndim=2, ks=3,\n                       pool=<function AvgPool>, pool_first=True,\n                       padding=None, bias=None, bn_1st=True,\n                       transpose=False, init='auto', xtra=None,\n                       bias_std=0.01,\n                       dilation:Union[int,Tuple[int,int]]=1,\n                       padding_mode:str='zeros', device=None, dtype=None)\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nc_in\n\n\n\n\n\nc_out\n\n\n\n\n\nact\ntype\nReLU\n\n\n\nstride\nint\n1\n\n\n\ngroups\nint\n1\n\n\n\nreduction\nNoneType\nNone\n\n\n\nnh1\nNoneType\nNone\n\n\n\nnh2\nNoneType\nNone\n\n\n\ndw\nbool\nFalse\n\n\n\ng2\nint\n1\n\n\n\nsa\nbool\nFalse\n\n\n\nsym\nbool\nFalse\n\n\n\nnorm_type\nNormType\nNormType.Batch\n\n\n\nact_cls\ntype\nReLU\n\n\n\nndim\nint\n2\n\n\n\nks\nint\n3\n\n\n\npool\nfunction\nAvgPool\n\n\n\npool_first\nbool\nTrue\n\n\n\npadding\nNoneType\nNone\n\n\n\nbias\nNoneType\nNone\n\n\n\nbn_1st\nbool\nTrue\n\n\n\ntranspose\nbool\nFalse\n\n\n\ninit\nstr\nauto\n\n\n\nxtra\nNoneType\nNone\n\n\n\nbias_std\nfloat\n0.01\n\n\n\ndilation\ntyping.Union[int, typing.Tuple[int, int]]\n1\n\n\n\npadding_mode\nstr\nzeros\nTODO: refine this type\n\n\ndevice\nNoneType\nNone\n\n\n\ndtype\nNoneType\nNone\n\n\n\n\n\nsource\n\n\nxresnet1d34_deepplus\n\n xresnet1d34_deepplus (c_in, c_out, act=<class\n                       'torch.nn.modules.activation.ReLU'>, stride=1,\n                       groups=1, reduction=None, nh1=None, nh2=None,\n                       dw=False, g2=1, sa=False, sym=False,\n                       norm_type=<NormType.Batch: 1>, act_cls=<class\n                       'torch.nn.modules.activation.ReLU'>, ndim=2, ks=3,\n                       pool=<function AvgPool>, pool_first=True,\n                       padding=None, bias=None, bn_1st=True,\n                       transpose=False, init='auto', xtra=None,\n                       bias_std=0.01,\n                       dilation:Union[int,Tuple[int,int]]=1,\n                       padding_mode:str='zeros', device=None, dtype=None)\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nc_in\n\n\n\n\n\nc_out\n\n\n\n\n\nact\ntype\nReLU\n\n\n\nstride\nint\n1\n\n\n\ngroups\nint\n1\n\n\n\nreduction\nNoneType\nNone\n\n\n\nnh1\nNoneType\nNone\n\n\n\nnh2\nNoneType\nNone\n\n\n\ndw\nbool\nFalse\n\n\n\ng2\nint\n1\n\n\n\nsa\nbool\nFalse\n\n\n\nsym\nbool\nFalse\n\n\n\nnorm_type\nNormType\nNormType.Batch\n\n\n\nact_cls\ntype\nReLU\n\n\n\nndim\nint\n2\n\n\n\nks\nint\n3\n\n\n\npool\nfunction\nAvgPool\n\n\n\npool_first\nbool\nTrue\n\n\n\npadding\nNoneType\nNone\n\n\n\nbias\nNoneType\nNone\n\n\n\nbn_1st\nbool\nTrue\n\n\n\ntranspose\nbool\nFalse\n\n\n\ninit\nstr\nauto\n\n\n\nxtra\nNoneType\nNone\n\n\n\nbias_std\nfloat\n0.01\n\n\n\ndilation\ntyping.Union[int, typing.Tuple[int, int]]\n1\n\n\n\npadding_mode\nstr\nzeros\nTODO: refine this type\n\n\ndevice\nNoneType\nNone\n\n\n\ndtype\nNoneType\nNone\n\n\n\n\n\nsource\n\n\nxresnet1d18_deepplus\n\n xresnet1d18_deepplus (c_in, c_out, act=<class\n                       'torch.nn.modules.activation.ReLU'>, stride=1,\n                       groups=1, reduction=None, nh1=None, nh2=None,\n                       dw=False, g2=1, sa=False, sym=False,\n                       norm_type=<NormType.Batch: 1>, act_cls=<class\n                       'torch.nn.modules.activation.ReLU'>, ndim=2, ks=3,\n                       pool=<function AvgPool>, pool_first=True,\n                       padding=None, bias=None, bn_1st=True,\n                       transpose=False, init='auto', xtra=None,\n                       bias_std=0.01,\n                       dilation:Union[int,Tuple[int,int]]=1,\n                       padding_mode:str='zeros', device=None, dtype=None)\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nc_in\n\n\n\n\n\nc_out\n\n\n\n\n\nact\ntype\nReLU\n\n\n\nstride\nint\n1\n\n\n\ngroups\nint\n1\n\n\n\nreduction\nNoneType\nNone\n\n\n\nnh1\nNoneType\nNone\n\n\n\nnh2\nNoneType\nNone\n\n\n\ndw\nbool\nFalse\n\n\n\ng2\nint\n1\n\n\n\nsa\nbool\nFalse\n\n\n\nsym\nbool\nFalse\n\n\n\nnorm_type\nNormType\nNormType.Batch\n\n\n\nact_cls\ntype\nReLU\n\n\n\nndim\nint\n2\n\n\n\nks\nint\n3\n\n\n\npool\nfunction\nAvgPool\n\n\n\npool_first\nbool\nTrue\n\n\n\npadding\nNoneType\nNone\n\n\n\nbias\nNoneType\nNone\n\n\n\nbn_1st\nbool\nTrue\n\n\n\ntranspose\nbool\nFalse\n\n\n\ninit\nstr\nauto\n\n\n\nxtra\nNoneType\nNone\n\n\n\nbias_std\nfloat\n0.01\n\n\n\ndilation\ntyping.Union[int, typing.Tuple[int, int]]\n1\n\n\n\npadding_mode\nstr\nzeros\nTODO: refine this type\n\n\ndevice\nNoneType\nNone\n\n\n\ndtype\nNoneType\nNone\n\n\n\n\n\nsource\n\n\nxresnet1d152plus\n\n xresnet1d152plus (c_in, c_out, act=<class\n                   'torch.nn.modules.activation.ReLU'>, stride=1,\n                   groups=1, reduction=None, nh1=None, nh2=None, dw=False,\n                   g2=1, sa=False, sym=False, norm_type=<NormType.Batch:\n                   1>, act_cls=<class 'torch.nn.modules.activation.ReLU'>,\n                   ndim=2, ks=3, pool=<function AvgPool>, pool_first=True,\n                   padding=None, bias=None, bn_1st=True, transpose=False,\n                   init='auto', xtra=None, bias_std=0.01,\n                   dilation:Union[int,Tuple[int,int]]=1,\n                   padding_mode:str='zeros', device=None, dtype=None)\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nc_in\n\n\n\n\n\nc_out\n\n\n\n\n\nact\ntype\nReLU\n\n\n\nstride\nint\n1\n\n\n\ngroups\nint\n1\n\n\n\nreduction\nNoneType\nNone\n\n\n\nnh1\nNoneType\nNone\n\n\n\nnh2\nNoneType\nNone\n\n\n\ndw\nbool\nFalse\n\n\n\ng2\nint\n1\n\n\n\nsa\nbool\nFalse\n\n\n\nsym\nbool\nFalse\n\n\n\nnorm_type\nNormType\nNormType.Batch\n\n\n\nact_cls\ntype\nReLU\n\n\n\nndim\nint\n2\n\n\n\nks\nint\n3\n\n\n\npool\nfunction\nAvgPool\n\n\n\npool_first\nbool\nTrue\n\n\n\npadding\nNoneType\nNone\n\n\n\nbias\nNoneType\nNone\n\n\n\nbn_1st\nbool\nTrue\n\n\n\ntranspose\nbool\nFalse\n\n\n\ninit\nstr\nauto\n\n\n\nxtra\nNoneType\nNone\n\n\n\nbias_std\nfloat\n0.01\n\n\n\ndilation\ntyping.Union[int, typing.Tuple[int, int]]\n1\n\n\n\npadding_mode\nstr\nzeros\nTODO: refine this type\n\n\ndevice\nNoneType\nNone\n\n\n\ndtype\nNoneType\nNone\n\n\n\n\n\nsource\n\n\nxresnet1d101plus\n\n xresnet1d101plus (c_in, c_out, act=<class\n                   'torch.nn.modules.activation.ReLU'>, stride=1,\n                   groups=1, reduction=None, nh1=None, nh2=None, dw=False,\n                   g2=1, sa=False, sym=False, norm_type=<NormType.Batch:\n                   1>, act_cls=<class 'torch.nn.modules.activation.ReLU'>,\n                   ndim=2, ks=3, pool=<function AvgPool>, pool_first=True,\n                   padding=None, bias=None, bn_1st=True, transpose=False,\n                   init='auto', xtra=None, bias_std=0.01,\n                   dilation:Union[int,Tuple[int,int]]=1,\n                   padding_mode:str='zeros', device=None, dtype=None)\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nc_in\n\n\n\n\n\nc_out\n\n\n\n\n\nact\ntype\nReLU\n\n\n\nstride\nint\n1\n\n\n\ngroups\nint\n1\n\n\n\nreduction\nNoneType\nNone\n\n\n\nnh1\nNoneType\nNone\n\n\n\nnh2\nNoneType\nNone\n\n\n\ndw\nbool\nFalse\n\n\n\ng2\nint\n1\n\n\n\nsa\nbool\nFalse\n\n\n\nsym\nbool\nFalse\n\n\n\nnorm_type\nNormType\nNormType.Batch\n\n\n\nact_cls\ntype\nReLU\n\n\n\nndim\nint\n2\n\n\n\nks\nint\n3\n\n\n\npool\nfunction\nAvgPool\n\n\n\npool_first\nbool\nTrue\n\n\n\npadding\nNoneType\nNone\n\n\n\nbias\nNoneType\nNone\n\n\n\nbn_1st\nbool\nTrue\n\n\n\ntranspose\nbool\nFalse\n\n\n\ninit\nstr\nauto\n\n\n\nxtra\nNoneType\nNone\n\n\n\nbias_std\nfloat\n0.01\n\n\n\ndilation\ntyping.Union[int, typing.Tuple[int, int]]\n1\n\n\n\npadding_mode\nstr\nzeros\nTODO: refine this type\n\n\ndevice\nNoneType\nNone\n\n\n\ndtype\nNoneType\nNone\n\n\n\n\n\nsource\n\n\nxresnet1d50plus\n\n xresnet1d50plus (c_in, c_out, act=<class\n                  'torch.nn.modules.activation.ReLU'>, stride=1, groups=1,\n                  reduction=None, nh1=None, nh2=None, dw=False, g2=1,\n                  sa=False, sym=False, norm_type=<NormType.Batch: 1>,\n                  act_cls=<class 'torch.nn.modules.activation.ReLU'>,\n                  ndim=2, ks=3, pool=<function AvgPool>, pool_first=True,\n                  padding=None, bias=None, bn_1st=True, transpose=False,\n                  init='auto', xtra=None, bias_std=0.01,\n                  dilation:Union[int,Tuple[int,int]]=1,\n                  padding_mode:str='zeros', device=None, dtype=None)\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nc_in\n\n\n\n\n\nc_out\n\n\n\n\n\nact\ntype\nReLU\n\n\n\nstride\nint\n1\n\n\n\ngroups\nint\n1\n\n\n\nreduction\nNoneType\nNone\n\n\n\nnh1\nNoneType\nNone\n\n\n\nnh2\nNoneType\nNone\n\n\n\ndw\nbool\nFalse\n\n\n\ng2\nint\n1\n\n\n\nsa\nbool\nFalse\n\n\n\nsym\nbool\nFalse\n\n\n\nnorm_type\nNormType\nNormType.Batch\n\n\n\nact_cls\ntype\nReLU\n\n\n\nndim\nint\n2\n\n\n\nks\nint\n3\n\n\n\npool\nfunction\nAvgPool\n\n\n\npool_first\nbool\nTrue\n\n\n\npadding\nNoneType\nNone\n\n\n\nbias\nNoneType\nNone\n\n\n\nbn_1st\nbool\nTrue\n\n\n\ntranspose\nbool\nFalse\n\n\n\ninit\nstr\nauto\n\n\n\nxtra\nNoneType\nNone\n\n\n\nbias_std\nfloat\n0.01\n\n\n\ndilation\ntyping.Union[int, typing.Tuple[int, int]]\n1\n\n\n\npadding_mode\nstr\nzeros\nTODO: refine this type\n\n\ndevice\nNoneType\nNone\n\n\n\ndtype\nNoneType\nNone\n\n\n\n\n\nsource\n\n\nxresnet1d34plus\n\n xresnet1d34plus (c_in, c_out, act=<class\n                  'torch.nn.modules.activation.ReLU'>, stride=1, groups=1,\n                  reduction=None, nh1=None, nh2=None, dw=False, g2=1,\n                  sa=False, sym=False, norm_type=<NormType.Batch: 1>,\n                  act_cls=<class 'torch.nn.modules.activation.ReLU'>,\n                  ndim=2, ks=3, pool=<function AvgPool>, pool_first=True,\n                  padding=None, bias=None, bn_1st=True, transpose=False,\n                  init='auto', xtra=None, bias_std=0.01,\n                  dilation:Union[int,Tuple[int,int]]=1,\n                  padding_mode:str='zeros', device=None, dtype=None)\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nc_in\n\n\n\n\n\nc_out\n\n\n\n\n\nact\ntype\nReLU\n\n\n\nstride\nint\n1\n\n\n\ngroups\nint\n1\n\n\n\nreduction\nNoneType\nNone\n\n\n\nnh1\nNoneType\nNone\n\n\n\nnh2\nNoneType\nNone\n\n\n\ndw\nbool\nFalse\n\n\n\ng2\nint\n1\n\n\n\nsa\nbool\nFalse\n\n\n\nsym\nbool\nFalse\n\n\n\nnorm_type\nNormType\nNormType.Batch\n\n\n\nact_cls\ntype\nReLU\n\n\n\nndim\nint\n2\n\n\n\nks\nint\n3\n\n\n\npool\nfunction\nAvgPool\n\n\n\npool_first\nbool\nTrue\n\n\n\npadding\nNoneType\nNone\n\n\n\nbias\nNoneType\nNone\n\n\n\nbn_1st\nbool\nTrue\n\n\n\ntranspose\nbool\nFalse\n\n\n\ninit\nstr\nauto\n\n\n\nxtra\nNoneType\nNone\n\n\n\nbias_std\nfloat\n0.01\n\n\n\ndilation\ntyping.Union[int, typing.Tuple[int, int]]\n1\n\n\n\npadding_mode\nstr\nzeros\nTODO: refine this type\n\n\ndevice\nNoneType\nNone\n\n\n\ndtype\nNoneType\nNone\n\n\n\n\n\nsource\n\n\nxresnet1d18plus\n\n xresnet1d18plus (c_in, c_out, act=<class\n                  'torch.nn.modules.activation.ReLU'>, stride=1, groups=1,\n                  reduction=None, nh1=None, nh2=None, dw=False, g2=1,\n                  sa=False, sym=False, norm_type=<NormType.Batch: 1>,\n                  act_cls=<class 'torch.nn.modules.activation.ReLU'>,\n                  ndim=2, ks=3, pool=<function AvgPool>, pool_first=True,\n                  padding=None, bias=None, bn_1st=True, transpose=False,\n                  init='auto', xtra=None, bias_std=0.01,\n                  dilation:Union[int,Tuple[int,int]]=1,\n                  padding_mode:str='zeros', device=None, dtype=None)\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nc_in\n\n\n\n\n\nc_out\n\n\n\n\n\nact\ntype\nReLU\n\n\n\nstride\nint\n1\n\n\n\ngroups\nint\n1\n\n\n\nreduction\nNoneType\nNone\n\n\n\nnh1\nNoneType\nNone\n\n\n\nnh2\nNoneType\nNone\n\n\n\ndw\nbool\nFalse\n\n\n\ng2\nint\n1\n\n\n\nsa\nbool\nFalse\n\n\n\nsym\nbool\nFalse\n\n\n\nnorm_type\nNormType\nNormType.Batch\n\n\n\nact_cls\ntype\nReLU\n\n\n\nndim\nint\n2\n\n\n\nks\nint\n3\n\n\n\npool\nfunction\nAvgPool\n\n\n\npool_first\nbool\nTrue\n\n\n\npadding\nNoneType\nNone\n\n\n\nbias\nNoneType\nNone\n\n\n\nbn_1st\nbool\nTrue\n\n\n\ntranspose\nbool\nFalse\n\n\n\ninit\nstr\nauto\n\n\n\nxtra\nNoneType\nNone\n\n\n\nbias_std\nfloat\n0.01\n\n\n\ndilation\ntyping.Union[int, typing.Tuple[int, int]]\n1\n\n\n\npadding_mode\nstr\nzeros\nTODO: refine this type\n\n\ndevice\nNoneType\nNone\n\n\n\ndtype\nNoneType\nNone\n\n\n\n\n\nnet = xresnet1d18plus(3, 2, coord=True)\nx = torch.rand(32, 3, 50)\nnet(x)\n\ntensor([[ 0.2051, -0.4082],\n        [ 0.1320, -0.2454],\n        [ 0.1718, -0.5900],\n        [ 0.3423, -0.1778],\n        [ 0.2053, -0.5386],\n        [ 0.2830, -0.5609],\n        [ 0.4794, -0.4538],\n        [ 0.1914,  0.1688],\n        [ 0.3116, -0.3382],\n        [ 0.0513, -0.3032],\n        [-0.0072, -0.4724],\n        [ 0.0463, -0.3346],\n        [ 0.4596, -0.7458],\n        [ 0.2832, -0.5932],\n        [ 0.2439, -0.0979],\n        [ 0.2160, -0.4570],\n        [ 0.2408, -0.2508],\n        [ 0.0452, -0.0187],\n        [ 0.4669, -0.1605],\n        [ 0.3136, -0.6333],\n        [-0.0930, -0.0047],\n        [-0.0522, -0.4743],\n        [-0.0063, -0.2872],\n        [ 0.5206, -0.2603],\n        [ 0.0385, -0.8225],\n        [ 0.1682, -0.7900],\n        [ 0.1275, -0.4700],\n        [ 0.0180, -0.4667],\n        [ 0.1500, -0.3306],\n        [ 0.4787, -0.5689],\n        [ 0.3416, -0.7539],\n        [ 0.0963, -0.5110]], grad_fn=<AddmmBackward>)\n\n\n\nbs, c_in, seq_len = 2, 4, 32\nc_out = 2\nx = torch.rand(bs, c_in, seq_len)\narchs = [\n    xresnet1d18plus, xresnet1d34plus, xresnet1d50plus, \n    xresnet1d18_deepplus, xresnet1d34_deepplus, xresnet1d50_deepplus, xresnet1d18_deeperplus,\n    xresnet1d34_deeperplus, xresnet1d50_deeperplus\n#     # Long test\n#     xresnet1d101, xresnet1d152,\n]\nfor i, arch in enumerate(archs):\n    print(i, arch.__name__)\n    test_eq(arch(c_in, c_out, sa=True, act=Mish, coord=True)(x).shape, (bs, c_out))\n\n0 xresnet1d18plus\n1 xresnet1d34plus\n2 xresnet1d50plus\n3 xresnet1d18_deepplus\n4 xresnet1d34_deepplus\n5 xresnet1d50_deepplus\n6 xresnet1d18_deeperplus\n7 xresnet1d34_deeperplus\n8 xresnet1d50_deeperplus\n\n\n\nm = xresnet1d34plus(4, 2, act=Mish)\ntest_eq(len(get_layers(m, is_bn)), 38)\ntest_eq(check_weight(m, is_bn)[0].sum(), 22)"
  },
  {
    "objectID": "learner.html",
    "href": "learner.html",
    "title": "Learner",
    "section": "",
    "text": "fastai Learner extensions.\n\n\nsource\n\nLearner.show_batch\n\n Learner.show_batch (**kwargs)\n\n\nsource\n\n\nLearner.remove_all_cbs\n\n Learner.remove_all_cbs (max_iters=10)\n\n\nsource\n\n\nLearner.one_batch\n\n Learner.one_batch (i, b)\n\n\nsource\n\n\nLearner.inverse_transform\n\n Learner.inverse_transform (df:pandas.core.frame.DataFrame)\n\nApplies sklearn-type pipeline inverse transforms\n\nsource\n\n\nLearner.transform\n\n Learner.transform (df:pandas.core.frame.DataFrame)\n\nApplies sklearn-type pipeline transforms\n⚠️ Important: save_all and load_all methods are designed for small datasets only. If you are using a larger dataset, you should use the standard save and load_learner methods.\n\nsource\n\n\nload_all\n\n load_all (path='export', dls_fname='dls', model_fname='model',\n           learner_fname='learner', device=None, pickle_module=<module\n           'pickle' from '/opt/hostedtoolcache/Python/3.9.16/x64/lib/pytho\n           n3.9/pickle.py'>, verbose=False)\n\n\nsource\n\n\nLearner.save_all\n\n Learner.save_all (path='export', dls_fname='dls', model_fname='model',\n                   learner_fname='learner', verbose=False)\n\n\nsource\n\n\nLearner.plot_metrics\n\n Learner.plot_metrics (nrows:int=1, ncols:int=1, figsize:tuple=None,\n                       imsize:int=3, suptitle:str=None, sharex=False,\n                       sharey=False, squeeze=True, width_ratios=None,\n                       height_ratios=None, subplot_kw=None,\n                       gridspec_kw=None)\n\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nnrows\nint\n1\nNumber of rows in returned axes grid\n\n\nncols\nint\n1\nNumber of columns in returned axes grid\n\n\nfigsize\ntuple\nNone\nWidth, height in inches of the returned figure\n\n\nimsize\nint\n3\nSize (in inches) of images that will be displayed in the returned figure\n\n\nsuptitle\nstr\nNone\nTitle to be set to returned figure\n\n\nsharex\nbool\nFalse\n\n\n\nsharey\nbool\nFalse\n\n\n\nsqueeze\nbool\nTrue\n\n\n\nwidth_ratios\nNoneType\nNone\n\n\n\nheight_ratios\nNoneType\nNone\n\n\n\nsubplot_kw\nNoneType\nNone\n\n\n\ngridspec_kw\nNoneType\nNone\n\n\n\nReturns\n(plt.Figure, plt.Axes)\n\nReturns both fig and ax as a tuple\n\n\n\n\nsource\n\n\nRecorder.plot_metrics\n\n Recorder.plot_metrics (nrows=None, ncols=None, figsize=None,\n                        final_losses=True, perc=0.5, imsize:int=3,\n                        suptitle:str=None, sharex=False, sharey=False,\n                        squeeze=True, width_ratios=None,\n                        height_ratios=None, subplot_kw=None,\n                        gridspec_kw=None)\n\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nnrows\nint\n1\nNumber of rows in returned axes grid\n\n\nncols\nint\n1\nNumber of columns in returned axes grid\n\n\nfigsize\ntuple\nNone\nWidth, height in inches of the returned figure\n\n\nfinal_losses\nbool\nTrue\n\n\n\nperc\nfloat\n0.5\n\n\n\nimsize\nint\n3\nSize (in inches) of images that will be displayed in the returned figure\n\n\nsuptitle\nstr\nNone\nTitle to be set to returned figure\n\n\nsharex\nbool\nFalse\n\n\n\nsharey\nbool\nFalse\n\n\n\nsqueeze\nbool\nTrue\n\n\n\nwidth_ratios\nNoneType\nNone\n\n\n\nheight_ratios\nNoneType\nNone\n\n\n\nsubplot_kw\nNoneType\nNone\n\n\n\ngridspec_kw\nNoneType\nNone\n\n\n\nReturns\n(plt.Figure, plt.Axes)\n\nReturns both fig and ax as a tuple\n\n\n\n\nsource\n\n\nget_arch\n\n get_arch (arch_name)\n\n\nfor arch_name in all_archs_names:\n    get_arch(arch_name)\n\n\nsource\n\n\nts_learner\n\n ts_learner (dls, arch=None, c_in=None, c_out=None, seq_len=None, d=None,\n             splitter=<function trainable_params>, loss_func=None,\n             opt_func=<function Adam>, lr=0.001, cbs=None, metrics=None,\n             path=None, model_dir='models', wd=None, wd_bn_bias=False,\n             train_bn=True, moms=(0.95, 0.85, 0.95), train_metrics=False,\n             valid_metrics=True, seed=None, device=None, verbose=False,\n             pretrained=False, weights_path=None, exclude_head=True,\n             cut=-1, init=None, arch_config={})\n\n\nsource\n\n\ntsimage_learner\n\n tsimage_learner (dls, arch=None, pretrained=False, loss_func=None,\n                  opt_func=<function Adam>, lr=0.001, cbs=None,\n                  metrics=None, path=None, model_dir='models', wd=None,\n                  wd_bn_bias=False, train_bn=True, moms=(0.95, 0.85,\n                  0.95), c_in=None, c_out=None, device=None,\n                  verbose=False, init=None, arch_config={})\n\n\nsource\n\n\nLearner.decoder\n\n Learner.decoder (o)\n\n\nfrom tsai.data.core import *\nfrom tsai.data.external import get_UCR_data\nfrom tsai.models.FCNPlus import FCNPlus\n\n\nX, y, splits = get_UCR_data('OliveOil', verbose=True, split_data=False)\ntfms  = [None, [TSCategorize()]]\ndls = get_ts_dls(X, y, splits=splits, tfms=tfms)\nlearn = ts_learner(dls, FCNPlus)\nfor p in learn.model.parameters():\n    p.requires_grad=False\ntest_eq(count_parameters(learn.model), 0)\nlearn.freeze()\ntest_eq(count_parameters(learn.model), 1540)\nlearn.unfreeze()\ntest_eq(count_parameters(learn.model), 264580)\n\nlearn = ts_learner(dls, 'FCNPlus')\nfor p in learn.model.parameters():\n    p.requires_grad=False\ntest_eq(count_parameters(learn.model), 0)\nlearn.freeze()\ntest_eq(count_parameters(learn.model), 1540)\nlearn.unfreeze()\ntest_eq(count_parameters(learn.model), 264580)\n\nDataset: OliveOil\nX      : (60, 1, 570)\ny      : (60,)\nsplits : (#30) [0,1,2,3,4,5,6,7,8,9...] (#30) [30,31,32,33,34,35,36,37,38,39...] \n\n\n\n\nlearn.show_batch();\n\n\n\n\n\nfrom fastai.metrics import accuracy\nfrom tsai.data.preprocessing import TSRobustScale\n\n\nX, y, splits = get_UCR_data('OliveOil', split_data=False)\ntfms  = [None, TSClassification()]\nbatch_tfms = TSRobustScale()\ndls = get_ts_dls(X, y, tfms=tfms, splits=splits, batch_tfms=batch_tfms)\nlearn = ts_learner(dls, FCNPlus, metrics=accuracy, train_metrics=True)\nlearn.fit_one_cycle(2)\nlearn.plot_metrics()\n\n\n\n\n\n\n\n\n\nif not os.path.exists(\"./models\"): os.mkdir(\"./models\")\nif not os.path.exists(\"./data\"): os.mkdir(\"./data\")\nnp.save(\"data/X_test.npy\", X[splits[1]])\nnp.save(\"data/y_test.npy\", y[splits[1]])\nlearn.export(\"./models/test.pth\")"
  },
  {
    "objectID": "models.resnet.html",
    "href": "models.resnet.html",
    "title": "ResNet",
    "section": "",
    "text": "This is an unofficial PyTorch implementation created by Ignacio Oguiza - oguiza@timeseriesAI.co\n\n\nsource\n\nResNet\n\n ResNet (c_in, c_out)\n\nSame as nn.Module, but no need for subclasses to call super().__init__\n\nsource\n\n\nResBlock\n\n ResBlock (ni, nf, kss=[7, 5, 3])\n\nSame as nn.Module, but no need for subclasses to call super().__init__\n\nxb = torch.rand(2, 3, 4)\ntest_eq(ResNet(3,2)(xb).shape, [xb.shape[0], 2])\ntest_eq(count_parameters(ResNet(3, 2)), 479490) # for (3,2)\n\n\nResNet(3,2)\n\nResNet(\n  (resblock1): ResBlock(\n    (convblock1): ConvBlock(\n      (0): Conv1d(3, 64, kernel_size=(7,), stride=(1,), padding=(3,), bias=False)\n      (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (2): ReLU()\n    )\n    (convblock2): ConvBlock(\n      (0): Conv1d(64, 64, kernel_size=(5,), stride=(1,), padding=(2,), bias=False)\n      (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (2): ReLU()\n    )\n    (convblock3): ConvBlock(\n      (0): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)\n      (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (shortcut): ConvBlock(\n      (0): Conv1d(3, 64, kernel_size=(1,), stride=(1,), bias=False)\n      (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (add): Add\n    (act): ReLU()\n  )\n  (resblock2): ResBlock(\n    (convblock1): ConvBlock(\n      (0): Conv1d(64, 128, kernel_size=(7,), stride=(1,), padding=(3,), bias=False)\n      (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (2): ReLU()\n    )\n    (convblock2): ConvBlock(\n      (0): Conv1d(128, 128, kernel_size=(5,), stride=(1,), padding=(2,), bias=False)\n      (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (2): ReLU()\n    )\n    (convblock3): ConvBlock(\n      (0): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)\n      (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (shortcut): ConvBlock(\n      (0): Conv1d(64, 128, kernel_size=(1,), stride=(1,), bias=False)\n      (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (add): Add\n    (act): ReLU()\n  )\n  (resblock3): ResBlock(\n    (convblock1): ConvBlock(\n      (0): Conv1d(128, 128, kernel_size=(7,), stride=(1,), padding=(3,), bias=False)\n      (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (2): ReLU()\n    )\n    (convblock2): ConvBlock(\n      (0): Conv1d(128, 128, kernel_size=(5,), stride=(1,), padding=(2,), bias=False)\n      (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (2): ReLU()\n    )\n    (convblock3): ConvBlock(\n      (0): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)\n      (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (shortcut): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (add): Add\n    (act): ReLU()\n  )\n  (gap): AdaptiveAvgPool1d(output_size=1)\n  (squeeze): Squeeze(dim=-1)\n  (fc): Linear(in_features=128, out_features=2, bias=True)\n)"
  },
  {
    "objectID": "models.tabtransformer.html",
    "href": "models.tabtransformer.html",
    "title": "TabTransformer",
    "section": "",
    "text": "This is an unofficial TabTransformer Pytorch implementation created by Ignacio Oguiza (oguiza@timeseriesAI.co)\nHuang, X., Khetan, A., Cvitkovic, M., & Karnin, Z. (2020). TabTransformer: Tabular Data Modeling Using Contextual Embeddings. arXiv preprint https://arxiv.org/pdf/2012.06678\nOfficial repo: https://github.com/awslabs/autogluon/tree/master/tabular/src/autogluon/tabular/models/tab_transformer\n\nsource\n\nTabTransformer\n\n TabTransformer (classes, cont_names, c_out, column_embed=True,\n                 add_shared_embed=False, shared_embed_div=8,\n                 embed_dropout=0.1, drop_whole_embed=False, d_model=32,\n                 n_layers=6, n_heads=8, d_k=None, d_v=None, d_ff=None,\n                 res_attention=True, attention_act='gelu',\n                 res_dropout=0.1, norm_cont=True, mlp_mults=(4, 2),\n                 mlp_dropout=0.0, mlp_act=None, mlp_skip=False,\n                 mlp_bn=False, bn_final=False)\n\nBase class for all neural network modules.\nYour models should also subclass this class.\nModules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes::\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 20, 5)\n        self.conv2 = nn.Conv2d(20, 20, 5)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        return F.relu(self.conv2(x))\nSubmodules assigned in this way will be registered, and will have their parameters converted too when you call :meth:to, etc.\n.. note:: As per the example above, an __init__() call to the parent class must be made before assignment on the child.\n:ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool\n\nsource\n\n\nFullEmbeddingDropout\n\n FullEmbeddingDropout (dropout:float)\n\nFrom https://github.com/jrzaurin/pytorch-widedeep/blob/be96b57f115e4a10fde9bb82c35380a3ac523f52/pytorch_widedeep/models/tab_transformer.py#L153\n\nsource\n\n\nSharedEmbedding\n\n SharedEmbedding (num_embeddings, embedding_dim, shared_embed=True,\n                  add_shared_embed=False, shared_embed_div=8)\n\nBase class for all neural network modules.\nYour models should also subclass this class.\nModules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes::\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 20, 5)\n        self.conv2 = nn.Conv2d(20, 20, 5)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        return F.relu(self.conv2(x))\nSubmodules assigned in this way will be registered, and will have their parameters converted too when you call :meth:to, etc.\n.. note:: As per the example above, an __init__() call to the parent class must be made before assignment on the child.\n:ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool\n\nsource\n\n\nifnone\n\n ifnone (a, b)\n\nb if a is None else a\n\nfrom fastai.tabular.all import *\n\n\npath = untar_data(URLs.ADULT_SAMPLE)\ndf = pd.read_csv(path/'adult.csv')\ndls = TabularDataLoaders.from_csv(path/'adult.csv', path=path, y_names=\"salary\",\n    cat_names = ['workclass', 'education', 'marital-status', 'occupation', 'relationship', 'race'],\n    cont_names = ['age', 'fnlwgt', 'education-num'],\n    procs = [Categorify, FillMissing, Normalize])\nx_cat, x_cont, yb = first(dls.train)\nmodel = TabTransformer(dls.classes, dls.cont_names, dls.c)\ntest_eq(model(x_cat, x_cont).shape, (dls.train.bs, dls.c))"
  },
  {
    "objectID": "models.minirocket.html",
    "href": "models.minirocket.html",
    "title": "MINIROCKET",
    "section": "",
    "text": "A Very Fast (Almost) Deterministic Transform for Time Series Classification.\n\n\nsource\n\nMiniRocketClassifier\n\n MiniRocketClassifier (num_features=10000, max_dilations_per_kernel=32,\n                       random_state=None, alphas=array([1.e-03, 1.e-02,\n                       1.e-01, 1.e+00, 1.e+01, 1.e+02, 1.e+03]),\n                       normalize_features=True, memory=None,\n                       verbose=False, scoring=None, class_weight=None,\n                       **kwargs)\n\nTime series classification using MINIROCKET features and a linear classifier\n\nsource\n\n\nload_minirocket\n\n load_minirocket (fname, path='./models')\n\n\nsource\n\n\nMiniRocketRegressor\n\n MiniRocketRegressor (num_features=10000, max_dilations_per_kernel=32,\n                      random_state=None, alphas=array([1.e-03, 1.e-02,\n                      1.e-01, 1.e+00, 1.e+01, 1.e+02, 1.e+03]),\n                      normalize_features=True, memory=None, verbose=False,\n                      scoring=None, **kwargs)\n\nTime series regression using MINIROCKET features and a linear regressor\n\nsource\n\n\nload_minirocket\n\n load_minirocket (fname, path='./models')\n\n\nsource\n\n\nMiniRocketVotingClassifier\n\n MiniRocketVotingClassifier (n_estimators=5, weights=None, n_jobs=-1,\n                             num_features=10000,\n                             max_dilations_per_kernel=32,\n                             random_state=None, alphas=array([1.e-03,\n                             1.e-02, 1.e-01, 1.e+00, 1.e+01, 1.e+02,\n                             1.e+03]), normalize_features=True,\n                             memory=None, verbose=False, scoring=None,\n                             class_weight=None, **kwargs)\n\nTime series classification ensemble using MINIROCKET features, a linear classifier and majority voting\n\nsource\n\n\nget_minirocket_preds\n\n get_minirocket_preds (X, fname, path='./models', model=None)\n\n\nsource\n\n\nMiniRocketVotingRegressor\n\n MiniRocketVotingRegressor (n_estimators=5, weights=None, n_jobs=-1,\n                            num_features=10000,\n                            max_dilations_per_kernel=32,\n                            random_state=None, alphas=array([1.e-03,\n                            1.e-02, 1.e-01, 1.e+00, 1.e+01, 1.e+02,\n                            1.e+03]), normalize_features=True,\n                            memory=None, verbose=False, scoring=None,\n                            **kwargs)\n\nTime series regression ensemble using MINIROCKET features, a linear regressor and a voting regressor\n\n# Univariate classification with sklearn-type API\ndsid = 'OliveOil'\nfname = 'MiniRocketClassifier'\nX_train, y_train, X_test, y_test = get_UCR_data(dsid)\ncls = MiniRocketClassifier()\ncls.fit(X_train, y_train)\ncls.save(fname)\npred = cls.score(X_test, y_test)\ndel cls\ncls = load_minirocket(fname)\ntest_eq(cls.score(X_test, y_test), pred)\n\n\n# Multivariate classification with sklearn-type API\ndsid = 'NATOPS'\nX_train, y_train, X_test, y_test = get_UCR_data(dsid)\ncls = MiniRocketClassifier()\ncls.fit(X_train, y_train)\ncls.score(X_test, y_test)\n\n0.9222222222222223\n\n\n\n# Multivariate classification with sklearn-type API\ndsid = 'NATOPS'\nX_train, y_train, X_test, y_test = get_UCR_data(dsid)\ncls = MiniRocketVotingClassifier(5)\ncls.fit(X_train, y_train)\ncls.score(X_test, y_test)\n\n0.9277777777777778\n\n\n\nfrom sklearn.metrics import mean_squared_error\n\n\n# Univariate regression with sklearn-type API\ndsid = 'Covid3Month'\nfname = 'MiniRocketRegressor'\nX_train, y_train, X_test, y_test = get_Monash_regression_data(dsid)\nif X_train is not None:\n    rmse_scorer = make_scorer(mean_squared_error, greater_is_better=False)\n    reg = MiniRocketRegressor(scoring=rmse_scorer)\n    reg.fit(X_train, y_train)\n    reg.save(fname)\n    del reg\n    reg = load_minirocket(fname)\n    y_pred = reg.predict(X_test)\n    print(mean_squared_error(y_test, y_pred, squared=False))\n\n0.0416464135570041\n\n\n\n# Multivariate regression with sklearn-type API\ndsid = 'AppliancesEnergy'\nX_train, y_train, X_test, y_test = get_Monash_regression_data(dsid)\nif X_train is not None:\n    rmse_scorer = make_scorer(mean_squared_error, greater_is_better=False)\n    reg = MiniRocketRegressor(scoring=rmse_scorer)\n    reg.fit(X_train, y_train)\n    reg.save(fname)\n    del reg\n    reg = load_minirocket(fname)\n    y_pred = reg.predict(X_test)\n    print(mean_squared_error(y_test, y_pred, squared=False))\n\n2.359614172890583\n\n\n\n# Multivariate regression ensemble with sklearn-type API\nif X_train is not None:\n    reg = MiniRocketVotingRegressor(5, scoring=rmse_scorer)\n    reg.fit(X_train, y_train)\n    y_pred = reg.predict(X_test)\n    print(mean_squared_error(y_test, y_pred, squared=False))\n\n2.1579181408369656"
  }
]